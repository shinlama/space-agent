import streamlit as st
import googlemaps
from openai import OpenAI
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Tuple
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import os
from pathlib import Path
from dotenv import load_dotenv
import json
import re
from wordcloud import WordCloud
import numpy as np
import warnings # ê²½ê³  ë©”ì‹œì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€
import folium
from folium.plugins import HeatMap, MarkerCluster
from streamlit_folium import st_folium
import time
from scipy import stats

# NLP ëª¨ë¸ ê´€ë ¨ ì„í¬íŠ¸ (Hugging Face Transformers)
try:
    from sentence_transformers import SentenceTransformer
    from transformers import pipeline
    from transformers import logging as transformers_logging # íŠ¸ëœìŠ¤í¬ë¨¸ ë¡œê¹… ì„í¬íŠ¸
    # ëª…ì‹œì  ë¡œë“œë¥¼ ìœ„í•œ ì¶”ê°€ import
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except ImportError:
    st.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(sentence-transformers, transformers)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `pip install sentence-transformers transformers` ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    st.stop()


# ----------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •, ìºì‹± ë° ìœ í‹¸ë¦¬í‹°
# ----------------------------------------------------

# í°íŠ¸ ê²½ë¡œ ìºì‹± (WordCloudìš©)
@st.cache_resource(show_spinner=False)
def get_font_path():
    # ì‹¤ì œ í™˜ê²½ì— ë§ì¶° í°íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆì‹œ: NotoSansKR-Regular.ttf íŒŒì¼ì´ 'fonts' í´ë”ì— ìˆë‹¤ê³  ê°€ì •
    # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜, ì‹œìŠ¤í…œ í°íŠ¸ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # í˜„ì¬ëŠ” ë”ë¯¸ ê²½ë¡œë¡œ ì„¤ì •í•˜ë©°, ì‹¤í–‰ í™˜ê²½ì— ë”°ë¼ ìˆ˜ì • í•„ìš”
    try:
        # Streamlit í°íŠ¸ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì‹¤ì œ í°íŠ¸ ê²½ë¡œë¥¼ ì§€ì •
        return os.path.join(os.path.dirname(__file__), 'fonts', 'NotoSansKR-Regular.ttf')
    except:
        return None # í°íŠ¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°

font_path = get_font_path()

# í‘œë³¸ CSV ê²½ë¡œ
SAMPLED_CAFE_CSV = Path(__file__).resolve().parent / "ì„œìš¸ì‹œ_ìƒê¶Œ_ì¹´í˜ë¹µ_í‘œë³¸.csv"
FULL_CAFE_CSV = Path(__file__).resolve().parent / "ì„œìš¸ì‹œ_ìƒê¶Œ_ì¹´í˜ë¹µ.csv"


@st.cache_data(show_spinner="í‘œë³¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
def load_sampled_cafes(csv_path: Path) -> pd.DataFrame:
    if not csv_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

    try:
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
    except UnicodeDecodeError:
        df = pd.read_csv(csv_path, encoding="cp949")
    return df

@st.cache_data(show_spinner="ì „ì²´ ì¹´í˜ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
def load_full_cafes(csv_path: Path) -> pd.DataFrame:
    if not csv_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

    try:
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
    except UnicodeDecodeError:
        df = pd.read_csv(csv_path, encoding="cp949")
    return df

# WordCloud ì´ë¯¸ì§€ ìºì‹± (PIL ì´ë¯¸ì§€ ë°˜í™˜)
@st.cache_data(show_spinner=False)
def generate_wordcloud(text: str, font_path: str, colormap: str = "Greens", **kwargs):
    if not text or not text.strip():
        return None
    
    # í°íŠ¸ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ê²½ê³ 
    if font_path and os.path.exists(font_path):
        font_kwarg = {'font_path': font_path}
    else:
        # st.warning("WordCloud í°íŠ¸ íŒŒì¼ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font_kwarg = {}
        
    wc = WordCloud(
        **font_kwarg,
        background_color="white",
        width=600,
        height=300,
        scale=2,
        max_words=180,
        prefer_horizontal=0.9,
        colormap=colormap,
        collocations=True,
        normalize_plurals=False,
        relative_scaling=0.35,
        min_font_size=8,
        max_font_size=90,
        random_state=42,
        regexp=r"[ê°€-í£a-zA-Z]+" # í•œê¸€, ì˜ë¬¸ë§Œ í¬í•¨
    ).generate(text)
    return wc.to_image()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="Seoul Place Recommendation", page_icon="ğŸ—ºï¸", layout="wide")

# ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜ (OpenAI ê¸°ë°˜)
def semantic_split(text: str) -> list[str]:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•´ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    """
    try:
        prompt = f"""
        ì•„ë˜ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„(í•˜ë‚˜ì˜ ê°ì •ì´ë‚˜ í‰ê°€ë¥¼ ë‹´ì€ ë‹¨ë½)ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.
        ê° ë¬¸ì¥ì€ ë…ë¦½ì ì¸ íŒë‹¨ì´ ê°€ëŠ¥í•œ ë‹¨ìœ„ì—¬ì•¼ í•˜ë©°, ë¶ˆí•„ìš”í•œ ì ‘ì†ì‚¬ëŠ” ì œê±°í•˜ì„¸ìš”.
        ì¶œë ¥ì€ JSON ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.

        ì˜ˆì‹œ:
        ì…ë ¥: "ì¹´í˜ ë¶„ìœ„ê¸°ê°€ ì¢‹ê³  ì»¤í”¼ëŠ” ë§›ìˆì§€ë§Œ ì¢Œì„ì´ ì¢ì•„ìš”."
        ì¶œë ¥: ["ì¹´í˜ ë¶„ìœ„ê¸°ê°€ ì¢‹ë‹¤.", "ì»¤í”¼ê°€ ë§›ìˆë‹¤.", "ì¢Œì„ì´ ì¢ë‹¤."]

        ë¦¬ë·° í…ìŠ¤íŠ¸:
        {text}
        """

        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "semantic_split_schema",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "sentences": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["sentences"],
                        "additionalProperties": False
                    }
                }
            },
            temperature=0
        )
        content = resp.choices[0].message.content
        parsed = json.loads(content) if content else {"sentences": []}
        if isinstance(parsed, list):
            candidates = parsed
        else:
            candidates = parsed.get("sentences", [])
        cleaned = [c.strip() for c in candidates if isinstance(c, str) and len(c.strip()) > 2]
        if cleaned:
            return cleaned
    except Exception as e:
        print(f"Semantic split error: {e}")
        # í´ë°±ì€ ì•„ë˜ ì¼ë°˜ ë¶„ê¸°ì—ì„œ ìˆ˜í–‰
        pass
    # í´ë°±: ë¬¸ì¥ë¶€í˜¸ â†’ ì ‘ì† í‘œí˜„ 2ë‹¨ê³„ ë¶„í• 
    fallback_units = []
    for s in re.split(r'[.!?]\s*', text):
        if not s or not s.strip():
            continue
        parts = re.split(r'(?:í•˜ì§€ë§Œ|ê·¸ëŸ¬ë‚˜|ê·¸ëŸ°ë°|ì¸ë°|ì§€ë§Œ|ëŠ”ë°)', s)
        for p in parts:
            p = p.strip()
            if len(p) > 2:
                fallback_units.append(p)
    return fallback_units


# ìºì‹œëœ ì˜ë¯¸ ë¶„í•  ë˜í¼
@st.cache_data(show_spinner=False)
def cached_semantic_split(text: str) -> List[str]:
    return semantic_split(text)


# ìºì‹œëœ LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ë˜í¼
@st.cache_data(show_spinner=False)
def cached_unified_summary(review_text: str):
    try:
        sample = review_text[:1200]
        unified_prompt = f"""
        ë‹¤ìŒ ë¦¬ë·°ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¥ì†Œë¥¼ ë¶„ì„í•˜ì—¬ í•œ ë²ˆì— ê²°ê³¼ë§Œ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
        1) positive_keywords: ì¥ì†Œ/ì¥ì†Œì„± ê´€ë ¨ ê¸ì • í•µì‹¬ ë‹¨ì–´ ìµœëŒ€ 10ê°œ (ë¬¸ìì—´ ë°°ì—´)
        2) negative_keywords: ì¥ì†Œ/ì¥ì†Œì„± ê´€ë ¨ ë¶€ì • í•µì‹¬ ë‹¨ì–´ ìµœëŒ€ 10ê°œ (ë¬¸ìì—´ ë°°ì—´)
        3) summary: ì „ë°˜ì  ë¶„ìœ„ê¸°/ê³µê°„ íŠ¹ì„±/ì£¼ìš” ê²½í—˜ ì¤‘ì‹¬ì˜ 5~8ë¬¸ì¥ ìš”ì•½ (ë¬¸ìì—´)

        ë¦¬ë·° í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì •ì˜ êµ¬ê°„ ì•ˆì˜ í…ìŠ¤íŠ¸ë§Œ ì°¸ê³ í•˜ì„¸ìš”:
        ```
        {sample}
        ```
        """
        unified_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": unified_prompt}],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "unified_summary_schema",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "positive_keywords": {"type": "array", "items": {"type": "string"}},
                            "negative_keywords": {"type": "array", "items": {"type": "string"}},
                            "summary": {"type": "string"}
                        },
                        "required": ["positive_keywords", "negative_keywords", "summary"],
                        "additionalProperties": False
                    }
                }
            },
            temperature=0
        )
        content = unified_response.choices[0].message.content
        parsed = json.loads(content) if content else {"positive_keywords": [], "negative_keywords": [], "summary": ""}
        return {
            "positive_keywords": parsed.get("positive_keywords", []) or [],
            "negative_keywords": parsed.get("negative_keywords", []) or [],
            "summary": (parsed.get("summary") or "").strip() or "ë¦¬ë·° ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ LLM ìš”ì•½ì´ ì–´ë µìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        print(f"LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "positive_keywords": [],
            "negative_keywords": [],
            "summary": "LLM ìš”ì•½ ì‹¤íŒ¨. NLP ë¶„ì„ë§Œ ì§„í–‰ë¨."
        }

# ----------------------------------------------------
# 2. API í‚¤ ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ----------------------------------------------------

if "history" not in st.session_state:
    st.session_state.history = []

# API í‚¤ ë¡œë“œ
gmaps_key = os.getenv("Maps_API_KEY") or st.secrets.get("Maps_API_KEY", "")
openai_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")

if "gmaps_key" not in st.session_state:
    st.session_state.gmaps_key = gmaps_key or ""
if "openai_key" not in st.session_state:
    st.session_state.openai_key = openai_key or ""

# API í‚¤ ì…ë ¥ UI (í‚¤ê°€ ì—†ëŠ” ê²½ìš°)
if not st.session_state.gmaps_key or not st.session_state.openai_key:
    st.title("ğŸ—ºï¸ Seoul Place Recommendation and Spatial Evaluation System")

    st.info("API í‚¤ë¥¼ `.env` íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
    st.markdown("---")
    gmaps_input = st.text_input("Google Maps API Key", type="password")
    openai_input = st.text_input("OpenAI API Key", type="password")

    if st.button("Start"):
        if gmaps_input and openai_input:
            st.session_state.gmaps_key = gmaps_input
            st.session_state.openai_key = openai_input
            st.rerun()
        else:
            st.warning("Please enter both API keys.")
    st.stop()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OpenAI clientëŠ” í´ë°± ë¡œì§ì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ ì „ì—­ì ìœ¼ë¡œ ìœ ì§€)
try:
    gmaps = googlemaps.Client(key=st.session_state.gmaps_key)
    # LLM í´ë¼ì´ì–¸íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    client = OpenAI(api_key=st.session_state.openai_key)
except Exception as e:
    st.error(f"API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()


# ----------------------------------------------------
# 3. LangGraph State ì •ì˜ ë° ëª¨ë¸ ë¡œë”© (ìºì‹œ)
# ----------------------------------------------------

class AgentState(BaseModel):
    query: str
    places: Optional[List[Dict[str, Any]]] = Field(default_factory=list)
    answer: Optional[str] = ""

# ì¥ì†Œì„± ìš”ì¸ ì„ë² ë”© ë° ëª¨ë¸ ë¡œë“œ (Sentence-BERT)
def _compute_factors_hash(path: str = "factors.json") -> str:
    try:
        with open(path, "rb") as f:
            import hashlib
            return hashlib.md5(f.read()).hexdigest()
    except Exception:
        return ""

@st.cache_resource(show_spinner="ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_category_embeddings(factors_hash: str):
    try:
        # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©ì— ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©
        model = SentenceTransformer("jhgan/ko-sroberta-multitask")
        # factors.json íŒŒì¼ ë¡œë“œ
        with open("factors.json", "r", encoding="utf-8") as f:
            factors = json.load(f)
    except FileNotFoundError:
        st.error("ì˜¤ë¥˜: 'factors.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— íŒŒì¼ì„ ìƒì„±í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    except Exception as e:
        st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    embeddings = {}
    score_structure = {}
    
    # 11ê°œ ì„¸ë¶€ ìš”ì¸ì˜ ì •ì˜ ë¬¸ì¥ì„ ì„ë² ë”©
    for main_cat, subcats in factors.items():
        score_structure[main_cat] = {}
        for subcat, definition in subcats.items():
            emb = model.encode(definition, normalize_embeddings=True)
            embeddings[subcat] = emb
            score_structure[main_cat][subcat] = None
            
    return embeddings, model, score_structure

factors_hash = _compute_factors_hash()
category_embeddings, embed_model, new_score_structure_template = load_category_embeddings(factors_hash)

@st.cache_resource(show_spinner="ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_sentiment_model_tabularis():
    """
    ê³µê°œ ê°ì • ë¶„í¬í˜• ëª¨ë¸ (tabularisai/multilingual-sentiment-analysis) ê¸°ë°˜
    - ì¶œë ¥: 0.0 ~ 1.0 ì—°ì† ì ìˆ˜ (ë¶€ì •â†’ê¸ì •)
    """
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    import numpy as np

    model_name = "tabularisai/multilingual-sentiment-analysis"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    pipe = pipeline("text-classification", model=model, tokenizer=tokenizer, return_all_scores=True)
    weights = np.linspace(0, 1, 5)  # [0.0, 0.25, 0.5, 0.75, 1.0]

    def predict_score(sentences: List[str]):
        if not sentences:
            return []
        results = pipe(sentences)
        scores = []
        for res in results:
            probs = np.array([r['score'] for r in res])
            score = float(np.dot(probs, weights))
            scores.append(score)
        return scores

    return predict_score


sentiment_model = load_sentiment_model_tabularis()


# ----------------------------------------------------
# 4. LangGraph Node ì •ì˜
# ----------------------------------------------------

def search_places(state: AgentState):
    """Google Maps APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜"""
    if state.places is None:
        state.places = []
    try:
        # ì„œìš¸ì‹œì²­ ê¸°ì¤€(37.5665,126.9780) ë°˜ê²½ 10km ê²€ìƒ‰
        res = gmaps.places(query=state.query, language="ko", location="37.5665,126.9780", radius=10000)
        state.places = res.get('results', [])[:5]
    except Exception as e:
        st.error(f"Google Maps ì¥ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("Google Maps API í‚¤ê°€ ìœ íš¨í•œì§€, ë˜ëŠ” Places APIê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        state.places = []
    return state.dict()

def analyze_reviews(state: AgentState):
    """SBERT + Sentiment ê¸°ë°˜ ì •ëŸ‰ í‰ê°€ + LLM í•´ì„"""
    if state.places is None:
        state.places = []

    place_infos = []
    
    SIMILARITY_THRESHOLD = 0.35
    ALPHA, BETA = 0.75, 0.25  # ìœ ì‚¬ë„ ë¹„ì¤‘ ì¶”ê°€ ìƒí–¥
    
    # factors.jsonì€ í•œ ë²ˆë§Œ ë¡œë“œ (ì†ë„ ê°œì„ )
    with open("factors.json", "r", encoding="utf-8") as f:
        factor_definitions = json.load(f)
    
    for place in state.places:
        place_id = place.get("place_id")
        if not place_id:
            continue

        details = gmaps.place(place_id=place_id, language="ko").get('result', {})
        reviews = details.get('reviews', [])[:10]
        review_texts = [r['text'] for r in reviews if r.get('text')]
        if not review_texts:
            continue

        # 1) ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬ (LLM ì‚¬ìš©)
        review_text = "\n".join(review_texts)
        review_units = cached_semantic_split(review_text)
        if not review_units:
            review_units = cached_semantic_split(" ".join(review_texts))

        # 2) SBERT + ê°ì„±ëª¨ë¸ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        factor_sentiments = {f: [] for f in category_embeddings.keys()}
        # ë¦¬ë·° ìš”ì•½/í‚¤ì›Œë“œ (summary)ì™€ ì ìˆ˜ í•´ì„¤(explanation)ì€ ë¶„ë¦¬ ìƒì„±
        summary = "ë¦¬ë·° ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        positive_keywords: List[str] = []
        negative_keywords: List[str] = []

        # 2-1) ë¦¬ë·° ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ (summary ì „ìš©)
        try:
            cached = cached_unified_summary(review_text)
            positive_keywords = cached.get("positive_keywords", []) or []
            negative_keywords = cached.get("negative_keywords", []) or []
            summary = cached.get("summary", "") or "ë¦¬ë·° ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ LLM ìš”ì•½ì´ ì–´ë µìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            summary = "LLM ìš”ì•½ ì‹¤íŒ¨. NLP ë¶„ì„ë§Œ ì§„í–‰ë¨."
            positive_keywords, negative_keywords = [], []

        # ê°ì„±(0~1) ìŠ¤ì½”ì–´ ë° ë°°ì¹˜ ì„ë² ë”©/ìœ ì‚¬ë„
        sentiment_scores = sentiment_model(review_units)
        unit_embs = embed_model.encode(review_units, normalize_embeddings=True)
        subcat_list = list(category_embeddings.keys())
        factor_mat = np.stack([category_embeddings[s] for s in subcat_list], axis=0)
        sim_mat = np.matmul(unit_embs, factor_mat.T)

        for i, unit in enumerate(review_units):
            raw_sent = float(sentiment_scores[i]) if i < len(sentiment_scores) else 0.5
            # ê°ì„± ë³´ì •: í•˜í•œ 0.3 ê¸°ì¤€ìœ¼ë¡œ ì¶”ê°€ ì™„í™”
            sent_adj = np.clip((raw_sent - 0.3) / 0.7, 0, 1)
            sims = sim_mat[i]
            for j, sim in enumerate(sims):
                # ìœ ì‚¬ë„ ë³´ì •: 0.3 ê¸°ì¤€ìœ¼ë¡œ ì¶”ê°€ ì™„í™” (ë” ë§ì€ ë¬¸ì¥ í¬í•¨)
                sim_adj = np.clip((float(sim) - 0.3) / 0.5, 0, 1)
                if sim_adj > 0:
                    f_name = subcat_list[j]
                    combined = ALPHA * sim_adj + BETA * sent_adj
                    # ì‹œê·¸ëª¨ì´ë“œ: ì¤‘ì‹¬ 0.4, ê¸°ìš¸ê¸° 2.2ë¡œ ìƒí•œ í™•ì¥
                    score_scaled = 1 / (1 + np.exp(-2.2 * (combined - 0.4)))
                    factor_sentiments[f_name].append(float(score_scaled))

        # 3) í‚¤ì›Œë“œ ê¸°ë°˜ ë¶€ìŠ¤íŒ… (ì„ë² ë”© í•œê³„ ë³´ì™„)
        keyword_boosts = {
            "ê³ ìœ ì„±": ["ë…íŠ¹", "ìœ ë‹ˆí¬", "ì°¨ë³„", "ì»¨ì…‰", "í…Œë§ˆ", "íŠ¹ìƒ‰", "ê°œì„±", "íŠ¹ë³„í•œ", "ì•„ì´ë´í‹°í‹°", "ìœ ì¼", "ë…ì°½"],
            "ë¬¸í™”ì  ë§¥ë½": ["ì „í†µ", "ì—­ì‚¬", "ë…„", "ì˜¤ë˜", "ì˜›", "ê³ í’", "ë¬¸í™”", "ë°°ê²½", "ìŠ¤í† ë¦¬", "ì„¸ì›”", "ë‚´ë ¥", "ìœ ì„œ", "ë ˆíŠ¸ë¡œ", "ë¹ˆí‹°ì§€", "í´ë˜ì‹", "ì•¤í‹°í¬", "ê³¼ê±°", "ì˜›ë‚ "],
            "ì§€ì—­ ì •ì²´ì„±": ["ì§€ì—­", "ë™ë„¤", "ë§ˆì„", "ê·¼ì²˜", "ì£¼ë³€", "ëª…ì†Œ", "ëœë“œë§ˆí¬", "ìƒì§•", "ëŒ€í‘œ", "ì‹ ì´Œ", "í™ëŒ€", "ê°•ë‚¨", "ì´íƒœì›", "ì—°ë‚¨", "ì„±ìˆ˜", "ì„ì§€ë¡œ", "ìµì„ ë™", "ë¶ì´Œ", "ì‚¼ì²­ë™", "ì¢…ë¡œ", "ëª…ë™"],
            "ê¸°ì–µ/ê²½í—˜": ["ì¶”ì–µ", "ê°ë™", "ì¸ìƒ", "íŠ¹ë³„", "ìŠì„ ìˆ˜", "ê¸°ì–µ", "íšŒìƒ", "ê²½í—˜", "ëŠë‚Œ"],
            "ì‹¬ë¯¸ì„±": ["ì˜ˆì˜", "ì•„ë¦„", "ë©‹ì§€", "ì„¸ë ¨", "ì•¼ê²½", "ë·°", "ì¸í…Œë¦¬ì–´", "ë””ìì¸", "ì¡°ëª…", "ì•„ëŠ‘", "ë¶„ìœ„ê¸°", "ê°ì„±"],
            "ê°ê°ì  ê²½í—˜": ["ìŒì•…", "í–¥", "ëƒ„ìƒˆ", "ì§ˆê°", "ë§›", "ì˜¤ê°", "ê°ê°", "ì†Œë¦¬", "ì´‰ê°"],
            "ì¾Œì ì„±": ["ì²­ê²°", "ê¹¨ë—", "ë°", "í†µí’", "í™”ì¥ì‹¤", "ìœ„ìƒ", "ì •ëˆ", "ì¾Œì "],
            "ì ‘ê·¼ì„±": ["ê°€ê¹", "ì ‘ê·¼", "ì—­", "ì •ë¥˜ì¥", "ë„ë³´", "ë¶„ ê±°ë¦¬", "í¸ë¦¬", "ì§€í•˜ì² ì—­", "ë²„ìŠ¤ì •ë¥˜ì¥", "ì—­ì—ì„œ", "ì—­ê¹Œì§€", "ì •ë¥˜ì¥ì—ì„œ", "ì •ë¥˜ì¥ê¹Œì§€", "ëŒ€ì¤‘êµí†µ", "êµí†µí¸", "ì˜¤ê¸° ì‰¬", "ì°¾ê¸° ì‰¬", "ìœ„ì¹˜ ì¢‹", "êµí†µ ì¢‹"],
            "í™œë™ì„±": ["ëŒ€í™”", "ì—…ë¬´", "ì‘ì—…", "íšŒì˜", "ê³µë¶€", "í™œë™", "ëª¨ì„", "ìŠ¤í„°ë””"],
            "ì‚¬íšŒì„±": ["ì¹œì ˆ", "ì„œë¹„ìŠ¤", "êµë¥˜", "ì†Œí†µ", "ì¹œê·¼", "ì¸ì‚¬", "ë°°ë ¤"],
            "í˜•íƒœì„±": ["ë„“", "ê³µê°„", "êµ¬ì¡°", "ë°°ì¹˜", "ê°œë°©", "ë™ì„ ", "ì¸µ", "ë£¸"],
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì§ì ‘ ê³ ì ìˆ˜ í• ë‹¹
        for factor, keywords in keyword_boosts.items():
            matched_kws = [kw for kw in keywords if kw in review_text]
            match_count = len(matched_kws)
            if match_count > 0:
                # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ìˆ˜ì— ë¹„ë¡€í•´ 0.75~0.95 í• ë‹¹
                boosted_score = min(0.75 + (match_count * 0.05), 0.95)
                # ì—¬ëŸ¬ ë²ˆ ì¶”ê°€í•´ í‰ê· ì—ì„œë„ ë†’ì€ ê°€ì¤‘ì¹˜ ìœ ì§€
                for _ in range(3):
                    factor_sentiments[factor].append(boosted_score)
                print(f"[BOOST] {factor}: {match_count}ê°œ í‚¤ì›Œë“œ ë§¤ì¹­ ({', '.join(matched_kws[:3])}) â†’ {boosted_score:.2f}")
        
        # 3-0) ì ‘ê·¼ì„± ì „ìš© íŒ¨í„´ ë§¤ì¹­ (ì‹œê°„+ê±°ë¦¬ í‘œí˜„ ê°•í™”)
        accessibility_patterns = [
            # ë„ë³´ ì‹œê°„ í‘œí˜„: "ë„ë³´ë¡œ 5ë¶„", "ë„ë³´ 10ë¶„", "5ë¶„ ë„ë³´" ë“±
            r'ë„ë³´\s*(?:ë¡œ\s*)?(\d+)\s*ë¶„',
            r'(\d+)\s*ë¶„\s*ë„ë³´',
            r'ë„ë³´\s*ë¡œ\s*(\d+)\s*ë¶„\s*(?:ì´ë©´|ë§Œì—|ê±¸ë¦¼|ê±¸ë ¤|ê°€ëŠ¥)',
            # ê±°ë¦¬ í‘œí˜„: "5ë¶„ ê±°ë¦¬", "10ë¶„ ê±°ë¦¬"
            r'(\d+)\s*ë¶„\s*ê±°ë¦¬',
            # ì—­/ì •ë¥˜ì¥ + ì‹œê°„: "ì§€í•˜ì² ì—­ì—ì„œ 5ë¶„", "ì—­ê¹Œì§€ 10ë¶„"
            r'(?:ì§€í•˜ì² ì—­|ì—­|ë²„ìŠ¤ì •ë¥˜ì¥|ì •ë¥˜ì¥)(?:ì—ì„œ|ê¹Œì§€)\s*(\d+)\s*ë¶„',
            r'(\d+)\s*ë¶„\s*(?:ì´ë©´|ë§Œì—|ê±¸ë¦¼|ê±¸ë ¤)\s*(?:ê°ˆ\s*ìˆ˜|ë„ì°©|ê°€ëŠ¥)',
            # "5ë¶„ì´ë©´ ê°ˆ ìˆ˜ ìˆë‹¤" ê°™ì€ í‘œí˜„
            r'(\d+)\s*ë¶„\s*ì´ë©´\s*(?:ê°ˆ\s*ìˆ˜|ë„ì°©|ê°€ëŠ¥)',
        ]
        
        accessibility_score_boost = 0.0
        matched_patterns = []
        for pattern in accessibility_patterns:
            matches = re.finditer(pattern, review_text, re.IGNORECASE)
            for match in matches:
                time_str = match.group(1) if match.groups() else None
                if time_str:
                    try:
                        time_minutes = int(time_str)
                        # 5ë¶„ ì´í•˜: ë§¤ìš° ë†’ì€ ì ìˆ˜ (0.90~0.95)
                        # 10ë¶„ ì´í•˜: ë†’ì€ ì ìˆ˜ (0.85~0.90)
                        # 15ë¶„ ì´í•˜: ë³´í†µ ì ìˆ˜ (0.75~0.85)
                        if time_minutes <= 5:
                            boost = 0.95
                        elif time_minutes <= 10:
                            boost = 0.90
                        elif time_minutes <= 15:
                            boost = 0.85
                        else:
                            boost = 0.80
                        
                        if boost > accessibility_score_boost:
                            accessibility_score_boost = boost
                        matched_patterns.append(f"{time_minutes}ë¶„ ({match.group(0)})")
                    except ValueError:
                        pass
        
        if matched_patterns:
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë§¤ìš° ë†’ì€ ì ìˆ˜ ë¶€ì—¬ (5ë²ˆ ì¶”ê°€ë¡œ í‰ê·  ê°€ì¤‘ì¹˜ ê°•í™”)
            for _ in range(5):
                factor_sentiments["ì ‘ê·¼ì„±"].append(accessibility_score_boost)
            print(f"[ACCESSIBILITY PATTERN] ì ‘ê·¼ì„±: {len(matched_patterns)}ê°œ íŒ¨í„´ ë§¤ì¹­ ({', '.join(matched_patterns[:3])}) â†’ {accessibility_score_boost:.2f}")
        
        # 3-1) ì„¸ë¶€ìš”ì¸ë³„ í‰ê·  ì ìˆ˜ (ì •ê·œí™” í¬í•¨)
        scores = json.loads(json.dumps(new_score_structure_template))
        all_vals = []
        for vals in factor_sentiments.values():
            all_vals.extend(vals)
        if all_vals:
            vmin, vmax = float(np.min(all_vals)), float(np.max(all_vals))
        else:
            vmin, vmax = 0.5, 0.5

        for main_cat, subcats in scores.items():
            for subcat in subcats.keys():
                vals = factor_sentiments.get(subcat, [])
                if vals and vmax > vmin:
                    raw = float(np.mean(vals))
                    # 0.30~1.0 ë²”ìœ„ë¡œ min-max ì •ê·œí™”
                    normed = 0.30 + 0.70 * ((raw - vmin) / (vmax - vmin + 1e-8))
                    scores[main_cat][subcat] = float(np.clip(normed, 0.30, 1.0))
                elif vals:
                    scores[main_cat][subcat] = float(np.clip(vals[0], 0.30, 1.0))
                else:
                    scores[main_cat][subcat] = 0.5

        # 4) LLM ê¸°ë°˜ ì ìˆ˜ ê²€ì¦ ë° ë³´ì • (GPT-4o ì¶”ë¡ )
        corrected_scores = json.loads(json.dumps(scores))  # ë³´ì • ì „ ë³µì‚¬
        correction_log = []
        try:
            sample_reviews = "\n".join(review_texts[:3])  # ë¦¬ë·° 5ê°œâ†’3ê°œë¡œ ì¶•ì†Œ
            
            validation_prompt = f"""
ë‹¹ì‹ ì€ ì¥ì†Œì„± í‰ê°€ ê°ì‚¬ìì…ë‹ˆë‹¤.
ì…ë ¥ëœ ì ìˆ˜ëŠ” SBERT + ê°ì„± íšŒê·€ëª¨ë¸ë¡œ ì‚°ì¶œëœ ê°’ì…ë‹ˆë‹¤.
ê° ìš”ì¸ë³„ ì ìˆ˜ì˜ íƒ€ë‹¹ì„±ì„ **ìš”ì¸ì˜ ì •ì˜ì— ë”°ë¼** ì •í™•íˆ ê²€í† í•˜ì„¸ìš”.

## ìš”ì¸ ì •ì˜ (ë°˜ë“œì‹œ ì°¸ê³ )
{json.dumps(factor_definitions, ensure_ascii=False, indent=2)}

## í˜„ì¬ ì ìˆ˜
{json.dumps(scores, ensure_ascii=False, indent=2)}

## ë¦¬ë·° ë‚´ìš©
{sample_reviews}

## ê²€í†  ê·œì¹™
1. ê° ìš”ì¸ì˜ ì •ì˜ì™€ í‚¤ì›Œë“œë¥¼ **ì •í™•íˆ** í™•ì¸í•˜ì„¸ìš”.
   ì˜ˆ: "ê°ê°ì  ê²½í—˜"ì€ ìŒì•…, í–¥ê¸°, ì§ˆê° ë“± ì˜¤ê° ìê·¹ / "ë¬¸í™”ì  ë§¥ë½"ì€ ì—­ì‚¬, ì „í†µ, ì§€ì—­ ë°°ê²½
2. ë¦¬ë·°ì—ì„œ í•´ë‹¹ ìš”ì¸ ì •ì˜ì— ë§ëŠ” ì–¸ê¸‰ì´ ìˆëŠ”ë° ì ìˆ˜ê°€ ë‚®ê±°ë‚˜, ì–¸ê¸‰ì´ ì—†ëŠ”ë° ì ìˆ˜ê°€ ë†’ìœ¼ë©´ delta ì œì•ˆ
3. deltaëŠ” -0.3 ~ +0.3 ë²”ìœ„
4. ê·¼ê±°ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±

## ì ‘ê·¼ì„± íŠ¹ë³„ ê²€í†  ê°€ì´ë“œ
**ì ‘ê·¼ì„±**ì€ ëŒ€ì¤‘êµí†µ ì ‘ê·¼, ë„ë³´ ê°€ëŠ¥ì„± ë“± ì¥ì†Œë¥¼ ì‰½ê²Œ ì°¾ì•„ì˜¤ê³  ì´ìš©í•  ìˆ˜ ìˆëŠ” ì •ë„ì…ë‹ˆë‹¤.
ë‹¤ìŒê³¼ ê°™ì€ í‘œí˜„ì´ ë¦¬ë·°ì— ìˆìœ¼ë©´ ì ‘ê·¼ì„± ì ìˆ˜ê°€ ë†’ì•„ì•¼ í•©ë‹ˆë‹¤:
- "ì§€í•˜ì² ì—­ì—ì„œ ë„ë³´ë¡œ 5ë¶„ì´ë©´ ê°ˆ ìˆ˜ ìˆë‹¤" â†’ ì ‘ê·¼ì„± ë§¤ìš° ë†’ìŒ (0.9 ì´ìƒ)
- "ë²„ìŠ¤ì •ë¥˜ì¥ì—ì„œ 10ë¶„ ê±°ë¦¬" â†’ ì ‘ê·¼ì„± ë†’ìŒ (0.85 ì´ìƒ)
- "ì—­ê¹Œì§€ 5ë¶„", "ë„ë³´ 5ë¶„", "5ë¶„ ê±°ë¦¬" â†’ ì ‘ê·¼ì„± ë†’ìŒ
- "ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì´ í¸ë¦¬í•˜ë‹¤", "êµí†µí¸ì´ ì¢‹ë‹¤" â†’ ì ‘ê·¼ì„± ë†’ìŒ
- "ê°€ê¹Œìš´ ê³³", "ì ‘ê·¼í•˜ê¸° ì‰¬ìš´ ìœ„ì¹˜" â†’ ì ‘ê·¼ì„± ë†’ìŒ

ë§Œì•½ ë¦¬ë·°ì— ìœ„ì™€ ê°™ì€ í‘œí˜„ì´ ìˆëŠ”ë° ì ‘ê·¼ì„± ì ìˆ˜ê°€ ë‚®ë‹¤ë©´(0.7 ì´í•˜), deltaë¥¼ +0.2~+0.3ìœ¼ë¡œ ì œì•ˆí•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSONë§Œ)
{{
  "corrections": [
    {{"factor": "ì¾Œì ì„±", "delta": 0.15, "reason": "ì²­ê²°, í™”ì¥ì‹¤, ì¶©ì „ì‹œì„¤ ê¸ì • ì–¸ê¸‰ ë§ìŒ"}},
    {{"factor": "ê°ê°ì  ê²½í—˜", "delta": 0.12, "reason": "ë””ì €íŠ¸ ë§›ê³¼ ë‹¤ì–‘ì„± ê°•ì¡°"}}
  ]
}}

ë³´ì • ë¶ˆí•„ìš” ì‹œ: {{"corrections": []}}
"""
            resp = client.chat.completions.create(
                model="gpt-4o",  # ë³´ì •ì€ ì •í™•í•œ ì¶”ë¡ ì´ í•„ìš”í•˜ë¯€ë¡œ gpt-4o ì‚¬ìš©
                messages=[{"role": "user", "content": validation_prompt}],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=500,  # ì¶©ë¶„í•œ í† í°ìœ¼ë¡œ ì •í™•í•œ ë³´ì •
            )
            correction_result = json.loads(resp.choices[0].message.content)
            corrections = correction_result.get("corrections", [])
            
            print(f"[DEBUG] GPT-4o ì‘ë‹µ: {correction_result}")  # ë””ë²„ê¹…ìš©
            
            # ë³´ì • ì ìš©
            for correction in corrections:
                if isinstance(correction, dict):
                    factor_name = correction.get("factor", "")
                    delta = float(correction.get("delta", 0))
                    reason = correction.get("reason", "")
                    
                    # ìš”ì¸ëª… ë§¤ì¹­ í›„ ì ìˆ˜ ë³´ì •
                    for main_cat, subcats in corrected_scores.items():
                        if factor_name in subcats:
                            old_val = subcats[factor_name]
                            new_val = np.clip(old_val + delta, 0.30, 1.0)
                            corrected_scores[main_cat][factor_name] = float(new_val)
                            correction_log.append({
                                "factor": factor_name,
                                "original": round(old_val, 2),
                                "adjusted": round(new_val, 2),
                                "delta": round(delta, 2),
                                "reason": reason
                            })
                            break
            
            # ë³´ì •ëœ ì ìˆ˜ë¥¼ ìµœì¢… ì ìˆ˜ë¡œ ì‚¬ìš©
            scores = corrected_scores
            
            if correction_log:
                print(f"[INFO] {len(correction_log)}ê°œ ìš”ì¸ ë³´ì •ë¨")
            else:
                print(f"[INFO] ë³´ì • í•„ìš” ì—†ìŒ")
            
        except Exception as e:
            print(f"[ERROR] LLM ì ìˆ˜ ë³´ì • ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            correction_log = []

        # 5) LLM ê¸°ë°˜ í•´ì„(explanation) - ë³´ì •ëœ ì ìˆ˜ ê¸°ì¤€ (ê°„ëµí™”)
        try:
            # ëŒ€í‘œ ì ìˆ˜ë§Œ ì¶”ì¶œ (ìƒìœ„ 3ê°œ + í•˜ìœ„ 2ê°œ)
            flat_scores = [(f"{mc}/{sc}", v) for mc, subs in scores.items() for sc, v in subs.items()]
            flat_scores.sort(key=lambda x: x[1], reverse=True)
            top_factors = flat_scores[:3]
            low_factors = flat_scores[-2:]
            
            explanation_prompt = f"""
ì•„ë˜ ì ìˆ˜ì—ì„œ ìƒìœ„/í•˜ìœ„ ìš”ì¸ì˜ ì´ìœ ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
ìƒìœ„: {", ".join([f"{f}({v:.2f})" for f, v in top_factors])}
í•˜ìœ„: {", ".join([f"{f}({v:.2f})" for f, v in low_factors])}
ë¦¬ë·°: {sample_reviews[:500]}
"""
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": explanation_prompt}],
                temperature=0.2,
                max_tokens=300,  # í† í° ì œí•œ
            )
            explanation = resp.choices[0].message.content.strip()
        except Exception as e:
            explanation = f"LLM í•´ì„ ì‹¤íŒ¨: {e}"

        # 6) ê²°ê³¼ ì €ì¥
        place_infos.append({
            'name': place.get('name', 'ì´ë¦„ ì—†ìŒ'), 
            'summary': summary,
            'address': place.get('formatted_address', place.get('vicinity', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')),
            'scores': scores, 
            'geometry': place.get('geometry', {}), 
            'place_id': place.get('place_id', ''),
            'positive_keywords': positive_keywords, 
            'negative_keywords': negative_keywords, 
            'explanation': explanation,
            'corrections': correction_log,  # ë³´ì • ë‚´ì—­ ì¶”ê°€
        })

    state.places = place_infos
    return state.dict()

# ----------------------------------------------------
# 5. LangGraph êµ¬ì„±
# ----------------------------------------------------

graph = StateGraph(AgentState)
graph.add_node("search_places", search_places)
graph.add_node("analyze_reviews", analyze_reviews)
graph.set_entry_point("search_places")
graph.add_edge("search_places", "analyze_reviews")
graph.add_edge("analyze_reviews", END)
agent = graph.compile()


# ----------------------------------------------------
# 6. ì‹¤í—˜ìš© ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ í•¨ìˆ˜
# ----------------------------------------------------

# ì„œìš¸ì‹œ 25ê°œ êµ¬ ëª©ë¡
SEOUL_DISTRICTS = [
    "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ìš©ì‚°êµ¬", "ì„±ë™êµ¬", "ê´‘ì§„êµ¬", 
    "ë™ëŒ€ë¬¸êµ¬", "ì¤‘ë‘êµ¬", "ì„±ë¶êµ¬", "ê°•ë¶êµ¬", "ë„ë´‰êµ¬",
    "ë…¸ì›êµ¬", "ì€í‰êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ë§ˆí¬êµ¬", "ì–‘ì²œêµ¬",
    "ê°•ì„œêµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬", "ì˜ë“±í¬êµ¬", "ë™ì‘êµ¬",
    "ê´€ì•…êµ¬", "ì„œì´ˆêµ¬", "ê°•ë‚¨êµ¬", "ì†¡íŒŒêµ¬", "ê°•ë™êµ¬"
]

# ì„œìš¸ì‹œ ì¤‘ì‹¬ ì¢Œí‘œ
SEOUL_CENTER = (37.5665, 126.9780)

def collect_cafes_in_district(district: str, max_results: int = 50) -> List[Dict]:
    """
    íŠ¹ì • êµ¬ì˜ ì¹´í˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    try:
        query = f"ì„œìš¸íŠ¹ë³„ì‹œ {district} ì¹´í˜"
        # Places APIë¡œ ê²€ìƒ‰
        results = gmaps.places(
            query=query,
            language="ko",
            type="cafe"
        ).get('results', [])
        
        # place_id, ì´ë¦„, ìœ„ì¹˜, ì£¼ì†Œ ë“± ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        cafes = []
        for place in results[:max_results]:
            if place.get('geometry') and place.get('place_id'):
                cafes.append({
                    'place_id': place['place_id'],
                    'name': place.get('name', ''),
                    'lat': place['geometry']['location']['lat'],
                    'lng': place['geometry']['location']['lng'],
                    'address': place.get('formatted_address', place.get('vicinity', '')),
                    'district': district,
                    'rating': place.get('rating', None),
                    'user_ratings_total': place.get('user_ratings_total', 0)
                })
        
        print(f"[INFO] {district}: {len(cafes)}ê°œ ì¹´í˜ ìˆ˜ì§‘")
        return cafes
    
    except Exception as e:
        print(f"[ERROR] {district} ì¹´í˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

@st.cache_data(ttl=3600*24, show_spinner="ì„œìš¸ ì „ì—­ ì¹´í˜ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
def collect_all_cafes_seoul(_gmaps_client, max_per_district: int = 30) -> pd.DataFrame:
    """
    ì„œìš¸ ì „ì²´ 25ê°œ êµ¬ì˜ ì¹´í˜ ë°ì´í„°ë¥¼ ë³‘ë ¬ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    all_cafes = []
    
    # ìˆœì°¨ ìˆ˜ì§‘ (API quota ê³ ë ¤)
    for district in SEOUL_DISTRICTS:
        cafes = collect_cafes_in_district(district, max_per_district)
        all_cafes.extend(cafes)
        time.sleep(0.5)  # API rate limit ë°©ì§€
    
    df = pd.DataFrame(all_cafes)
    
    # ì¤‘ë³µ ì œê±° (place_id ê¸°ì¤€)
    if not df.empty:
        df = df.drop_duplicates(subset=['place_id']).reset_index(drop=True)
        print(f"[INFO] ì´ {len(df)}ê°œ ì¹´í˜ ìˆ˜ì§‘ ì™„ë£Œ")
    
    return df

def calculate_transit_accessibility(lat: float, lng: float, max_distance: int = 600) -> Tuple[float, str, str]:
    """
    íŠ¹ì • ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì§€í•˜ì² ì—­/ë²„ìŠ¤ì •ë¥˜ì¥ê¹Œì§€ì˜ ë„ë³´ ì‹œê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Returns:
        (ë„ë³´_ë¶„, ìµœê·¼ì ‘_ì—­ëª…, íƒ€ì…)
    """
    try:
        print(f"[DEBUG] ì ‘ê·¼ì„± ê³„ì‚° ì‹œì‘: lat={lat}, lng={lng}")
        
        # ë°˜ê²½ 600m ë‚´ ì§€í•˜ì² ì—­ ê²€ìƒ‰
        try:
            subway_results = gmaps.places_nearby(
                location=(lat, lng),
                radius=max_distance,
                type='subway_station',
                language='ko'
            ).get('results', [])
            print(f"[DEBUG] ì§€í•˜ì² ì—­ ê²€ìƒ‰ ê²°ê³¼: {len(subway_results)}ê°œ")
        except Exception as e:
            print(f"[ERROR] ì§€í•˜ì² ì—­ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            subway_results = []
        
        # ë°˜ê²½ 600m ë‚´ ë²„ìŠ¤ ì •ë¥˜ì¥ ê²€ìƒ‰
        try:
            bus_results = gmaps.places_nearby(
                location=(lat, lng),
                radius=max_distance,
                type='bus_station',
                language='ko'
            ).get('results', [])
            print(f"[DEBUG] ë²„ìŠ¤ì •ë¥˜ì¥ ê²€ìƒ‰ ê²°ê³¼: {len(bus_results)}ê°œ")
        except Exception as e:
            print(f"[ERROR] ë²„ìŠ¤ì •ë¥˜ì¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            bus_results = []
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¡°ê¸° ë°˜í™˜
        if not subway_results and not bus_results:
            print(f"[WARN] 600m ë‚´ ì—­/ì •ë¥˜ì¥ ì—†ìŒ")
            return None, "ì •ë³´ ì—†ìŒ", "ì—†ìŒ"
        
        # ê°€ì¥ ê°€ê¹Œìš´ ì—­/ì •ë¥˜ì¥ ì°¾ê¸°
        min_walk_time = 999
        nearest_name = "ì •ë³´ ì—†ìŒ"
        nearest_type = "ì—†ìŒ"
        distance_matrix_success = False
        
        # ì§€í•˜ì² ì—­ ì²˜ë¦¬
        for idx, station in enumerate(subway_results[:3]):  # ìƒìœ„ 3ê°œë§Œ ê²€ì‚¬
            try:
                station_loc = station['geometry']['location']
                station_name = station.get('name', 'ì§€í•˜ì² ì—­')
                print(f"[DEBUG] [{idx+1}/3] ì§€í•˜ì² ì—­ ë„ë³´ ì‹œê°„ ê³„ì‚°: {station_name}")
                
                # Distance Matrix APIë¡œ ë„ë³´ ì‹œê°„ ê³„ì‚° ì‹œë„
                try:
                    result = gmaps.distance_matrix(
                        origins=[(station_loc['lat'], station_loc['lng'])],  # ì¶œë°œ: ì—­/ì •ë¥˜ì¥
                        destinations=[(lat, lng)],  # ë„ì°©: ì¹´í˜
                        mode='walking',
                        language='ko',
                        region='kr'  # í•œêµ­ ì§€ì—­ ëª…ì‹œ
                    )
                    
                    status = result['rows'][0]['elements'][0]['status']
                    print(f"[DEBUG] Distance Matrix ì‘ë‹µ status: {status}")
                    
                    if status == 'OK':
                        duration = result['rows'][0]['elements'][0]['duration']['value'] / 60  # ì´ˆ -> ë¶„
                        distance = result['rows'][0]['elements'][0]['distance']['value']  # ë¯¸í„°
                        print(f"[DEBUG] âœ“ {station_name}: {duration:.1f}ë¶„ ({distance}m)")
                        distance_matrix_success = True
                        
                        if duration < min_walk_time:
                            min_walk_time = duration
                            nearest_name = station_name
                            nearest_type = 'ì§€í•˜ì² ì—­'
                        continue  # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ fallback ë¶ˆí•„ìš”
                    else:
                        print(f"[WARN] {station_name}: Distance Matrix ìƒíƒœ - {status}, ì§ì„ ê±°ë¦¬ë¡œ ëŒ€ì²´")
                except Exception as e:
                    print(f"[WARN] Distance Matrix API ì˜¤ë¥˜: {e}, ì§ì„ ê±°ë¦¬ë¡œ ëŒ€ì²´")
                
                # ZERO_RESULTS ë˜ëŠ” ì˜¤ë¥˜ ì‹œ ì§ì„  ê±°ë¦¬ë¡œ fallback
                from math import radians, cos, sin, asin, sqrt
                
                def haversine(lat1, lon1, lat2, lon2):
                    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
                    dlon = lon2 - lon1
                    dlat = lat2 - lat1
                    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
                    c = 2 * asin(sqrt(a))
                    return 6371 * c * 1000  # km to meters
                
                distance_m = haversine(lat, lng, station_loc['lat'], station_loc['lng'])
                # ì§ì„ ê±°ë¦¬ì— ìš°íšŒê³„ìˆ˜ ì ìš© (ë„ì‹œ ì§€ì—­ ì‹¤ì œ ë„ë³´ ê²½ë¡œëŠ” ì§ì„ ì˜ ì•½ 1.4ë°°)
                actual_distance_m = distance_m * 1.4
                # í‰ê·  ë„ë³´ ì†ë„: 67m/ë¶„ (4km/h)
                duration = actual_distance_m / 67.0
                print(f"[DEBUG] âš  {station_name}: {duration:.1f}ë¶„ (ì§ì„  {distance_m:.0f}m â†’ ì‹¤ì œê²½ë¡œ ì¶”ì • {actual_distance_m:.0f}m)")
                distance_matrix_success = True  # fallbackë„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                
                if duration < min_walk_time:
                    min_walk_time = duration
                    nearest_name = station_name
                    nearest_type = 'ì§€í•˜ì² ì—­'
                    
            except Exception as e:
                print(f"[ERROR] ì§€í•˜ì² ì—­ '{station_name}' ì²˜ë¦¬ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
        
        # ë²„ìŠ¤ì •ë¥˜ì¥ ì²˜ë¦¬
        for idx, bus in enumerate(bus_results[:3]):
            try:
                bus_loc = bus['geometry']['location']
                bus_name = bus.get('name', 'ë²„ìŠ¤ì •ë¥˜ì¥')
                print(f"[DEBUG] [{idx+1}/3] ë²„ìŠ¤ì •ë¥˜ì¥ ë„ë³´ ì‹œê°„ ê³„ì‚°: {bus_name}")
                
                # Distance Matrix APIë¡œ ë„ë³´ ì‹œê°„ ê³„ì‚° ì‹œë„
                try:
                    result = gmaps.distance_matrix(
                        origins=[(bus_loc['lat'], bus_loc['lng'])],  # ì¶œë°œ: ì—­/ì •ë¥˜ì¥
                        destinations=[(lat, lng)],  # ë„ì°©: ì¹´í˜
                        mode='walking',
                        language='ko',
                        region='kr'  # í•œêµ­ ì§€ì—­ ëª…ì‹œ
                    )
                    
                    status = result['rows'][0]['elements'][0]['status']
                    print(f"[DEBUG] Distance Matrix ì‘ë‹µ status: {status}")
                    
                    if status == 'OK':
                        duration = result['rows'][0]['elements'][0]['duration']['value'] / 60
                        distance = result['rows'][0]['elements'][0]['distance']['value']
                        print(f"[DEBUG] âœ“ {bus_name}: {duration:.1f}ë¶„ ({distance}m)")
                        distance_matrix_success = True
                        
                        if duration < min_walk_time:
                            min_walk_time = duration
                            nearest_name = bus_name
                            nearest_type = 'ë²„ìŠ¤ì •ë¥˜ì¥'
                        continue  # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ fallback ë¶ˆí•„ìš”
                    else:
                        print(f"[WARN] {bus_name}: Distance Matrix ìƒíƒœ - {status}, ì§ì„ ê±°ë¦¬ë¡œ ëŒ€ì²´")
                except Exception as e:
                    print(f"[WARN] Distance Matrix API ì˜¤ë¥˜: {e}, ì§ì„ ê±°ë¦¬ë¡œ ëŒ€ì²´")
                
                # ZERO_RESULTS ë˜ëŠ” ì˜¤ë¥˜ ì‹œ ì§ì„  ê±°ë¦¬ë¡œ fallback
                from math import radians, cos, sin, asin, sqrt
                
                def haversine(lat1, lon1, lat2, lon2):
                    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
                    dlon = lon2 - lon1
                    dlat = lat2 - lat1
                    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
                    c = 2 * asin(sqrt(a))
                    return 6371 * c * 1000  # km to meters
                
                distance_m = haversine(lat, lng, bus_loc['lat'], bus_loc['lng'])
                # ì§ì„ ê±°ë¦¬ì— ìš°íšŒê³„ìˆ˜ ì ìš© (ë„ì‹œ ì§€ì—­ ì‹¤ì œ ë„ë³´ ê²½ë¡œëŠ” ì§ì„ ì˜ ì•½ 1.4ë°°)
                actual_distance_m = distance_m * 1.4
                # í‰ê·  ë„ë³´ ì†ë„: 67m/ë¶„ (4km/h)
                duration = actual_distance_m / 67.0
                print(f"[DEBUG] âš  {bus_name}: {duration:.1f}ë¶„ (ì§ì„  {distance_m:.0f}m â†’ ì‹¤ì œê²½ë¡œ ì¶”ì • {actual_distance_m:.0f}m)")
                distance_matrix_success = True  # fallbackë„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
                
                if duration < min_walk_time:
                    min_walk_time = duration
                    nearest_name = bus_name
                    nearest_type = 'ë²„ìŠ¤ì •ë¥˜ì¥'
                    
            except Exception as e:
                print(f"[ERROR] ë²„ìŠ¤ì •ë¥˜ì¥ '{bus_name}' ì²˜ë¦¬ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                import traceback
                traceback.print_exc()
        
        # Distance Matrix APIê°€ í•œ ë²ˆë„ ì„±ê³µí•˜ì§€ ëª»í–ˆë‹¤ë©´
        if not distance_matrix_success:
            print(f"[ERROR] Distance Matrix API í˜¸ì¶œì´ ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            print(f"[INFO] ê°€ëŠ¥í•œ ì›ì¸:")
            print(f"  1. Distance Matrix APIê°€ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
            print(f"  2. API í‚¤ì— Distance Matrix API ê¶Œí•œì´ ì—†ìŒ")
            print(f"  3. Billingì´ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
            print(f"  4. API quota ì´ˆê³¼")
            return None, "Distance Matrix ì‹¤íŒ¨", "ì˜¤ë¥˜"
        
        # ê²°ê³¼ ë°˜í™˜
        if min_walk_time < 999:
            print(f"[SUCCESS] ìµœê·¼ì ‘: {nearest_name} ({nearest_type}), ë„ë³´ {min_walk_time:.1f}ë¶„")
            return round(min_walk_time, 1), nearest_name, nearest_type
        else:
            print(f"[WARN] Distance Matrix í˜¸ì¶œì€ ì„±ê³µí–ˆì§€ë§Œ ìœ íš¨í•œ ê²½ë¡œë¥¼ ì°¾ì§€ ëª»í•¨")
            return None, "ê²½ë¡œ ì—†ìŒ", "ì—†ìŒ"
    
    except Exception as e:
        print(f"[CRITICAL ERROR] ì ‘ê·¼ì„± ê³„ì‚° ì¹˜ëª…ì  ì˜¤ë¥˜: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return None, "ì¹˜ëª…ì  ì˜¤ë¥˜", "ì˜¤ë¥˜"

def calculate_placeness_batch(df: pd.DataFrame, sample_size: int = None, progress_callback=None) -> pd.DataFrame:
    """
    ì¹´í˜ ë°ì´í„°í”„ë ˆì„ì— ëŒ€í•´ ì¥ì†Œì„± ì ìˆ˜ë¥¼ ì¼ê´„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if sample_size and len(df) > sample_size:
        df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)
    
    results = []
    total = len(df)
    
    for idx, row in df.iterrows():
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if progress_callback:
            progress_callback(idx + 1, total, row.get('name', '?'))
        try:
            # Place Details APIë¡œ ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
            details = gmaps.place(place_id=row['place_id'], language='ko').get('result', {})
            reviews = details.get('reviews', [])[:10]
            review_texts = [r['text'] for r in reviews if r.get('text')]
            
            if not review_texts:
                # ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                continue
            
            # ë¦¬ë·° í…ìŠ¤íŠ¸ ë³‘í•©
            review_text = "\n".join(review_texts)
            
            # ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬
            review_units = cached_semantic_split(review_text)
            if not review_units:
                continue
            
            # SBERT + ê°ì„± ëª¨ë¸ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (analyze_reviews ë¡œì§ ì¬ì‚¬ìš©)
            factor_sentiments = {f: [] for f in category_embeddings.keys()}
            
            sentiment_scores = sentiment_model(review_units)
            unit_embs = embed_model.encode(review_units, normalize_embeddings=True)
            subcat_list = list(category_embeddings.keys())
            factor_mat = np.stack([category_embeddings[s] for s in subcat_list], axis=0)
            sim_mat = np.matmul(unit_embs, factor_mat.T)
            
            ALPHA, BETA = 0.75, 0.25
            
            for i, unit in enumerate(review_units):
                raw_sent = float(sentiment_scores[i]) if i < len(sentiment_scores) else 0.5
                sent_adj = np.clip((raw_sent - 0.3) / 0.7, 0, 1)
                sims = sim_mat[i]
                for j, sim in enumerate(sims):
                    sim_adj = np.clip((float(sim) - 0.3) / 0.5, 0, 1)
                    if sim_adj > 0:
                        f_name = subcat_list[j]
                        combined = ALPHA * sim_adj + BETA * sent_adj
                        score_scaled = 1 / (1 + np.exp(-2.2 * (combined - 0.4)))
                        factor_sentiments[f_name].append(float(score_scaled))
            
            # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… (ì ‘ê·¼ì„± í¬í•¨)
            keyword_boosts = {
                "ê³ ìœ ì„±": ["ë…íŠ¹", "ìœ ë‹ˆí¬", "ì°¨ë³„", "ì»¨ì…‰", "í…Œë§ˆ", "íŠ¹ìƒ‰", "ê°œì„±", "ìœ ì¼", "ë…ì°½"],
                "ë¬¸í™”ì  ë§¥ë½": ["ì „í†µ", "ì—­ì‚¬", "ë…„", "ì˜¤ë˜", "ì˜›", "ê³ í’", "ë¬¸í™”", "ë°°ê²½", "ìŠ¤í† ë¦¬", "ì„¸ì›”", "ë‚´ë ¥", "ìœ ì„œ", "ë ˆíŠ¸ë¡œ", "ë¹ˆí‹°ì§€", "í´ë˜ì‹", "ì•¤í‹°í¬", "ê³¼ê±°", "ì˜›ë‚ "],
                "ì§€ì—­ ì •ì²´ì„±": ["ì§€ì—­", "ë™ë„¤", "ë§ˆì„", "ê·¼ì²˜", "ì£¼ë³€", "ëª…ì†Œ", "ëœë“œë§ˆí¬", "ìƒì§•", "ëŒ€í‘œ", "ì‹ ì´Œ", "í™ëŒ€", "ê°•ë‚¨", "ì´íƒœì›", "ì—°ë‚¨", "ì„±ìˆ˜", "ì„ì§€ë¡œ", "ìµì„ ë™", "ë¶ì´Œ", "ì‚¼ì²­ë™", "ì¢…ë¡œ", "ëª…ë™"],
                "ì‹¬ë¯¸ì„±": ["ì˜ˆì˜", "ì•„ë¦„", "ë©‹ì§€", "ì„¸ë ¨", "ì•¼ê²½", "ë·°", "ì¸í…Œë¦¬ì–´", "ë””ìì¸", "ì¡°ëª…", "ì•„ëŠ‘", "ë¶„ìœ„ê¸°", "ê°ì„±"],
                "ì ‘ê·¼ì„±": ["ê°€ê¹", "ì ‘ê·¼", "ì—­", "ì •ë¥˜ì¥", "ë„ë³´", "í¸ë¦¬"],
            }
            
            for factor, keywords in keyword_boosts.items():
                matched_kws = [kw for kw in keywords if kw in review_text]
                if matched_kws:
                    boosted_score = min(0.75 + (len(matched_kws) * 0.05), 0.95)
                    for _ in range(2):
                        factor_sentiments[factor].append(boosted_score)
            
            # ì ‘ê·¼ì„± íŒ¨í„´ ë§¤ì¹­
            accessibility_patterns = [
                r'ë„ë³´\s*(?:ë¡œ\s*)?(\d+)\s*ë¶„',
                r'(\d+)\s*ë¶„\s*ë„ë³´',
                r'(\d+)\s*ë¶„\s*ê±°ë¦¬',
            ]
            
            for pattern in accessibility_patterns:
                matches = re.finditer(pattern, review_text)
                for match in matches:
                    time_str = match.group(1)
                    if time_str:
                        try:
                            time_minutes = int(time_str)
                            if time_minutes <= 5:
                                boost = 0.95
                            elif time_minutes <= 10:
                                boost = 0.90
                            else:
                                boost = 0.85
                            factor_sentiments["ì ‘ê·¼ì„±"].append(boost)
                        except:
                            pass
            
            # ì ìˆ˜ ì •ê·œí™”
            scores = json.loads(json.dumps(new_score_structure_template))
            all_vals = []
            for vals in factor_sentiments.values():
                all_vals.extend(vals)
            
            if all_vals:
                vmin, vmax = float(np.min(all_vals)), float(np.max(all_vals))
            else:
                vmin, vmax = 0.5, 0.5
            
            for main_cat, subcats in scores.items():
                for subcat in subcats.keys():
                    vals = factor_sentiments.get(subcat, [])
                    if vals and vmax > vmin:
                        raw = float(np.mean(vals))
                        normed = 0.30 + 0.70 * ((raw - vmin) / (vmax - vmin + 1e-8))
                        scores[main_cat][subcat] = float(np.clip(normed, 0.30, 1.0))
                    elif vals:
                        scores[main_cat][subcat] = float(np.clip(vals[0], 0.30, 1.0))
                    else:
                        scores[main_cat][subcat] = 0.5
            
            # Overall ì ìˆ˜ ê³„ì‚° (ì „ì²´ í‰ê· )
            all_scores = [s for main_cat, sub_scores in scores.items() for s in sub_scores.values() if s is not None]
            overall_score = np.mean(all_scores) if all_scores else 0.5
            
            # ê²°ê³¼ ì €ì¥
            result = row.to_dict()
            result['overall_score'] = round(overall_score, 3)
            result['accessibility_score'] = round(scores.get('ë¬¼ë¦¬ì  íŠ¹ì„±', {}).get('ì ‘ê·¼ì„±', 0.5), 3)
            result['scores'] = scores
            results.append(result)
            
            print(f"[{idx+1}/{len(df)}] {row['name']}: overall={overall_score:.2f}, ì ‘ê·¼ì„±={result['accessibility_score']:.2f}")
            
        except Exception as e:
            print(f"[ERROR] {row.get('name', '?')} ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            continue
    
    return pd.DataFrame(results)


# ----------------------------------------------------
# 7. Streamlit UI
# ----------------------------------------------------

st.title("ì¥ì†Œì„± ìš”ì¸ ê¸°ë°˜ ê³µê°„ ì •ëŸ‰ í‰ê°€ ë„êµ¬")

# CSSë¡œ í…ìŠ¤íŠ¸ ìƒ‰ìƒ ê°•ì œ (ê°€ë…ì„± ê°œì„ )
st.markdown("""
<style>
    .stMarkdown, .stCaption, p, div {
        color: #000000 !important;
    }
    h1, h2, h3, h4, h5, h6 {
        color: #000000 !important;
    }
    /* ì˜ˆì‹œ í…ìŠ¤íŠ¸ëŠ” íšŒìƒ‰ ìœ ì§€ */
    .example-text {
        color: #888888 !important;
    }
</style>
""", unsafe_allow_html=True)

# íƒ­ êµ¬ì„±
tab1, tab2, tab3 = st.tabs(["ğŸ” ê°œë³„ ì¥ì†Œ ë¶„ì„", "ğŸ—ºï¸ ì„œìš¸ ì „ì—­ ì‹¤í—˜", "ğŸ“Š í‘œë³¸ ë°ì´í„° í™•ì¸"])

# ========================================
# íƒ­ 1: ê°œë³„ ì¥ì†Œ ë¶„ì„ (ê¸°ì¡´ ê¸°ëŠ¥)
# ========================================
with tab1:
    st.markdown("ë¶„ì„í•  ê³µê°„ì˜ ìœ„ì¹˜ì™€ ê°ì„±/ê¸°ëŠ¥ì  íŠ¹ì„±ì„ ì…ë ¥í•˜ì‹­ì‹œì˜¤. "
                 "<span class='example-text'>(ì˜ˆ: ì‹ ì´Œ ì¡°ìš©í•œ ì¹´í˜, ì¢…ë¡œêµ¬ ì „í†µì ì¸ ìŒì‹ì , ë§ˆí¬êµ¬ ì‚°ì±…ë¡œ ê³µì›)</span>", 
                 unsafe_allow_html=True)
    query = st.text_input("", placeholder="ì˜ˆ: ì‹ ì´Œ ì¡°ìš©í•œ ì¹´í˜", key="query_tab1")

    if st.button("ì¥ì†Œì„± ì •ëŸ‰ ë¶„ì„ ì‹œì‘", key="btn_analyze_tab1"):
        if not query.strip():
            st.warning("ì¥ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("NLP ë° LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ë¦¬ë·°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¥ì†Œì„± ìš”ì¸ì„ ì •ëŸ‰ í‰ê°€í•˜ëŠ” ì¤‘..."):
                result = agent.invoke({"query": query, "places": [], "answer": ""})
                places = result.get('places', [])
                st.session_state.history.append((query, places))
                st.rerun()

    # ê²°ê³¼ ì¶œë ¥
    if st.session_state.history:
        latest_query, latest_places = st.session_state.history[-1]
        st.markdown(f"---")
        st.markdown(f"### '{latest_query}'ì— ëŒ€í•œ ì¥ì†Œì„± í‰ê°€ ê²°ê³¼")

        for i, place in enumerate(latest_places):
            with st.container(border=True):
                st.subheader(place.get('name', 'ì´ë¦„ ì •ë³´ ì—†ìŒ'))
                st.markdown(f"**ğŸ“ ì£¼ì†Œ:** {place.get('address', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')}")
                
                # 2ì—´ ë ˆì´ì•„ì›ƒ: ì™¼ìª½(ì‹œê°í™”), ì˜¤ë¥¸ìª½(ë³´ì •/í•´ì„¤)
                col_left, col_right = st.columns([1.2, 1])
                
                scores = place.get('scores')
                
                # ========== ì™¼ìª½ ì—´: ë¦¬ë·° ìš”ì•½ + ì‹œê°í™” ==========
                with col_left:
                    st.markdown(f"**ğŸ“ ë¦¬ë·° ìš”ì•½**")
                    st.markdown(place.get('summary', 'ìš”ì•½ ì •ë³´ ì—†ìŒ'))
                
                with col_left:
                    if scores:
                        st.markdown(f"**ğŸ“Š ì¥ì†Œì„± ì¢…í•© í‰ê°€**")

                        # Sunburst ì°¨íŠ¸ ë°ì´í„° ìƒì„±
                        labels = []
                        parents = []
                        values = []
                        colors = []

                        # ë¶€ë“œëŸ¬ìš´ íŒŒìŠ¤í…”í†¤ ìƒ‰ìƒ ë§µ (factors.json êµ¬ì¡°ì™€ ë™ì¼í•œ ëŒ€ë¶„ë¥˜ ë¼ë²¨)
                        color_map = {
                            "ë¬¼ë¦¬ì  íŠ¹ì„±": "rgb(173, 216, 230)",     # ì—°í•œ íŒŒë€ìƒ‰ (Light Blue)
                            "í™œë™ì  íŠ¹ì„±": "rgb(152, 251, 152)",   # ì—°í•œ ì—°ë‘ìƒ‰ (Light Lime Green)
                            "ì˜ë¯¸ì  íŠ¹ì„±": "rgb(255, 182, 193)" # ì—°í•œ ë¶„í™ìƒ‰ (Light Pink)
                        }

                        # ë£¨íŠ¸ ë…¸ë“œ ì¶”ê°€ (ì „ì²´ ì ìˆ˜ì˜ í‰ê· ìœ¼ë¡œ ì„¤ì •)
                        all_scores = [s for main_cat, sub_scores in scores.items() for s in sub_scores.values() if s is not None]
                        total_score = sum(all_scores)
                        score_count = len(all_scores)
                        root_value = total_score / score_count if score_count > 0 else 0.5

                        labels.append(place['name'])
                        parents.append("")
                        values.append(root_value)
                        colors.append("#FFFFFF")

                        # ëŒ€ë¶„ë¥˜ì™€ ì„¸ë¶€ ë¶„ë¥˜ ì¶”ê°€
                        for main_cat, sub_scores in scores.items():
                            main_scores = [s for s in sub_scores.values() if s is not None]
                            main_avg = sum(main_scores) / len(main_scores) if main_scores else 0
                
                            labels.append(main_cat)
                            parents.append(place['name'])
                            values.append(main_avg)
                            colors.append(color_map.get(main_cat, "#CCCCCC"))
                
                            for sub_cat, score in sub_scores.items():
                                if score is not None:
                                    labels.append(f"{sub_cat}: {score:.2f}") # ì ìˆ˜ë¥¼ ë¼ë²¨ì— í¬í•¨
                                    parents.append(main_cat)
                                    values.append(float(score))
                                    colors.append(color_map.get(main_cat, "#CCCCCC"))
                
                        # Sunburst ì°¨íŠ¸ ìƒì„±
                        try:
                            fig_sunburst = go.Figure(go.Sunburst(
                                labels=labels,
                                parents=parents,
                                values=values,
                                branchvalues="remainder",
                                marker=dict(colors=colors),
                                hovertemplate='<b>%{customdata[0]}</b><br>ì ìˆ˜: %{value:.2f}', # customdataëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ valueë§Œ í‘œì‹œ
                                maxdepth=2,
                                insidetextorientation='radial'
                            ))
                
                            fig_sunburst.update_layout(
                                margin=dict(t=20, l=10, r=10, b=10),
                                height=400,
                                title_text=f"{place['name']} ì¥ì†Œì„± ì¢…í•© í‰ê°€",
                                font=dict(size=12, family="NotoSansKR, sans-serif")
                            )
                
                            st.plotly_chart(fig_sunburst, use_container_width=True, key=f"sunburst_{i}_{place.get('place_id','')}")
                
                        except Exception as e:
                            # Sunburst ì‹¤íŒ¨ ì‹œ Treemap ì‹œë„
                            st.error(f"Sunburst ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                            pass

                        # Radar Chart ìƒì„± í•¨ìˆ˜ ì •ì˜
                        def make_radar_chart(scores_dict, title="ì¥ì†Œì„± ìš”ì¸ íŠ¹ì„± ë¶„í¬"):
                            # ìƒ‰ìƒ ë§¤í•‘ (ëŒ€ë¶„ë¥˜ ê¸°ì¤€)
                            fill_color_map = {
                                "ë¬¼ë¦¬ì  íŠ¹ì„±": "rgba(173, 216, 230, 0.5)",  # ì—°í•œ íŒŒë€ìƒ‰
                                "í™œë™ì  íŠ¹ì„±": "rgba(152, 251, 152, 0.5)",  # ì—°í•œ ì´ˆë¡ìƒ‰
                                "ì˜ë¯¸ì  íŠ¹ì„±": "rgba(255, 182, 193, 0.5)"   # ì—°í•œ ë¶„í™ìƒ‰
                            }
                
                            # ì „ì²´ ìš”ì¸ì„ ìˆœì„œëŒ€ë¡œ ë°°ì¹˜ + ìƒ‰ìƒ ë§¤í•‘
                            categories = []
                            values = []
                            colors = []
                
                            for main_cat, subcats in scores_dict.items():
                                for subcat, val in subcats.items():
                                    categories.append(subcat)
                                    values.append(val if val is not None else 0.5)
                                    colors.append(fill_color_map.get(main_cat, "rgba(200,200,200,0.5)"))
                
                            fig = go.Figure()
                
                            # Barpolarë¡œ ê° ì¶•ë³„ ìƒ‰ìƒ êµ¬ë¶„
                            fig.add_trace(go.Barpolar(
                                r=values,
                                theta=categories,
                                marker=dict(
                                    color=colors,
                                    line=dict(color="rgba(80,80,80,0.3)", width=1)
                                ),
                                hovertemplate='<b>%{theta}</b><br>ì ìˆ˜: %{r:.2f}<extra></extra>',
                                name="ìš”ì¸ë³„ ì ìˆ˜"
                            ))
                
                            # ìœ¤ê³½ì„ ì„ ìœ„í•œ Scatterpolar ì¶”ê°€
                            categories_closed = categories + categories[:1]
                            values_closed = values + values[:1]
                
                            fig.add_trace(go.Scatterpolar(
                                r=values_closed,
                                theta=categories_closed,
                                mode='lines',
                                line=dict(color="rgba(60, 60, 60, 0.8)", width=2.5),
                                showlegend=False,
                                hoverinfo='skip'
                            ))
                
                            fig.update_layout(
                                polar=dict(
                                    radialaxis=dict(
                                        visible=True, 
                                        range=[0, 1], 
                                        tickvals=[0.2, 0.4, 0.6, 0.8, 1.0],
                                        showline=True,
                                        gridcolor="rgba(200,200,200,0.5)"
                                    ),
                                    angularaxis=dict(rotation=90, direction="clockwise")
                                ),
                                showlegend=False,
                                height=580,
                                margin=dict(l=140, r=140, t=110, b=110),  # ì—¬ë°± ìµœëŒ€ í™•ëŒ€
                                title=dict(text=title, x=0.5, font=dict(size=14, family="NotoSansKR"))
                            )
                
                            return fig
                
                        st.markdown(f"**ğŸ“Š ì¥ì†Œì„± ìš”ì¸ íŠ¹ì„± ë¶„í¬ë„**")
                        # Radar Chart ì¶œë ¥
                        fig_radar = make_radar_chart(scores, title=f"{place['name']} ì¥ì†Œì„± íŠ¹ì„± ë¶„í¬")
                    st.plotly_chart(fig_radar, use_container_width=True, key=f"radar_{i}_{place.get('place_id','')}")

            
                # ========== ì˜¤ë¥¸ìª½ ì—´: LLM ë³´ì • + í•´ì„¤ ==========
                with col_right:
                    # LLM ë³´ì • ë‚´ì—­ í‘œì‹œ
                    corrections = place.get('corrections', [])
                    if corrections:
                        st.markdown("**âš™ï¸ LLM ì ìˆ˜ ë³´ì •**")
                        st.caption("GPT-4o ê²€ì¦ ê²°ê³¼")
                
                        correction_df = pd.DataFrame(corrections)
                        correction_df = correction_df.rename(columns={
                            "factor": "ìš”ì¸",
                            "original": "ì›ì ìˆ˜",
                            "adjusted": "ë³´ì •",
                            "delta": "Î”",
                            "reason": "ê·¼ê±°"
                        })
                        st.dataframe(
                            correction_df,
                            use_container_width=True,
                            hide_index=True,
                            column_config={
                                "ìš”ì¸": st.column_config.TextColumn(width="small"),
                                "ì›ì ìˆ˜": st.column_config.NumberColumn(format="%.2f", width="small"),
                                "ë³´ì •": st.column_config.NumberColumn(format="%.2f", width="small"),
                                "Î”": st.column_config.NumberColumn(format="%+.2f", width="small"),
                                "ê·¼ê±°": st.column_config.TextColumn(width="medium"),
                            }
                        )
                    else:
                        st.markdown("**âš™ï¸ LLM ì ìˆ˜ ë³´ì •**")
                        st.caption("ë³´ì • í•„ìš” ì—†ìŒ")
                
                    # ì ìˆ˜ í•´ì„¤ (ë³´ì • í›„ ìµœì¢… ì ìˆ˜ ê¸°ì¤€)
                    if place.get('explanation'):
                        st.markdown("**ğŸ” ìµœì¢… ì ìˆ˜ í•´ì„¤**")
                        st.markdown(place.get('explanation'))
                
                    # ì›Œë“œ í´ë¼ìš°ë“œ ì‹œê°í™” (ì˜¤ë¥¸ìª½ ì—´ í•˜ë‹¨, ì¢Œìš° ë°°ì¹˜)
                    if place.get('positive_keywords') or place.get('negative_keywords'):
                        st.markdown("**ğŸ“ í‚¤ì›Œë“œ ë¶„ì„**")
                
                        wc_col1, wc_col2 = st.columns(2)
                
                        # ê¸ì • ì›Œë“œ í´ë¼ìš°ë“œ
                        if place.get('positive_keywords'):
                            with wc_col1:
                                st.caption("âœ… ê¸ì •")
                                text = " ".join(place['positive_keywords'])
                                if text:
                                    img = generate_wordcloud(text, font_path, colormap="Greens")
                                    if img is not None:
                                        st.image(img, use_container_width=True)
                
                        # ë¶€ì • ì›Œë“œ í´ë¼ìš°ë“œ
                        if place.get('negative_keywords'):
                            with wc_col2:
                                st.caption("âŒ ë¶€ì •")
                                text = " ".join(place['negative_keywords'])
                                if text:
                                    img = generate_wordcloud(text, font_path, colormap="Reds")
                                    if img is not None:
                                        st.image(img, use_container_width=True)
                
                # ì§€ë„ ë° ë¡œë“œë·° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                if place.get('geometry') and place['geometry'].get('location'):
                    lat, lng = place['geometry']['location']['lat'], place['geometry']['location']['lng']
                
                    map_key = f"map_{i}_{place['place_id']}"
                    streetview_key = f"street_{i}_{place['place_id']}"
                
                    if map_key not in st.session_state:
                        st.session_state[map_key] = False
                    if streetview_key not in st.session_state:
                        st.session_state[streetview_key] = False
                
                    col1, col2 = st.columns(2)
                
                    # ë²„íŠ¼ í´ë¦­ ì‹œ ìƒíƒœ í† ê¸€ í›„ ì¬ì‹¤í–‰í•˜ì—¬ ì§€ë„ í‘œì‹œ
                    if col1.button("ğŸ—ºï¸ ì§€ë„ ë³´ê¸°", key=f"btn_{map_key}"):
                        st.session_state[map_key] = not st.session_state[map_key]
                        st.rerun()
                
                    if col2.button("ğŸš— ë¡œë“œë·° ë³´ê¸°", key=f"btn_{streetview_key}"):
                        st.session_state[streetview_key] = not st.session_state[streetview_key]
                        st.rerun()
                
                    if st.session_state[map_key] or st.session_state[streetview_key]:
                        st.markdown("**ğŸ“ ìœ„ì¹˜ ì •ë³´**")
                
                        map_col1, map_col2 = st.columns(2)
                
                        if st.session_state[map_key]:
                            with map_col1:
                                st.markdown("**ğŸ—ºï¸ ì§€ë„**")
                                # Google Maps Embed API (ì „ì²´ í­ ì‚¬ìš©)
                                map_url = f"https://www.google.com/maps/embed/v1/place?key={st.session_state.gmaps_key}&q={lat},{lng}"
                                st.markdown(
                                    f'<iframe src="{map_url}" width="100%" height="450" style="border:0;" allowfullscreen="" loading="lazy"></iframe>',
                                    unsafe_allow_html=True
                                )
                
                        if st.session_state[streetview_key]:
                            with map_col2:
                                st.markdown("**ğŸš— ë¡œë“œë·°**")
                                # Google Maps Street View Embed API (ì „ì²´ í­ ì‚¬ìš©)
                                streetview_url = f"https://www.google.com/maps/embed/v1/streetview?key={st.session_state.gmaps_key}&location={lat},{lng}"
                                st.markdown(
                                    f'<iframe src="{streetview_url}" width="100%" height="450" style="border:0;" allowfullscreen="" loading="lazy"></iframe>',
                                    unsafe_allow_html=True
                                )
                else:
                    st.info("ğŸ“ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ì–´ ì§€ë„ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ========================================
# íƒ­ 2: ì„œìš¸ ì „ì—­ ì‹¤í—˜ (ë…¼ë¬¸ìš© ë°ì´í„° ìˆ˜ì§‘ ë° ê²€ì¦)
# ========================================
with tab2:
    st.markdown("### ğŸ—ºï¸ ì„œìš¸ ì¹´í˜ ì¥ì†Œì„± ì‹¤í—˜ ë° ê²€ì¦")
    st.markdown("""
    ì´ íƒ­ì—ì„œëŠ” ì„œìš¸ ì „ì—­ì˜ ì¹´í˜ë¥¼ ëŒ€ìƒìœ¼ë¡œ ëŒ€ê·œëª¨ ì¥ì†Œì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³ ,
    ì ‘ê·¼ì„± ì ìˆ˜ì™€ ì‹¤ì œ ëŒ€ì¤‘êµí†µ ë„ë³´ ì‹œê°„ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
    """)
    
    # ë°ì´í„° ìˆ˜ì§‘ ì˜µì…˜
    st.subheader("1ï¸âƒ£ ë°ì´í„° ìˆ˜ì§‘")
    
    col_config1, col_config2 = st.columns(2)
    with col_config1:
        sample_per_district = st.slider(
            "êµ¬ë‹¹ ìµœëŒ€ ì¹´í˜ ìˆ˜",
            min_value=10,
            max_value=50,
            value=20,
            help="ê° êµ¬ì—ì„œ ìˆ˜ì§‘í•  ìµœëŒ€ ì¹´í˜ ê°œìˆ˜"
        )
    
    with col_config2:
        score_sample_size = st.slider(
            "ì ìˆ˜ ê³„ì‚° ìƒ˜í”Œ ìˆ˜",
            min_value=10,
            max_value=300,
            value=30,
            help="ì¥ì†Œì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•  ì¹´í˜ ìƒ˜í”Œ ìˆ˜ (API quota ê³ ë ¤)"
        )
        st.caption(f"â±ï¸ ì˜ˆìƒ ì‹œê°„: ì•½ {score_sample_size * 30 / 60:.0f}ë¶„ (ì¹´í˜ë‹¹ ~30ì´ˆ)")
        if score_sample_size > 50:
            st.warning("âš ï¸ 50ê°œ ì´ìƒì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤ (25ë¶„+)")
    
    if st.button("ğŸ”„ ì„œìš¸ ì „ì—­ ì¹´í˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", key="btn_collect_cafes"):
        with st.spinner(f"ì„œìš¸ 25ê°œ êµ¬ì—ì„œ ì¹´í˜ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (êµ¬ë‹¹ ìµœëŒ€ {sample_per_district}ê°œ)"):
            # ì¹´í˜ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            cafes_df = collect_all_cafes_seoul(gmaps, max_per_district=sample_per_district)
            
            if not cafes_df.empty:
                st.session_state['cafes_df'] = cafes_df
                st.success(f"âœ… ì´ {len(cafes_df)}ê°œ ì¹´í˜ ìˆ˜ì§‘ ì™„ë£Œ!")
                
                # ê°„ë‹¨í•œ í†µê³„
                district_counts = cafes_df['district'].value_counts()
                st.dataframe(district_counts.head(10), use_container_width=True)
            else:
                st.error("ì¹´í˜ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
    
    # ì¥ì†Œì„± ì ìˆ˜ ê³„ì‚°
    if 'cafes_df' in st.session_state and not st.session_state['cafes_df'].empty:
        st.markdown("---")
        st.subheader("2ï¸âƒ£ ì¥ì†Œì„± ì ìˆ˜ ê³„ì‚°")
        
        if st.button("ğŸ“Š ì¥ì†Œì„± ì ìˆ˜ ì¼ê´„ ê³„ì‚°", key="btn_calc_scores"):
            cafes_df = st.session_state['cafes_df']
            
            # ì˜ˆìƒ ì‹œê°„ í‘œì‹œ
            estimated_time = score_sample_size * 30 / 60  # ì¹´í˜ë‹¹ 30ì´ˆ ê°€ì •
            st.info(f"â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {estimated_time:.1f}ë¶„ (ì¹´í˜ë‹¹ ~30ì´ˆ)")
            
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë° ìƒíƒœ í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            time_text = st.empty()
            
            import time
            start_time = time.time()
            
            def update_progress(current, total, cafe_name):
                progress = current / total
                progress_bar.progress(progress)
                
                elapsed = time.time() - start_time
                avg_time = elapsed / current if current > 0 else 30
                remaining = (total - current) * avg_time
                
                status_text.text(f"ì§„í–‰ ì¤‘: {current}/{total} - {cafe_name}")
                time_text.text(f"â±ï¸ ê²½ê³¼: {elapsed/60:.1f}ë¶„ | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„ | í‰ê· : {avg_time:.1f}ì´ˆ/ì¹´í˜")
            
            scored_df = calculate_placeness_batch(
                cafes_df, 
                sample_size=score_sample_size,
                progress_callback=update_progress
            )
            
            # ì •ë¦¬
            progress_bar.empty()
            status_text.empty()
            time_text.empty()
            
            if not scored_df.empty:
                st.session_state['scored_df'] = scored_df
                total_time = time.time() - start_time
                st.success(f"âœ… {len(scored_df)}ê°œ ì¹´í˜ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„)")
                
                # ê¸°ë³¸ í†µê³„
                st.markdown("**ì ìˆ˜ ë¶„í¬ í†µê³„**")
                stats_df = scored_df[['overall_score', 'accessibility_score']].describe()
                st.dataframe(stats_df, use_container_width=True)
            else:
                st.error("ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨")
    
    # ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„± ê³„ì‚°
    if 'scored_df' in st.session_state and not st.session_state['scored_df'].empty:
        st.markdown("---")
        st.subheader("3ï¸âƒ£ ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„± ë°ì´í„° ìˆ˜ì§‘")
        
        st.info("""
        ğŸ’¡ **ì´ ë‹¨ê³„ëŠ” ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤**
        - ì§€ë„ ë§ˆì»¤ì— **ìµœê·¼ì ‘ ì—­/ì •ë¥˜ì¥, ë„ë³´ ì‹œê°„** ì •ë³´ ì¶”ê°€
        - H1 ê°€ì„¤ ê²€ì¦ (ì ‘ê·¼ì„± ì ìˆ˜ vs ì‹¤ì œ ë„ë³´ ì‹œê°„ ìƒê´€ê´€ê³„)ì— í•„ìš”
        """)
        
        transit_sample = st.number_input(
            "ì ‘ê·¼ì„± ê³„ì‚° ìƒ˜í”Œ ìˆ˜ (Distance Matrix API quota ê³ ë ¤)",
            min_value=10,
            max_value=100,
            value=30,
            help="ìµœê·¼ì ‘ ì—­/ì •ë¥˜ì¥ê¹Œì§€ì˜ ë„ë³´ ì‹œê°„ì„ ê³„ì‚°í•  ì¹´í˜ ìˆ˜"
        )
        st.caption(f"â±ï¸ ì˜ˆìƒ ì‹œê°„: ì•½ {transit_sample * 0.5:.0f}ë¶„ (ì¹´í˜ë‹¹ ~30ì´ˆ)")
        
        if st.button("ğŸš‡ ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„± ê³„ì‚°", key="btn_calc_transit"):
            scored_df = st.session_state['scored_df']
            sample_df = scored_df.sample(n=min(transit_sample, len(scored_df)), random_state=42)
            
            results = []
            error_log = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            error_text = st.empty()
            
            for counter, (idx, row) in enumerate(sample_df.iterrows(), start=1):
                status_text.text(f"ê³„ì‚° ì¤‘: {row['name']} ({counter}/{len(sample_df)})")
                
                try:
                    walk_time, nearest_name, transit_type = calculate_transit_accessibility(
                        row['lat'], row['lng']
                    )
                    
                    results.append({
                        'place_id': row['place_id'],
                        'name': row['name'],
                        'lat': row['lat'],
                        'lng': row['lng'],
                        'district': row['district'],
                        'overall_score': row['overall_score'],
                        'accessibility_score': row['accessibility_score'],
                        'walk_time_minutes': walk_time,
                        'nearest_station': nearest_name,
                        'transit_type': transit_type
                    })
                    
                    if walk_time is None:
                        error_log.append(f"{row['name']}: ì£¼ë³€ì— ì—­/ì •ë¥˜ì¥ ì—†ìŒ")
                    
                except Exception as e:
                    error_log.append(f"{row['name']}: {str(e)}")
                    results.append({
                        'place_id': row['place_id'],
                        'name': row['name'],
                        'lat': row['lat'],
                        'lng': row['lng'],
                        'district': row['district'],
                        'overall_score': row['overall_score'],
                        'accessibility_score': row['accessibility_score'],
                        'walk_time_minutes': None,
                        'nearest_station': 'ì˜¤ë¥˜',
                        'transit_type': 'ì˜¤ë¥˜'
                    })
                
                progress_bar.progress(counter / len(sample_df))
                
                # ì—ëŸ¬ ë¡œê·¸ í‘œì‹œ
                if error_log:
                    error_text.text(f"âš ï¸ ì˜¤ë¥˜: {len(error_log)}ê°œ | ë§ˆì§€ë§‰: {error_log[-1]}")
                
                time.sleep(0.3)  # API rate limit
            
            transit_df = pd.DataFrame(results)
            # None ê°’ í•„í„°ë§ (ì ‘ê·¼ì„± ê³„ì‚° ì‹¤íŒ¨í•œ ê²½ìš°)
            valid_transit_df = transit_df[transit_df['walk_time_minutes'].notna()].reset_index(drop=True)
            
            if not valid_transit_df.empty:
                st.session_state['transit_df'] = valid_transit_df
                st.success(f"âœ… {len(valid_transit_df)}ê°œ ì¹´í˜ ì ‘ê·¼ì„± ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
                
                if error_log:
                    st.warning(f"âš ï¸ {len(error_log)}ê°œ ì¹´í˜ëŠ” ì ‘ê·¼ì„± ê³„ì‚° ì‹¤íŒ¨ (600m ë‚´ ì—­/ì •ë¥˜ì¥ ì—†ìŒ)")
                    with st.expander("ì˜¤ë¥˜ ìƒì„¸ ë³´ê¸°"):
                        for err in error_log:
                            st.text(err)
                
                st.dataframe(valid_transit_df.head(10), use_container_width=True)
            else:
                st.error("âŒ ì ‘ê·¼ì„± ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: ëª¨ë“  ì¹´í˜ì—ì„œ 600m ë‚´ ì—­/ì •ë¥˜ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                st.info("""
                **ê°€ëŠ¥í•œ ì›ì¸:**
                1. Google Maps APIì˜ Places Nearby APIê°€ ë¹„í™œì„±í™”ë¨
                2. API í‚¤ì˜ quota ì´ˆê³¼
                3. ì„ íƒëœ ì¹´í˜ë“¤ì´ ëŒ€ì¤‘êµí†µì—ì„œ ë„ˆë¬´ ë©€ë¦¬ ìœ„ì¹˜
                
                **í•´ê²° ë°©ë²•:**
                - Google Cloud Consoleì—ì„œ Places API, Distance Matrix API í™œì„±í™” í™•ì¸
                - ë‹¤ë¥¸ êµ¬ì—­ì˜ ì¹´í˜ë¡œ ì¬ì‹œë„
                """)
                
                if error_log:
                    with st.expander("ì˜¤ë¥˜ ìƒì„¸ ë³´ê¸°"):
                        for err in error_log:
                            st.text(err)
            
            status_text.empty()
            progress_bar.empty()
            error_text.empty()
    
    # ì‹œê°í™” ë° ë¶„ì„
    if 'scored_df' in st.session_state and not st.session_state['scored_df'].empty:
        st.markdown("---")
        st.subheader("4ï¸âƒ£ ì‹œê°í™” ë° ë¶„ì„")
        
        scored_df = st.session_state['scored_df']
        
        # E1: êµ¬ ë‹¨ìœ„ Choropleth
        st.markdown("**E1: êµ¬ ë‹¨ìœ„ ì¥ì†Œì„± ê°•ë„**")
        district_scores = scored_df.groupby('district')['overall_score'].mean().sort_values(ascending=False)
        
        # Choropleth ì§€ë„
        st.markdown("**í–‰ì •êµ¬ë³„ ì¥ì†Œì„± íˆíŠ¸ë§µ**")
        
        # GeoJSON URL (ì„œìš¸ì‹œ êµ¬ ê²½ê³„)
        seoul_geo_url = "https://raw.githubusercontent.com/southkorea/seoul-maps/master/kostat/2013/json/seoul_municipalities_geo_simple.json"
        
        try:
            import requests
            geo_response = requests.get(seoul_geo_url)
            seoul_geo = geo_response.json()
            
            # Folium Choropleth ì§€ë„ ìƒì„±
            m_choropleth = folium.Map(location=SEOUL_CENTER, zoom_start=11, tiles='OpenStreetMap')
            
            # district_scoresë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            choropleth_data = district_scores.reset_index()
            choropleth_data.columns = ['district', 'score']
            
            # Choropleth ë ˆì´ì–´ ì¶”ê°€
            folium.Choropleth(
                geo_data=seoul_geo,
                name='choropleth',
                data=choropleth_data,
                columns=['district', 'score'],
                key_on='feature.properties.name',
                fill_color='YlOrRd',
                fill_opacity=0.7,
                line_opacity=0.2,
                legend_name='í‰ê·  ì¥ì†Œì„± ì ìˆ˜',
                highlight=True
            ).add_to(m_choropleth)
            
            # êµ¬ë³„ í‰ê·  ì ìˆ˜ í‘œì‹œ (íˆ´íŒ)
            style_function = lambda x: {'fillColor': '#ffffff', 'color':'#000000', 'fillOpacity': 0.1, 'weight': 0.1}
            highlight_function = lambda x: {'fillColor': '#000000', 'color':'#000000', 'fillOpacity': 0.50, 'weight': 0.1}
            
            folium.GeoJson(
                seoul_geo,
                style_function=style_function,
                control=False,
                highlight_function=highlight_function,
                tooltip=folium.features.GeoJsonTooltip(
                    fields=['name'],
                    aliases=['êµ¬:'],
                    style=("background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;")
                )
            ).add_to(m_choropleth)
            
            folium.LayerControl().add_to(m_choropleth)
            st_folium(m_choropleth, width=None, height=500)
            
        except Exception as e:
            st.error(f"Choropleth ì§€ë„ ìƒì„± ì‹¤íŒ¨: {e}")
            # ë§‰ëŒ€ ì°¨íŠ¸ë¡œ ëŒ€ì²´
            fig_district = go.Figure(go.Bar(
                x=district_scores.values,
                y=district_scores.index,
                orientation='h',
                marker=dict(color=district_scores.values, colorscale='Viridis', showscale=True),
                text=[f"{v:.2f}" for v in district_scores.values],
                textposition='auto'
            ))
            fig_district.update_layout(
                title="ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ í‰ê·  ì¥ì†Œì„± ì ìˆ˜",
                xaxis_title="í‰ê·  ì¥ì†Œì„± ì ìˆ˜",
                yaxis_title="ìì¹˜êµ¬",
                height=600,
                showlegend=False
            )
            st.plotly_chart(fig_district, use_container_width=True)
        
        # E2: í¬ì¸íŠ¸ íˆíŠ¸ë§µ (Folium)
        st.markdown("**E2: ì¹´í˜ ìœ„ì¹˜ ë° ì¥ì†Œì„± íˆíŠ¸ë§µ**")
        
        # transit_dfê°€ ìˆìœ¼ë©´ ëŒ€ì¤‘êµí†µ ì •ë³´ í¬í•¨, ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë³´ë§Œ
        use_transit_data = 'transit_df' in st.session_state and not st.session_state['transit_df'].empty
        
        if use_transit_data:
            st.info("âœ… ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„± ì •ë³´ í¬í•¨")
            display_df = st.session_state['transit_df']
        else:
            st.info("â„¹ï¸ ê¸°ë³¸ ì •ë³´ë§Œ í‘œì‹œ (ëŒ€ì¤‘êµí†µ ì •ë³´ëŠ” 3ë‹¨ê³„ì—ì„œ ê³„ì‚° ê°€ëŠ¥)")
            display_df = scored_df
        
        # Folium ì§€ë„ ìƒì„±
        m = folium.Map(location=SEOUL_CENTER, zoom_start=11, tiles='OpenStreetMap')
        
        # íˆíŠ¸ë§µ ë°ì´í„° ì¤€ë¹„
        heat_data = [[row['lat'], row['lng'], row['overall_score']] 
                     for _, row in display_df.iterrows() if row['overall_score'] > 0]
        
        HeatMap(heat_data, radius=15, blur=25, max_zoom=13).add_to(m)
        
        # ë§ˆì»¤ í´ëŸ¬ìŠ¤í„°
        marker_cluster = MarkerCluster().add_to(m)
        
        for _, row in display_df.head(100).iterrows():
            has_transit_info = (use_transit_data and 'walk_time_minutes' in row and 
                               row.get('walk_time_minutes') is not None and pd.notna(row.get('walk_time_minutes')))
            
            if has_transit_info:
                popup_html = f"""
                <div style="font-family: NotoSansKR, sans-serif; min-width: 200px;">
                    <h4 style="margin: 0 0 10px 0; color: #1f77b4;">{row['name']}</h4>
                    <hr style="margin: 5px 0;">
                    <b>ğŸ“Š ì¥ì†Œì„± ì ìˆ˜</b>
                    <ul style="margin: 5px 0; padding-left: 20px;">
                        <li>ì „ì²´: <b>{row['overall_score']:.2f}</b></li>
                        <li>ì ‘ê·¼ì„±: <b>{row['accessibility_score']:.2f}</b></li>
                    </ul>
                    <b>ğŸš‡ ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„±</b>
                    <ul style="margin: 5px 0; padding-left: 20px;">
                        <li>ìµœê·¼ì ‘: <b>{row['nearest_station']}</b></li>
                        <li>ë„ë³´: <b style="color: {'green' if row['walk_time_minutes'] <= 5 else 'orange' if row['walk_time_minutes'] <= 10 else 'red'};">{row['walk_time_minutes']:.1f}ë¶„</b></li>
                        <li>ìœ í˜•: {row['transit_type']}</li>
                    </ul>
                    <b>ğŸ“ ìœ„ì¹˜</b><br>
                    <span style="color: #666;">{row['district']}</span>
                </div>
                """
                marker_color = 'green' if row['walk_time_minutes'] <= 5 else 'blue' if row['walk_time_minutes'] <= 10 else 'orange'
            else:
                popup_html = f"""
                <div style="font-family: NotoSansKR, sans-serif; min-width: 180px;">
                    <h4 style="margin: 0 0 10px 0; color: #1f77b4;">{row['name']}</h4>
                    <hr style="margin: 5px 0;">
                    <b>ğŸ“Š ì ìˆ˜</b>
                    <ul style="margin: 5px 0; padding-left: 20px;">
                        <li>ì¥ì†Œì„±: <b>{row['overall_score']:.2f}</b></li>
                        <li>ì ‘ê·¼ì„±: <b>{row['accessibility_score']:.2f}</b></li>
                    </ul>
                    <b>ğŸ“ êµ¬</b><br>
                    <span style="color: #666;">{row['district']}</span>
                    <hr style="margin: 5px 0;">
                    <small style="color: #999;">ğŸ’¡ ëŒ€ì¤‘êµí†µ ì •ë³´ëŠ”<br>3ë‹¨ê³„ì—ì„œ ì¶”ê°€ ê°€ëŠ¥</small>
                </div>
                """
                marker_color = 'blue' if row['overall_score'] > 0.7 else 'gray'
            
            folium.Marker(
                location=[row['lat'], row['lng']],
                popup=folium.Popup(popup_html, max_width=300),
                tooltip=row['name'],
                icon=folium.Icon(color=marker_color, icon='coffee', prefix='fa')
            ).add_to(marker_cluster)
        
        st_folium(m, width=None, height=600)
    
    # H1 ê²€ì¦: ì ‘ê·¼ì„± ì ìˆ˜ vs ë„ë³´ ì‹œê°„ ìƒê´€ê´€ê³„
    if 'transit_df' in st.session_state and not st.session_state['transit_df'].empty:
        st.markdown("---")
        st.subheader("5ï¸âƒ£ H1 ê²€ì¦: ì ‘ê·¼ì„± ì ìˆ˜ vs ëŒ€ì¤‘êµí†µ ë„ë³´ ì‹œê°„")
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        transit_df = st.session_state['transit_df'].dropna(subset=['walk_time_minutes', 'accessibility_score'])
        
        st.markdown("""
        **ê°€ì„¤ H1**: "ìµœê·¼ì ‘ ëŒ€ì¤‘êµí†µ ë„ë³´ ì‹œê°„ì´ ì§§ì„ìˆ˜ë¡ ì ‘ê·¼ì„± ì ìˆ˜ê°€ ë†’ë‹¤."
        
        ìƒê´€ê³„ìˆ˜ê°€ ìŒìˆ˜(-) ê°’ì„ ê°€ì§€ë©´ ê°€ì„¤ì´ ì§€ì§€ë©ë‹ˆë‹¤.
        (ë„ë³´ ì‹œê°„ â†“ â†’ ì ‘ê·¼ì„± ì ìˆ˜ â†‘)
        """)
        
        # â‘  ì •ê·œì„± ê²€ì •
        shapiro_walk = stats.shapiro(transit_df['walk_time_minutes'])
        shapiro_acc = stats.shapiro(transit_df['accessibility_score'])
        is_normal = shapiro_walk.pvalue > 0.05 and shapiro_acc.pvalue > 0.05
    
        # â‘¡ ìƒê´€ê³„ìˆ˜ ê³„ì‚° (ì •ê·œë¶„í¬ë©´ Pearson, ì•„ë‹ˆë©´ Spearman)
        if is_normal:
            corr_type = "Pearson"
            correlation, p_value = stats.pearsonr(
                transit_df['walk_time_minutes'], transit_df['accessibility_score']
            )
        else:
            corr_type = "Spearman"
            correlation, p_value = stats.spearmanr(
                transit_df['walk_time_minutes'], transit_df['accessibility_score']
            )
    
        col_stat1, col_stat2 = st.columns(2)
        with col_stat1:
            st.metric(f"{corr_type} ìƒê´€ê³„ìˆ˜ (r)", f"{correlation:.3f}")
        with col_stat2:
            st.metric("p-value", f"{p_value:.4f}")
        
        if p_value < 0.05:
            if correlation < 0:
                st.success(f"âœ… ê°€ì„¤ ì§€ì§€: ì ‘ê·¼ì„± ì ìˆ˜ì™€ ë„ë³´ ì‹œê°„ ê°„ ìœ ì˜ë¯¸í•œ ìŒì˜ ìƒê´€ê´€ê³„ (r={correlation:.3f}, p<0.05)")
            else:
                st.warning(f"âš ï¸ ê°€ì„¤ ê¸°ê°: ì–‘ì˜ ìƒê´€ê´€ê³„ ë°œê²¬ (r={correlation:.3f}, p<0.05)")
        else:
            st.info(f"ğŸ“Š ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ì—†ìŒ (p={p_value:.4f} > 0.05)")
        
        # ì‚°ì ë„
        fig_scatter = go.Figure()
        
        fig_scatter.add_trace(go.Scatter(
            x=transit_df['walk_time_minutes'],
            y=transit_df['accessibility_score'],
            mode='markers',
            marker=dict(
                size=10,
                color=transit_df['accessibility_score'],
                colorscale='RdYlGn',
                showscale=True,
                colorbar=dict(title="ì ‘ê·¼ì„±<br>ì ìˆ˜"),
                line=dict(width=1, color='DarkSlateGrey')
            ),
            text=[f"{row['name']}<br>{row['nearest_station']}" for _, row in transit_df.iterrows()],
            hovertemplate='<b>%{text}</b><br>ë„ë³´: %{x:.1f}ë¶„<br>ì ‘ê·¼ì„±: %{y:.2f}<extra></extra>'
        ))
        
        # íšŒê·€ì„  ì¶”ê°€
        from scipy.stats import linregress
        slope, intercept, r_value, p_value_reg, std_err = linregress(
            transit_df['walk_time_minutes'],
            transit_df['accessibility_score']
        )
        
        x_range = np.linspace(
            transit_df['walk_time_minutes'].min(),
            transit_df['walk_time_minutes'].max(),
            100
        )
        y_pred = slope * x_range + intercept
        
        fig_scatter.add_trace(go.Scatter(
            x=x_range,
            y=y_pred,
            mode='lines',
            name=f'íšŒê·€ì„  (y={slope:.3f}x+{intercept:.3f})',
            line=dict(color='red', width=2, dash='dash')
        ))
        
        fig_scatter.update_layout(
            title=f"ì ‘ê·¼ì„± ì ìˆ˜ vs ëŒ€ì¤‘êµí†µ ë„ë³´ ì‹œê°„ (r={correlation:.3f}, p={p_value:.4f})",
            xaxis_title="ìµœê·¼ì ‘ ì—­/ì •ë¥˜ì¥ê¹Œì§€ ë„ë³´ ì‹œê°„ (ë¶„)",
            yaxis_title="ì ‘ê·¼ì„± ì ìˆ˜ (ëª¨ë¸ ì˜ˆì¸¡)",
            height=500,
            hovermode='closest',
            showlegend=True
        )
        
        st.plotly_chart(fig_scatter, use_container_width=True)
        
        # ë°ì´í„° í…Œì´ë¸”
        st.markdown("**ìƒì„¸ ë°ì´í„°**")
        display_df_table = transit_df[['name', 'district', 'overall_score', 'accessibility_score', 
                                 'walk_time_minutes', 'nearest_station', 'transit_type']].copy()
        display_df_table.columns = ['ì¹´í˜ëª…', 'êµ¬', 'ì „ì²´ ì¥ì†Œì„±', 'ì ‘ê·¼ì„± ì ìˆ˜', 
                              'ë„ë³´(ë¶„)', 'ìµœê·¼ì ‘ ì—­/ì •ë¥˜ì¥', 'ìœ í˜•']
        st.dataframe(display_df_table, use_container_width=True, height=300)


# ========================================
# íƒ­ 3: í‘œë³¸ ë°ì´í„° í™•ì¸
# ========================================
with tab3:
    st.markdown("### ğŸ“Š ì„œìš¸ì‹œ ìƒê¶Œ ê¸°ë°˜ ì¹´í˜ í‘œë³¸ ë°ì´í„°")
    st.caption(
        "`scripts/sample_cafes.py`ë¡œ ìƒì„±í•œ `ì„œìš¸ì‹œ_ìƒê¶Œ_ì¹´í˜ë¹µ_í‘œë³¸.csv`ë¥¼ ë¶ˆëŸ¬ì™€ "
        "êµ¬ë³„ í‘œë³¸ ë¶„í¬ì™€ ê°œë³„ ë ˆì½”ë“œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

    if not SAMPLED_CAFE_CSV.exists():
        st.error("`ì„œìš¸ì‹œ_ìƒê¶Œ_ì¹´í˜ë¹µ_í‘œë³¸.csv` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‘œë³¸ ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    else:
        try:
            sampled_df = load_sampled_cafes(SAMPLED_CAFE_CSV)
        except Exception as e:
            st.error(f"CSV ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        else:
            TARGET_PER_DISTRICT = 100
            if "ì‹œêµ°êµ¬ëª…" in sampled_df.columns:
                district_counts = sampled_df["ì‹œêµ°êµ¬ëª…"].value_counts(dropna=False)

                need_resample = district_counts.min() < TARGET_PER_DISTRICT or len(district_counts) < len(SEOUL_DISTRICTS)
                if need_resample:
                    try:
                        full_df = load_full_cafes(FULL_CAFE_CSV)
                    except FileNotFoundError:
                        st.warning(
                            "`ì„œìš¸ì‹œ_ìƒê¶Œ_ì¹´í˜ë¹µ.csv` íŒŒì¼ì„ ì°¾ì§€ ëª»í•´ êµ¬ë‹¹ 100ê°œ ì¬êµ¬ì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. "
                            "ê¸°ì¡´ í‘œë³¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
                        )
                    except Exception as e:
                        st.warning(
                            f"ì „ì²´ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ êµ¬ë‹¹ 100ê°œ ì¬êµ¬ì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤: {e}"
                        )
                    else:
                        available_counts = full_df["ì‹œêµ°êµ¬ëª…"].value_counts()
                        missing_districts = [d for d in SEOUL_DISTRICTS if available_counts.get(d, 0) < TARGET_PER_DISTRICT]

                        if missing_districts:
                            st.warning(
                                f"ë‹¤ìŒ í–‰ì •êµ¬ëŠ” ì „ì²´ ë°ì´í„°ì—ì„œë„ {TARGET_PER_DISTRICT}ê°œ ë¯¸ë§Œì´ì–´ì„œ ì „ëŸ‰ ì‚¬ìš©í•©ë‹ˆë‹¤: {', '.join(missing_districts)}"
                            )

                        resampled_frames = []
                        for district in SEOUL_DISTRICTS:
                            district_df = full_df[full_df["ì‹œêµ°êµ¬ëª…"] == district]
                            if district_df.empty:
                                continue
                            if len(district_df) >= TARGET_PER_DISTRICT:
                                resampled_frames.append(
                                    district_df.sample(n=TARGET_PER_DISTRICT, random_state=42)
                                )
                            else:
                                resampled_frames.append(district_df)

                        if resampled_frames:
                            sampled_df = pd.concat(resampled_frames, ignore_index=True)

            st.success(f"ì´ {len(sampled_df):,}ê°œ ì¹´í˜ í‘œë³¸ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

            info_col1, info_col2, info_col3 = st.columns(3)
            with info_col1:
                st.metric("ì´ í‘œë³¸ ìˆ˜", f"{len(sampled_df):,}")
            with info_col2:
                st.metric("ì‹œêµ°êµ¬ ìˆ˜", f"{sampled_df['ì‹œêµ°êµ¬ëª…'].nunique():,}")
            with info_col3:
                st.metric("ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ ìˆ˜", f"{sampled_df['ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…'].nunique():,}")

            with st.expander("ğŸ” í•„í„°", expanded=True):
                district_options = sorted(sampled_df["ì‹œêµ°êµ¬ëª…"].dropna().unique().tolist())
                selected_districts = st.multiselect(
                    "ì‹œêµ°êµ¬ ì„ íƒ (ì„ íƒ ì‹œ í•„í„° ì ìš©)",
                    district_options,
                    placeholder="ì „ì²´ ì‹œêµ°êµ¬",
                    key="tab3_district_filter",
                )

                subclass_options = sorted(sampled_df["ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…"].dropna().unique().tolist())
                selected_subclasses = st.multiselect(
                    "ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª… ì„ íƒ",
                    subclass_options,
                    default=subclass_options,
                    key="tab3_subclass_filter",
                )

                keyword = st.text_input(
                    "ì¹´í˜ëª…/ì£¼ì†Œ ê²€ìƒ‰ (ë¶€ë¶„ ì¼ì¹˜)",
                    placeholder="ì˜ˆ: ì‹ ì´Œ, ì„ì§€ë¡œ, ë² ì´ì»¤ë¦¬",
                    key="tab3_keyword_filter",
                ).strip()

            filtered_df = sampled_df.copy()

            if selected_districts:
                filtered_df = filtered_df[filtered_df["ì‹œêµ°êµ¬ëª…"].isin(selected_districts)]

            if selected_subclasses:
                filtered_df = filtered_df[filtered_df["ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…"].isin(selected_subclasses)]

            if keyword:
                keyword_lower = keyword.lower()
                filtered_df = filtered_df[
                    filtered_df["ìƒí˜¸ëª…"].fillna("").str.lower().str.contains(keyword_lower)
                    | filtered_df["ë„ë¡œëª…ì£¼ì†Œ"].fillna("").str.lower().str.contains(keyword_lower)
                    | filtered_df["ì§€ë²ˆì£¼ì†Œ"].fillna("").str.lower().str.contains(keyword_lower)
                ]

            st.info(f"í‘œì‹œ ì¤‘: {len(filtered_df):,}ê°œ ì¹´í˜")

            summary_col1, summary_col2 = st.columns(2)
            with summary_col1:
                st.markdown("**ì‹œêµ°êµ¬ë³„ í‘œë³¸ ìˆ˜**")
                district_summary = (
                    filtered_df["ì‹œêµ°êµ¬ëª…"]
                    .value_counts()
                    .rename_axis("ì‹œêµ°êµ¬ëª…")
                    .reset_index(name="í‘œë³¸ìˆ˜")
                    .sort_values("ì‹œêµ°êµ¬ëª…")
                )
                st.dataframe(district_summary, hide_index=True, use_container_width=True, height=220)

            with summary_col2:
                st.markdown("**ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ë³„ ë¶„í¬**")
                subclass_summary = (
                    filtered_df["ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…"]
                    .value_counts()
                    .rename_axis("ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…")
                    .reset_index(name="í‘œë³¸ìˆ˜")
                    .sort_values("ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…")
                )
                st.dataframe(subclass_summary, hide_index=True, use_container_width=True, height=220)

            with st.expander("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", expanded=True):
                st.dataframe(
                    filtered_df,
                    use_container_width=True,
                    hide_index=True,
                    height=520,
                )

            download_bytes = filtered_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                "ğŸ“¥ í•„í„° ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                data=download_bytes,
                file_name="ì„œìš¸ì‹œ_ìƒê¶Œ_ì¹´í˜ë¹µ_í‘œë³¸_í•„í„°ë§.csv",
                mime="text/csv",
                key="tab3_download_sampled",
            )
