import streamlit as st
import googlemaps
from openai import OpenAI
from langgraph.graph import StateGraph, END
from langchain_core.pydantic_v1 import BaseModel, Field
from typing import List, Dict, Any, Optional
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import os
from dotenv import load_dotenv
import json
import re
from wordcloud import WordCloud
import numpy as np
import warnings # ê²½ê³  ë©”ì‹œì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€

# NLP ëª¨ë¸ ê´€ë ¨ ì„í¬íŠ¸ (Hugging Face Transformers)
try:
    from sentence_transformers import SentenceTransformer
    from transformers import pipeline
    from transformers import logging as transformers_logging # íŠ¸ëœìŠ¤í¬ë¨¸ ë¡œê¹… ì„í¬íŠ¸
    # ëª…ì‹œì  ë¡œë“œë¥¼ ìœ„í•œ ì¶”ê°€ import
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except ImportError:
    st.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(sentence-transformers, transformers)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `pip install sentence-transformers transformers` ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    st.stop()


# ----------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •, ìºì‹± ë° ìœ í‹¸ë¦¬í‹°
# ----------------------------------------------------

# í°íŠ¸ ê²½ë¡œ ìºì‹± (WordCloudìš©)
@st.cache_resource(show_spinner=False)
def get_font_path():
    # ì‹¤ì œ í™˜ê²½ì— ë§ì¶° í°íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆì‹œ: NotoSansKR-Regular.ttf íŒŒì¼ì´ 'fonts' í´ë”ì— ìˆë‹¤ê³  ê°€ì •
    # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜, ì‹œìŠ¤í…œ í°íŠ¸ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # í˜„ì¬ëŠ” ë”ë¯¸ ê²½ë¡œë¡œ ì„¤ì •í•˜ë©°, ì‹¤í–‰ í™˜ê²½ì— ë”°ë¼ ìˆ˜ì • í•„ìš”
    try:
        # Streamlit í°íŠ¸ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì‹¤ì œ í°íŠ¸ ê²½ë¡œë¥¼ ì§€ì •
        return os.path.join(os.path.dirname(__file__), 'fonts', 'NotoSansKR-Regular.ttf')
    except:
        return None # í°íŠ¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°

font_path = get_font_path()

# WordCloud ì´ë¯¸ì§€ ìºì‹± (PIL ì´ë¯¸ì§€ ë°˜í™˜)
@st.cache_data(show_spinner=False)
def generate_wordcloud(text: str, font_path: str, colormap: str = "Greens", **kwargs):
    if not text or not text.strip():
        return None
    
    # í°íŠ¸ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ê²½ê³ 
    if font_path and os.path.exists(font_path):
        font_kwarg = {'font_path': font_path}
    else:
        # st.warning("WordCloud í°íŠ¸ íŒŒì¼ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font_kwarg = {}
        
    wc = WordCloud(
        **font_kwarg,
        background_color="white",
        width=600,
        height=300,
        scale=2,
        max_words=180,
        prefer_horizontal=0.9,
        colormap=colormap,
        collocations=True,
        normalize_plurals=False,
        relative_scaling=0.35,
        min_font_size=8,
        max_font_size=90,
        random_state=42,
        regexp=r"[ê°€-í£a-zA-Z]+" # í•œê¸€, ì˜ë¬¸ë§Œ í¬í•¨
    ).generate(text)
    return wc.to_image()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="Seoul Place Recommendation", page_icon="ğŸ—ºï¸", layout="centered")

# ----------------------------------------------------
# 2. API í‚¤ ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ----------------------------------------------------

if "history" not in st.session_state:
    st.session_state.history = []

# API í‚¤ ë¡œë“œ
gmaps_key = os.getenv("Maps_API_KEY") or st.secrets.get("Maps_API_KEY", "")
openai_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")

if "gmaps_key" not in st.session_state:
    st.session_state.gmaps_key = gmaps_key or ""
if "openai_key" not in st.session_state:
    st.session_state.openai_key = openai_key or ""

# API í‚¤ ì…ë ¥ UI (í‚¤ê°€ ì—†ëŠ” ê²½ìš°)
if not st.session_state.gmaps_key or not st.session_state.openai_key:
    st.title("ğŸ—ºï¸ Seoul Place Recommendation and Spatial Evaluation System")

    st.info("API í‚¤ë¥¼ `.env` íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
    st.markdown("---")
    gmaps_input = st.text_input("Google Maps API Key", type="password")
    openai_input = st.text_input("OpenAI API Key", type="password")

    if st.button("Start"):
        if gmaps_input and openai_input:
            st.session_state.gmaps_key = gmaps_input
            st.session_state.openai_key = openai_input
            st.rerun()
        else:
            st.warning("Please enter both API keys.")
    st.stop()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OpenAI clientëŠ” í´ë°± ë¡œì§ì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ ì „ì—­ì ìœ¼ë¡œ ìœ ì§€)
try:
    gmaps = googlemaps.Client(key=st.session_state.gmaps_key)
    # LLM í´ë¼ì´ì–¸íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    client = OpenAI(api_key=st.session_state.openai_key)
except Exception as e:
    st.error(f"API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()


# ----------------------------------------------------
# 3. LangGraph State ì •ì˜ ë° ëª¨ë¸ ë¡œë”© (ìºì‹œ)
# ----------------------------------------------------

class AgentState(BaseModel):
    query: str
    places: Optional[List[Dict[str, Any]]] = Field(default_factory=list)
    answer: Optional[str] = ""

# ì¥ì†Œì„± ìš”ì¸ ì„ë² ë”© ë° ëª¨ë¸ ë¡œë“œ (Sentence-BERT)
@st.cache_resource(show_spinner="ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_category_embeddings():
    try:
        # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©ì— ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©
        model = SentenceTransformer("jhgan/ko-sroberta-multitask")
        # factors.json íŒŒì¼ ë¡œë“œ
        with open("factors.json", "r", encoding="utf-8") as f:
            factors = json.load(f)
    except FileNotFoundError:
        st.error("ì˜¤ë¥˜: 'factors.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— íŒŒì¼ì„ ìƒì„±í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    except Exception as e:
        st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    embeddings = {}
    score_structure = {}
    
    # 11ê°œ ì„¸ë¶€ ìš”ì¸ì˜ ì •ì˜ ë¬¸ì¥ì„ ì„ë² ë”©
    for main_cat, subcats in factors.items():
        score_structure[main_cat] = {}
        for subcat, definition in subcats.items():
            emb = model.encode(definition, normalize_embeddings=True)
            embeddings[subcat] = emb
            score_structure[main_cat][subcat] = None
            
    return embeddings, model, score_structure

category_embeddings, embed_model, new_score_structure_template = load_category_embeddings()

# ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ (ë‹¤ì¤‘ í´ë°± ë¡œì§ ì ìš©)
@st.cache_resource(show_spinner="ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_sentiment_model_with_fallback():
    """
    ì•ˆì •ì  ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë”.
    1) Hugging Faceì—ì„œ ê°€ëŠ¥í•œ ê³µê°œ ëª¨ë¸ë“¤ì„ ìˆœì°¨ ì‹œë„
    2) ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ OpenAI(ë˜ëŠ” ë¡œì»¬ ë£°)ë¡œ í´ë°±í•˜ëŠ” í•¨ìˆ˜ ë°˜í™˜
    ë°˜í™˜ê°’: callable(sentences: List[str]) -> List[dict(label: str, score: float, polarity: float)]
    polarityëŠ” -1.0 ~ 1.0 ìŠ¤ì¼€ì¼ì˜ ì—°ì†ê°’
    """
    # Hugging Face ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™”
    transformers_logging.set_verbosity_error()
    warnings.filterwarnings("ignore", category=FutureWarning)
    
    # í›„ë³´ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸: ê³µê°œì ìœ¼ë¡œ ì¡´ì¬í•˜ê±°ë‚˜ ì‚¬ìš© í”í•œ ëª¨ë¸ë“¤ (ëª…ì‹œì  ë¡œë“œë¥¼ ìœ„í•œ ëª¨ë¸ ì´ë¦„)
    # monologg/koelectra-base-v3-discriminator-finetuned-nsmcëŠ” ì´ë¯¸ ì‹œë„í–ˆê³  ì‹¤íŒ¨ìœ¨ì´ ë†’ì•˜ìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ì•ˆì •ì ì¸ í›„ë³´ë¡œ ëŒ€ì²´
    hf_candidates = [
        "monologg/koelectra-base-finetuned-nsmc",     # KoELECTRA ê¸°ë°˜, NSMC fine-tuned (ìœ ë ¥)
        "daekeun-ml/koelectra-small-v3-nsmc",        # ì‘ì€ NSMC fine-tuned ëª¨ë¸ (ë¹ ë¦„)
        "WhitePeak/bert-base-cased-Korean-sentiment" # ì»¤ìŠ¤í…€ í•œêµ­ì–´ ê°ì„±
    ]

    # try loading HF pipeline for each candidate
    for model_name in hf_candidates:
        try:
            # ëª…ì‹œì  ë¡œë“œë¥¼ í†µí•´ ì•ˆì •ì„± í™•ë³´
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(model_name)
            pipe = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=-1)
            st.info(f"ê°ì„± ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
            
            # ë˜í¼ í•¨ìˆ˜: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ label, score, polarity ë°˜í™˜
            def hf_sentiment(sentences: List[str]):
                results = []
                # HuggingFace pipelineì€ batchë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì•ˆì •ì„±ì„ ìœ„í•´ ìˆœì°¨ ì²˜ë¦¬í•˜ê±°ë‚˜,
                # pipelineì´ ìì²´ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ ë‹¨ì¼ ë¬¸ì¥ì”© í˜¸ì¶œ (ì•ˆì „í•œ ë°©ì‹)
                raw_results = pipe(sentences) # pipelineì´ ë‚´ë¶€ì ìœ¼ë¡œ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ë„ë¡ ìˆ˜ì •

                for r in raw_results:
                    label = r.get("label", "")
                    score = float(r.get("score", 0.0))
                    
                    # ë‹¤ì–‘í•œ label í˜•ì‹ì— ëŒ€ì‘í•˜ì—¬ polarity (-1.0 ~ 1.0) ê³„ì‚°
                    lab_lower = label.lower()
                    polarity = 0.0
                    
                    # NSMC ê¸°ë°˜ ëª¨ë¸ì€ ì£¼ë¡œ LABEL_0(ë¶€ì •)/LABEL_1(ê¸ì •)ì„ ë°˜í™˜
                    if "label_1" in lab_lower or "positive" in lab_lower or "5" in lab_lower or "4" in lab_lower:
                        # ê¸ì • í™•ì‹ ë„(score: 0.5~1.0) -> (ê·¹ì„±: 0.0~1.0)
                        polarity = max(-1.0, min(1.0, score * 2 - 1))
                    elif "label_0" in lab_lower or "negative" in lab_lower or "1" in lab_lower or "2" in lab_lower:
                        # ë¶€ì • í™•ì‹ ë„(score: 0.5~1.0) -> (ê·¹ì„±: -1.0~0.0)
                        polarity = -max(0.0, min(1.0, score * 2 - 1))
                    else:
                        # ì¤‘ë¦½ ë˜ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ë ˆì´ë¸”ì¸ ê²½ìš°
                        polarity = (score - 0.5) * 2

                    results.append({"label": label, "score": score, "polarity": float(polarity)})
                
                return results

            return hf_sentiment

        except Exception as e:
            # ë¡œë“œ ì‹¤íŒ¨ì‹œ ë‹¤ìŒ í›„ë³´ë¡œ ë„˜ì–´ê° (ë¡œê·¸ ë‚¨ê¸°ê¸°)
            print(f"HuggingFace ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {model_name} -> {e}")
            continue

    # ============== HF í›„ë³´ ëª¨ë‘ ì‹¤íŒ¨í•œ ê²½ìš° í´ë°± ==============
    st.warning("ëª¨ë“  Hugging Face ê°ì„± ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. OpenAI í´ë°±(ë¬¸ì¥ë³„ ê°ì„± API) ì‚¬ìš©ì„ ì‹œë„í•©ë‹ˆë‹¤.")

    # OpenAI í´ë°± í•¨ìˆ˜: ë¬¸ì¥ ë‹¨ìœ„ë¡œ -1..1 polarity ë°˜í™˜
    def openai_sentiment(sentences: List[str]):
        results = []
        
        for s in sentences:
            try:
                # LLMì—ê²Œ ì§ì ‘ -1.0 ~ 1.0 ì‚¬ì´ì˜ ì‹¤ìˆ˜ ê°’ë§Œ ìš”ì²­
                prompt = (
                    "í•œêµ­ì–´ ë¬¸ì¥ì˜ ê°ì„±(polarity)ì„ -1.0(ë§¤ìš° ë¶€ì •)ì—ì„œ 1.0(ë§¤ìš° ê¸ì •) ì‚¬ì´ì˜ ìˆ«ìë¡œë§Œ "
                    f"ë‹µí•´ì£¼ì„¸ìš”. ë¬¸ì¥: \"{s}\" ì˜ˆ: -0.75, 0.0, 0.88"
                )
                resp = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=8,
                )
                text = resp.choices[0].message.content.strip()
                
                # ìˆ«ìë§Œ íŒŒì‹± ì‹œë„
                m = re.search(r"-?\d+(\.\d+)?", text)
                if m:
                    polarity = float(m.group(0))
                    # ê·¹ì„± ì ìˆ˜ ë²”ìœ„ë¥¼ -1.0 ~ 1.0ìœ¼ë¡œ ê°•ì œ
                    polarity = max(-1.0, min(1.0, polarity))
                    results.append({"label": "openai", "score": None, "polarity": polarity})
                else:
                    results.append({"label": "openai_parse_fail", "score": None, "polarity": 0.0})
            except Exception as e:
                print(f"OpenAI sentiment error: {e}")
                results.append({"label": "openai_error", "score": None, "polarity": 0.0})
        return results

    # ë£° ê¸°ë°˜ ìµœì¢… í´ë°± í•¨ìˆ˜:
    def rule_sentiment(sentences: List[str]):
        # OpenAIë„ ì‹¤íŒ¨í•˜ê±°ë‚˜ í‚¤ê°€ ì—†ëŠ” ê²½ìš°, ëª¨ë“  ë¬¸ì¥ì„ ì¤‘ë¦½(0.0)ìœ¼ë¡œ ì²˜ë¦¬
        st.warning("ê²½ê³ : OpenAI ê°ì„± ë¶„ì„ë„ ë¶ˆê°€ëŠ¥í•˜ì—¬ ì¤‘ë¦½(0.5) ì ìˆ˜ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        return [{"label": "rule_neutral", "score": None, "polarity": 0.0} for _ in sentences]

    # ìš°ì„  OpenAI í´ë°±ì„ ë°˜í™˜í•˜ë˜, ì‚¬ìš©ìê°€ OPENAI í‚¤ë¥¼ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸
    if st.session_state.get("openai_key"):
        return openai_sentiment
    else:
        st.warning("OPENAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì•„ rule-based ì¤‘ë¦½ í´ë°±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return rule_sentiment

sentiment_model = load_sentiment_model_with_fallback()


# ----------------------------------------------------
# 4. LangGraph Node ì •ì˜
# ----------------------------------------------------

def search_places(state: AgentState):
    """Google Maps APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜"""
    if state.places is None:
        state.places = []
    try:
        # ì„œìš¸ì‹œì²­ ê¸°ì¤€(37.5665,126.9780) ë°˜ê²½ 10km ê²€ìƒ‰
        res = gmaps.places(query=state.query, language="ko", location="37.5665,126.9780", radius=10000)
        state.places = res.get('results', [])[:5]
    except Exception as e:
        st.error(f"Google Maps ì¥ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("Google Maps API í‚¤ê°€ ìœ íš¨í•œì§€, ë˜ëŠ” Places APIê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        state.places = []
    return state.dict()

def analyze_reviews(state: AgentState):
    """Sentence-BERT ê¸°ë°˜ ìœ ì‚¬ë„ í•„í„°ë§ ë° ê°ì„± ë¶„ì„ìœ¼ë¡œ ì¥ì†Œì„± ì •ëŸ‰ í‰ê°€"""
    if state.places is None:
        state.places = []

    place_infos = []
    
    # ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì • (ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì • ê¶Œì¥)
    SIMILARITY_THRESHOLD = 0.35
    
    for place in state.places:
        place_id = place.get("place_id")
        if not place_id:
            continue

        details = gmaps.place(place_id=place_id, language="ko").get('result', {})
        reviews = details.get('reviews', [])[:10] # ìµœëŒ€ 10ê°œ ë¦¬ë·°
        review_text = "\n".join([review['text'] for review in reviews if review.get('text')])

        scores = json.loads(json.dumps(new_score_structure_template))
        
        # LLM í˜¸ì¶œì„ ìœ„í•œ í‚¤ì›Œë“œ/ìš”ì•½ ë³€ìˆ˜
        summary = "ë¶„ì„ ì¤‘..."
        positive_keywords = [] # LLMì´ ì¶”ì¶œí•œ ë‹¨ì–´
        negative_keywords = [] # LLMì´ ì¶”ì¶œí•œ ë‹¨ì–´
        
        # ì¥ì†Œì„± ì •ëŸ‰ í‰ê°€ (NLP ê¸°ë°˜)
        if review_text.strip():
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ê¸°ì¤€)
            review_sentences = re.split(r'[.!?]\s*', review_text)
            
            # 1. LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ë° ì›Œë“œí´ë¼ìš°ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ (OpenAI API ì‚¬ìš©)
            try:
                unified_prompt = f"""ë‹¤ìŒ ë¦¬ë·°ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¥ì†Œë¥¼ ë¶„ì„í•˜ì—¬ í•œ ë²ˆì— JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
                1) positive_keywords: ì¥ì†Œ/ì¥ì†Œì„± ê´€ë ¨ ê¸ì •ì ì¸ **í•µì‹¬ ë‹¨ì–´** ìµœëŒ€ 10ê°œ
                2) negative_keywords: ì¥ì†Œ/ì¥ì†Œì„± ê´€ë ¨ ë¶€ì •ì ì¸ **í•µì‹¬ ë‹¨ì–´** ìµœëŒ€ 10ê°œ
                3) summary: ì „ë°˜ì  ë¶„ìœ„ê¸°, ê³µê°„ íŠ¹ì„±, ì£¼ìš” ê²½í—˜ ì¤‘ì‹¬ì˜ 5~8ë¬¸ì¥ ìš”ì•½ (LLMì´ ë‹´ë‹¹)
                
                ### ë¦¬ë·°
                {review_text}

                ### ì‘ë‹µ í˜•ì‹ (JSONë§Œ)
                {{
                  "positive_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...],
                  "negative_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...],
                  "summary": "ìš”ì•½ ë¬¸ì¥"
                }}
                """
                unified_response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": unified_prompt}],
                    response_format={"type": "json_object"}
                )
                parsed = json.loads(unified_response.choices[0].message.content)

                positive_keywords = parsed.get("positive_keywords", []) or []
                negative_keywords = parsed.get("negative_keywords", []) or []
                summary = (parsed.get("summary") or "").strip() or "ë¦¬ë·° ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ LLM ìš”ì•½ì´ ì–´ë µìŠµë‹ˆë‹¤."
                
            except Exception as e:
                print(f"LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                summary = "LLM ìš”ì•½ ì‹¤íŒ¨. NLP ë¶„ì„ë§Œ ì§„í–‰ë¨."
                positive_keywords, negative_keywords = [], []


            # 2. ì¥ì†Œì„± ì„¸ë¶€ í•­ëª©ë³„ ì ìˆ˜ ì‚°ì • (SBERT + SA ê¸°ë°˜)
            
            # ì¥ì†Œì„± ì„¸ë¶€ í•­ëª© ì´ë¦„: [ê´€ë ¨ ë¬¸ì¥ì˜ ê·¹ì„± ì ìˆ˜ ë¦¬ìŠ¤íŠ¸]
            factor_sentiment_map = {f_name: [] for f_name in category_embeddings.keys()}
            
            # ëª¨ë“  ë¬¸ì¥ì— ëŒ€í•´ ê°ì„± ë¶„ì„ì„ ì¼ê´„ì ìœ¼ë¡œ ìˆ˜í–‰
            sent_results = sentiment_model(review_sentences)
            
            # ê°ì„± ë¶„ì„ ê²°ê³¼ì™€ ë¬¸ì¥ì„ ë§¤í•‘
            processed_sentences = [{"sent": sent, "result": result} for sent, result in zip(review_sentences, sent_results)]
            
            for item in processed_sentences:
                sent = item['sent']
                result = item['result']
                
                if not sent.strip() or len(sent) < 5:
                    continue
                
                try:
                    sent_emb = embed_model.encode(sent, normalize_embeddings=True)
                    
                    polarity = result['polarity'] # í´ë°± í•¨ìˆ˜ì—ì„œ ì´ë¯¸ -1.0 ~ 1.0ìœ¼ë¡œ ë³€í™˜ëœ ê°’ ì‚¬ìš©
                    
                    # 11ê°œ ì¥ì†Œì„± ìš”ì¸ ê°ê°ì— ëŒ€í•´ ìœ ì‚¬ë„ ê²€ì‚¬
                    for f_name, f_emb in category_embeddings.items():
                        sim = np.dot(sent_emb, f_emb) # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                        
                        if sim > SIMILARITY_THRESHOLD:
                            factor_sentiment_map[f_name].append(polarity)
                
                except Exception as e:
                    print(f"ë¬¸ì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}, ë¬¸ì¥: {sent}")
                    continue

            # 3. í•­ëª©ë³„ ìµœì¢… ì ìˆ˜ ê³„ì‚° (ì •ê·œí™”)
            for main_cat, subcats in scores.items():
                for subcat in subcats.keys():
                    polarities = factor_sentiment_map.get(subcat, [])
                    
                    if polarities:
                        # (Polarity + 1) / 2 ë¡œ ì •ê·œí™”: -1.0 -> 0.0, 0.0 -> 0.5, 1.0 -> 1.0
                        avg_polarity_norm = np.mean([(p + 1) / 2 for p in polarities])
                        scores[main_cat][subcat] = float(avg_polarity_norm)
                    else:
                        # ê´€ë ¨ ë¬¸ì¥ì´ ì—†ëŠ” ê²½ìš° ì¤‘ë¦½ ì ìˆ˜ (0.5) ë¶€ì—¬
                        scores[main_cat][subcat] = 0.5
        
        # ìµœì¢… ì •ë³´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        place_infos.append({
            'name': place.get('name', 'ì´ë¦„ ì—†ìŒ'), 
            'summary': summary,
            'address': place.get('formatted_address', place.get('vicinity', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')),
            'scores': scores, 
            'geometry': place.get('geometry', {}), 
            'place_id': place.get('place_id', ''),
            'positive_keywords': positive_keywords, 
            'negative_keywords': negative_keywords, 
        })

    state.places = place_infos
    return state.dict()

# ----------------------------------------------------
# 5. LangGraph êµ¬ì„±
# ----------------------------------------------------

graph = StateGraph(AgentState)
graph.add_node("search_places", search_places)
graph.add_node("analyze_reviews", analyze_reviews)
graph.set_entry_point("search_places")
graph.add_edge("search_places", "analyze_reviews")
graph.add_edge("analyze_reviews", END)
agent = graph.compile()


# ----------------------------------------------------
# 6. Streamlit UI
# ----------------------------------------------------

st.title("ì¥ì†Œì„± ìš”ì¸ ê¸°ë°˜ ê³µê°„ ì •ëŸ‰ í‰ê°€ ë„êµ¬")

st.markdown("ë¶„ì„í•  ê³µê°„ì˜ ìœ„ì¹˜ì™€ ê°ì„±/ê¸°ëŠ¥ì  íŠ¹ì„±ì„ ì…ë ¥í•˜ì‹­ì‹œì˜¤. Â \n"
             "<span style='color:gray'>(ì˜ˆ: ì‹ ì´Œ ì¡°ìš©í•œ ì¹´í˜, ì¢…ë¡œêµ¬ ì „í†µì ì¸ ìŒì‹ì , ë§ˆí¬êµ¬ ì‚°ì±…ë¡œ ê³µì›)</span>", 
             unsafe_allow_html=True)
query = st.text_input("", placeholder="ì˜ˆ: ì‹ ì´Œ ì¡°ìš©í•œ ì¹´í˜")

if st.button("ì¥ì†Œì„± ì •ëŸ‰ ë¶„ì„ ì‹œì‘"):
    if not query.strip():
        st.warning("ì¥ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("NLP ë° LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ë¦¬ë·°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¥ì†Œì„± ìš”ì¸ì„ ì •ëŸ‰ í‰ê°€í•˜ëŠ” ì¤‘..."):
            result = agent.invoke({"query": query, "places": [], "answer": ""})
            places = result.get('places', [])
            st.session_state.history.append((query, places))
            st.rerun()

# ê²°ê³¼ ì¶œë ¥
if st.session_state.history:
    latest_query, latest_places = st.session_state.history[-1]
    st.markdown(f"---")
    st.markdown(f"### '{latest_query}'ì— ëŒ€í•œ ì¥ì†Œì„± í‰ê°€ ê²°ê³¼")

    for i, place in enumerate(latest_places):
        with st.container(border=True):
            st.subheader(place.get('name', 'ì´ë¦„ ì •ë³´ ì—†ìŒ'))
            st.markdown(f"**ğŸ“ ì£¼ì†Œ:** {place.get('address', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')}")
            st.markdown(f"**ğŸ“ ë¦¬ë·° ìš”ì•½ (LLM ìƒì„±):** {place.get('summary', 'ìš”ì•½ ì •ë³´ ì—†ìŒ')}")

            scores = place.get('scores')
            if scores:
                st.markdown(f"**ğŸ“Š ì¥ì†Œì„± ì¢…í•© í‰ê°€ (NLP ê¸°ë°˜)**")

                # Sunburst ì°¨íŠ¸ ë°ì´í„° ìƒì„±
                labels = []
                parents = []
                values = []
                colors = []

                # ë¶€ë“œëŸ¬ìš´ íŒŒìŠ¤í…”í†¤ ìƒ‰ìƒ ë§µ
                color_map = {
                    "ë¬¼ë¦¬ì  í™˜ê²½": "rgb(173, 216, 230)",     # ì—°í•œ íŒŒë€ìƒ‰ (Light Blue)
                    "ì‚¬íšŒì  ìƒí˜¸ì‘ìš©": "rgb(152, 251, 152)",   # ì—°í•œ ì—°ë‘ìƒ‰ (Light Lime Green)
                    "ê°œì¸ì /ë¬¸í™”ì  ì˜ë¯¸": "rgb(255, 182, 193)" # ì—°í•œ ë¶„í™ìƒ‰ (Light Pink)
                }

                # ë£¨íŠ¸ ë…¸ë“œ ì¶”ê°€ (ì „ì²´ ì ìˆ˜ì˜ í‰ê· ìœ¼ë¡œ ì„¤ì •)
                all_scores = [s for main_cat, sub_scores in scores.items() for s in sub_scores.values() if s is not None]
                total_score = sum(all_scores)
                score_count = len(all_scores)
                root_value = total_score / score_count if score_count > 0 else 0.5

                labels.append(place['name'])
                parents.append("")
                values.append(root_value)
                colors.append("#FFFFFF")

                # ëŒ€ë¶„ë¥˜ì™€ ì„¸ë¶€ ë¶„ë¥˜ ì¶”ê°€
                for main_cat, sub_scores in scores.items():
                    main_scores = [s for s in sub_scores.values() if s is not None]
                    main_avg = sum(main_scores) / len(main_scores) if main_scores else 0
                    
                    labels.append(main_cat)
                    parents.append(place['name'])
                    values.append(main_avg)
                    colors.append(color_map.get(main_cat, "#CCCCCC"))
                    
                    for sub_cat, score in sub_scores.items():
                        if score is not None:
                            labels.append(f"{sub_cat}: {score:.2f}") # ì ìˆ˜ë¥¼ ë¼ë²¨ì— í¬í•¨
                            parents.append(main_cat)
                            values.append(float(score))
                            colors.append(color_map.get(main_cat, "#CCCCCC"))
                
                # Sunburst ì°¨íŠ¸ ìƒì„±
                try:
                    fig_sunburst = go.Figure(go.Sunburst(
                        labels=labels,
                        parents=parents,
                        values=values,
                        branchvalues="remainder",
                        marker=dict(colors=colors),
                        hovertemplate='<b>%{customdata[0]}</b><br>ì ìˆ˜: %{value:.2f}', # customdataëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ valueë§Œ í‘œì‹œ
                        maxdepth=2,
                        insidetextorientation='radial'
                    ))
                    
                    fig_sunburst.update_layout(
                        margin=dict(t=20, l=10, r=10, b=10),
                        height=400,
                        title_text=f"{place['name']} ì¥ì†Œì„± ì¢…í•© í‰ê°€",
                        font=dict(size=12, family="NotoSansKR, sans-serif")
                    )
                    
                    st.plotly_chart(fig_sunburst, use_container_width=True, key=f"sunburst_{i}_{place.get('place_id','')}")
                    
                except Exception as e:
                    # Sunburst ì‹¤íŒ¨ ì‹œ Treemap ì‹œë„
                    st.error(f"Sunburst ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    pass

                st.markdown(f"**ğŸ“Š ì¥ì†Œì„± ëŒ€ë¶„ë¥˜ í‰ê°€**")
                main_scores = {main: round(sum(filter(None, sub.values())) / len(sub), 2) for main, sub in scores.items() if any(s is not None for s in sub.values())}
                if main_scores:
                    df = pd.DataFrame(list(main_scores.items()), columns=['ë¶„ë¥˜', 'ì ìˆ˜'])
                    fig_bar = px.bar(df, x='ë¶„ë¥˜', y='ì ìˆ˜', color='ë¶„ë¥˜', color_discrete_map=color_map, range_y=[0, 1], text_auto='.2f')
                    fig_bar.update_layout(showlegend=False, title_text="")
                    st.plotly_chart(fig_bar, use_container_width=True, key=f"bar_{i}_{place.get('place_id','')}")
                else:
                    st.warning("ì •ëŸ‰ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

                # ì›Œë“œ í´ë¼ìš°ë“œ ì‹œê°í™” (LLM ì¶”ì¶œ í‚¤ì›Œë“œ ì‚¬ìš©)
                if place.get('positive_keywords') or place.get('negative_keywords'):
                    st.markdown("---")
                    st.markdown("**ğŸ“ ë¦¬ë·° í‚¤ì›Œë“œ ë¶„ì„ (LLM ì¶”ì¶œ)**")
                    
                    col_pos, col_neg = st.columns(2)
                    
                    # ê¸ì • ì›Œë“œ í´ë¼ìš°ë“œ
                    if place.get('positive_keywords'):
                        with col_pos:
                            st.markdown("#### âœ… ê¸ì • í‚¤ì›Œë“œ")
                            text = " ".join(place['positive_keywords'])
                            if text:
                                img = generate_wordcloud(text, font_path, colormap="Greens")
                                if img is not None:
                                    st.image(img, use_container_width=True)
                            else:
                                st.info("ê¸ì • í‚¤ì›Œë“œ ì—†ìŒ")
                    
                    # ë¶€ì • ì›Œë“œ í´ë¼ìš°ë“œ
                    if place.get('negative_keywords'):
                        with col_neg:
                            st.markdown("#### âŒ ë¶€ì • í‚¤ì›Œë“œ")
                            text = " ".join(place['negative_keywords'])
                            if text:
                                img = generate_wordcloud(text, font_path, colormap="Reds")
                                if img is not None:
                                    st.image(img, use_container_width=True)
                            else:
                                st.info("ë¶€ì • í‚¤ì›Œë“œ ì—†ìŒ")
                
                # ì§€ë„ ë° ë¡œë“œë·° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                if place.get('geometry') and place['geometry'].get('location'):
                    lat, lng = place['geometry']['location']['lat'], place['geometry']['location']['lng']
                    
                    map_key = f"map_{i}_{place['place_id']}"
                    streetview_key = f"street_{i}_{place['place_id']}"
                    
                    if map_key not in st.session_state:
                        st.session_state[map_key] = False
                    if streetview_key not in st.session_state:
                        st.session_state[streetview_key] = False
                    
                    col1, col2 = st.columns(2)
                    
                    # ë²„íŠ¼ í´ë¦­ ì‹œ ìƒíƒœ í† ê¸€ í›„ ì¬ì‹¤í–‰í•˜ì—¬ ì§€ë„ í‘œì‹œ
                    if col1.button("ğŸ—ºï¸ ì§€ë„ ë³´ê¸°", key=f"btn_{map_key}"):
                        st.session_state[map_key] = not st.session_state[map_key]
                        st.rerun()
                    
                    if col2.button("ğŸš— ë¡œë“œë·° ë³´ê¸°", key=f"btn_{streetview_key}"):
                        st.session_state[streetview_key] = not st.session_state[streetview_key]
                        st.rerun()
                    
                    if st.session_state[map_key] or st.session_state[streetview_key]:
                        st.markdown("**ğŸ“ ìœ„ì¹˜ ì •ë³´**")
                        
                        if st.session_state[map_key]:
                            st.markdown("**ğŸ—ºï¸ ì§€ë„**")
                            # Google Maps Embed API
                            map_url = f"https://www.google.com/maps/embed/v1/place?key={st.session_state.gmaps_key}&q={lat},{lng}"
                            st.components.v1.iframe(map_url, height=400, width=700)
                        
                        if st.session_state[streetview_key]:
                            st.markdown("**ğŸš— ë¡œë“œë·°**")
                            # Google Maps Street View Embed API
                            streetview_url = f"https://www.google.com/maps/embed/v1/streetview?key={st.session_state.gmaps_key}&location={lat},{lng}"
                            st.components.v1.iframe(streetview_url, height=400, width=700)
                else:
                    st.info("ğŸ“ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ì–´ ì§€ë„ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
