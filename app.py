import streamlit as st
import googlemaps
from openai import OpenAI
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import os
from dotenv import load_dotenv
import json
import re
from wordcloud import WordCloud
import numpy as np
import warnings # ê²½ê³  ë©”ì‹œì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€

# NLP ëª¨ë¸ ê´€ë ¨ ì„í¬íŠ¸ (Hugging Face Transformers)
try:
    from sentence_transformers import SentenceTransformer
    from transformers import pipeline
    from transformers import logging as transformers_logging # íŠ¸ëœìŠ¤í¬ë¨¸ ë¡œê¹… ì„í¬íŠ¸
    # ëª…ì‹œì  ë¡œë“œë¥¼ ìœ„í•œ ì¶”ê°€ import
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except ImportError:
    st.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(sentence-transformers, transformers)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `pip install sentence-transformers transformers` ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    st.stop()


# ----------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •, ìºì‹± ë° ìœ í‹¸ë¦¬í‹°
# ----------------------------------------------------

# í°íŠ¸ ê²½ë¡œ ìºì‹± (WordCloudìš©)
@st.cache_resource(show_spinner=False)
def get_font_path():
    # ì‹¤ì œ í™˜ê²½ì— ë§ì¶° í°íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆì‹œ: NotoSansKR-Regular.ttf íŒŒì¼ì´ 'fonts' í´ë”ì— ìˆë‹¤ê³  ê°€ì •
    # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜, ì‹œìŠ¤í…œ í°íŠ¸ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # í˜„ì¬ëŠ” ë”ë¯¸ ê²½ë¡œë¡œ ì„¤ì •í•˜ë©°, ì‹¤í–‰ í™˜ê²½ì— ë”°ë¼ ìˆ˜ì • í•„ìš”
    try:
        # Streamlit í°íŠ¸ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì‹¤ì œ í°íŠ¸ ê²½ë¡œë¥¼ ì§€ì •
        return os.path.join(os.path.dirname(__file__), 'fonts', 'NotoSansKR-Regular.ttf')
    except:
        return None # í°íŠ¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°

font_path = get_font_path()

# WordCloud ì´ë¯¸ì§€ ìºì‹± (PIL ì´ë¯¸ì§€ ë°˜í™˜)
@st.cache_data(show_spinner=False)
def generate_wordcloud(text: str, font_path: str, colormap: str = "Greens", **kwargs):
    if not text or not text.strip():
        return None
    
    # í°íŠ¸ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ê²½ê³ 
    if font_path and os.path.exists(font_path):
        font_kwarg = {'font_path': font_path}
    else:
        # st.warning("WordCloud í°íŠ¸ íŒŒì¼ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font_kwarg = {}
        
    wc = WordCloud(
        **font_kwarg,
        background_color="white",
        width=600,
        height=300,
        scale=2,
        max_words=180,
        prefer_horizontal=0.9,
        colormap=colormap,
        collocations=True,
        normalize_plurals=False,
        relative_scaling=0.35,
        min_font_size=8,
        max_font_size=90,
        random_state=42,
        regexp=r"[ê°€-í£a-zA-Z]+" # í•œê¸€, ì˜ë¬¸ë§Œ í¬í•¨
    ).generate(text)
    return wc.to_image()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="Seoul Place Recommendation", page_icon="ğŸ—ºï¸", layout="wide")

# ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜ (OpenAI ê¸°ë°˜)
def semantic_split(text: str) -> list[str]:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•´ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    """
    try:
        prompt = f"""
        ì•„ë˜ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„(í•˜ë‚˜ì˜ ê°ì •ì´ë‚˜ í‰ê°€ë¥¼ ë‹´ì€ ë‹¨ë½)ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.
        ê° ë¬¸ì¥ì€ ë…ë¦½ì ì¸ íŒë‹¨ì´ ê°€ëŠ¥í•œ ë‹¨ìœ„ì—¬ì•¼ í•˜ë©°, ë¶ˆí•„ìš”í•œ ì ‘ì†ì‚¬ëŠ” ì œê±°í•˜ì„¸ìš”.
        ì¶œë ¥ì€ JSON ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.

        ì˜ˆì‹œ:
        ì…ë ¥: "ì¹´í˜ ë¶„ìœ„ê¸°ê°€ ì¢‹ê³  ì»¤í”¼ëŠ” ë§›ìˆì§€ë§Œ ì¢Œì„ì´ ì¢ì•„ìš”."
        ì¶œë ¥: ["ì¹´í˜ ë¶„ìœ„ê¸°ê°€ ì¢‹ë‹¤.", "ì»¤í”¼ê°€ ë§›ìˆë‹¤.", "ì¢Œì„ì´ ì¢ë‹¤."]

        ë¦¬ë·° í…ìŠ¤íŠ¸:
        {text}
        """

        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "semantic_split_schema",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "sentences": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["sentences"],
                        "additionalProperties": False
                    }
                }
            },
            temperature=0
        )
        content = resp.choices[0].message.content
        parsed = json.loads(content) if content else {"sentences": []}
        if isinstance(parsed, list):
            candidates = parsed
        else:
            candidates = parsed.get("sentences", [])
        cleaned = [c.strip() for c in candidates if isinstance(c, str) and len(c.strip()) > 2]
        if cleaned:
            return cleaned
    except Exception as e:
        print(f"Semantic split error: {e}")
        # í´ë°±ì€ ì•„ë˜ ì¼ë°˜ ë¶„ê¸°ì—ì„œ ìˆ˜í–‰
        pass
    # í´ë°±: ë¬¸ì¥ë¶€í˜¸ â†’ ì ‘ì† í‘œí˜„ 2ë‹¨ê³„ ë¶„í• 
    fallback_units = []
    for s in re.split(r'[.!?]\s*', text):
        if not s or not s.strip():
            continue
        parts = re.split(r'(?:í•˜ì§€ë§Œ|ê·¸ëŸ¬ë‚˜|ê·¸ëŸ°ë°|ì¸ë°|ì§€ë§Œ|ëŠ”ë°)', s)
        for p in parts:
            p = p.strip()
            if len(p) > 2:
                fallback_units.append(p)
    return fallback_units


# ìºì‹œëœ ì˜ë¯¸ ë¶„í•  ë˜í¼
@st.cache_data(show_spinner=False)
def cached_semantic_split(text: str) -> List[str]:
    return semantic_split(text)


# ìºì‹œëœ LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ë˜í¼
@st.cache_data(show_spinner=False)
def cached_unified_summary(review_text: str):
    try:
        sample = review_text[:1200]
        unified_prompt = f"""
        ë‹¤ìŒ ë¦¬ë·°ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¥ì†Œë¥¼ ë¶„ì„í•˜ì—¬ í•œ ë²ˆì— ê²°ê³¼ë§Œ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
        1) positive_keywords: ì¥ì†Œ/ì¥ì†Œì„± ê´€ë ¨ ê¸ì • í•µì‹¬ ë‹¨ì–´ ìµœëŒ€ 10ê°œ (ë¬¸ìì—´ ë°°ì—´)
        2) negative_keywords: ì¥ì†Œ/ì¥ì†Œì„± ê´€ë ¨ ë¶€ì • í•µì‹¬ ë‹¨ì–´ ìµœëŒ€ 10ê°œ (ë¬¸ìì—´ ë°°ì—´)
        3) summary: ì „ë°˜ì  ë¶„ìœ„ê¸°/ê³µê°„ íŠ¹ì„±/ì£¼ìš” ê²½í—˜ ì¤‘ì‹¬ì˜ 5~8ë¬¸ì¥ ìš”ì•½ (ë¬¸ìì—´)

        ë¦¬ë·° í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì •ì˜ êµ¬ê°„ ì•ˆì˜ í…ìŠ¤íŠ¸ë§Œ ì°¸ê³ í•˜ì„¸ìš”:
        ```
        {sample}
        ```
        """
        unified_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": unified_prompt}],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "unified_summary_schema",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "positive_keywords": {"type": "array", "items": {"type": "string"}},
                            "negative_keywords": {"type": "array", "items": {"type": "string"}},
                            "summary": {"type": "string"}
                        },
                        "required": ["positive_keywords", "negative_keywords", "summary"],
                        "additionalProperties": False
                    }
                }
            },
            temperature=0
        )
        content = unified_response.choices[0].message.content
        parsed = json.loads(content) if content else {"positive_keywords": [], "negative_keywords": [], "summary": ""}
        return {
            "positive_keywords": parsed.get("positive_keywords", []) or [],
            "negative_keywords": parsed.get("negative_keywords", []) or [],
            "summary": (parsed.get("summary") or "").strip() or "ë¦¬ë·° ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ LLM ìš”ì•½ì´ ì–´ë µìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        print(f"LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "positive_keywords": [],
            "negative_keywords": [],
            "summary": "LLM ìš”ì•½ ì‹¤íŒ¨. NLP ë¶„ì„ë§Œ ì§„í–‰ë¨."
        }

# ----------------------------------------------------
# 2. API í‚¤ ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ----------------------------------------------------

if "history" not in st.session_state:
    st.session_state.history = []

# API í‚¤ ë¡œë“œ
gmaps_key = os.getenv("Maps_API_KEY") or st.secrets.get("Maps_API_KEY", "")
openai_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")

if "gmaps_key" not in st.session_state:
    st.session_state.gmaps_key = gmaps_key or ""
if "openai_key" not in st.session_state:
    st.session_state.openai_key = openai_key or ""

# API í‚¤ ì…ë ¥ UI (í‚¤ê°€ ì—†ëŠ” ê²½ìš°)
if not st.session_state.gmaps_key or not st.session_state.openai_key:
    st.title("ğŸ—ºï¸ Seoul Place Recommendation and Spatial Evaluation System")

    st.info("API í‚¤ë¥¼ `.env` íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
    st.markdown("---")
    gmaps_input = st.text_input("Google Maps API Key", type="password")
    openai_input = st.text_input("OpenAI API Key", type="password")

    if st.button("Start"):
        if gmaps_input and openai_input:
            st.session_state.gmaps_key = gmaps_input
            st.session_state.openai_key = openai_input
            st.rerun()
        else:
            st.warning("Please enter both API keys.")
    st.stop()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OpenAI clientëŠ” í´ë°± ë¡œì§ì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ ì „ì—­ì ìœ¼ë¡œ ìœ ì§€)
try:
    gmaps = googlemaps.Client(key=st.session_state.gmaps_key)
    # LLM í´ë¼ì´ì–¸íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    client = OpenAI(api_key=st.session_state.openai_key)
except Exception as e:
    st.error(f"API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()


# ----------------------------------------------------
# 3. LangGraph State ì •ì˜ ë° ëª¨ë¸ ë¡œë”© (ìºì‹œ)
# ----------------------------------------------------

class AgentState(BaseModel):
    query: str
    places: Optional[List[Dict[str, Any]]] = Field(default_factory=list)
    answer: Optional[str] = ""

# ì¥ì†Œì„± ìš”ì¸ ì„ë² ë”© ë° ëª¨ë¸ ë¡œë“œ (Sentence-BERT)
def _compute_factors_hash(path: str = "factors.json") -> str:
    try:
        with open(path, "rb") as f:
            import hashlib
            return hashlib.md5(f.read()).hexdigest()
    except Exception:
        return ""

@st.cache_resource(show_spinner="ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_category_embeddings(factors_hash: str):
    try:
        # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©ì— ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©
        model = SentenceTransformer("jhgan/ko-sroberta-multitask")
        # factors.json íŒŒì¼ ë¡œë“œ
        with open("factors.json", "r", encoding="utf-8") as f:
            factors = json.load(f)
    except FileNotFoundError:
        st.error("ì˜¤ë¥˜: 'factors.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— íŒŒì¼ì„ ìƒì„±í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    except Exception as e:
        st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    embeddings = {}
    score_structure = {}
    
    # 11ê°œ ì„¸ë¶€ ìš”ì¸ì˜ ì •ì˜ ë¬¸ì¥ì„ ì„ë² ë”©
    for main_cat, subcats in factors.items():
        score_structure[main_cat] = {}
        for subcat, definition in subcats.items():
            emb = model.encode(definition, normalize_embeddings=True)
            embeddings[subcat] = emb
            score_structure[main_cat][subcat] = None
            
    return embeddings, model, score_structure

factors_hash = _compute_factors_hash()
category_embeddings, embed_model, new_score_structure_template = load_category_embeddings(factors_hash)

@st.cache_resource(show_spinner="ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_sentiment_model_tabularis():
    """
    ê³µê°œ ê°ì • ë¶„í¬í˜• ëª¨ë¸ (tabularisai/multilingual-sentiment-analysis) ê¸°ë°˜
    - ì¶œë ¥: 0.0 ~ 1.0 ì—°ì† ì ìˆ˜ (ë¶€ì •â†’ê¸ì •)
    """
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    import numpy as np

    model_name = "tabularisai/multilingual-sentiment-analysis"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    pipe = pipeline("text-classification", model=model, tokenizer=tokenizer, return_all_scores=True)
    weights = np.linspace(0, 1, 5)  # [0.0, 0.25, 0.5, 0.75, 1.0]

    def predict_score(sentences: List[str]):
        if not sentences:
            return []
        results = pipe(sentences)
        scores = []
        for res in results:
            probs = np.array([r['score'] for r in res])
            score = float(np.dot(probs, weights))
            scores.append(score)
        return scores

    return predict_score


sentiment_model = load_sentiment_model_tabularis()


# ----------------------------------------------------
# 4. LangGraph Node ì •ì˜
# ----------------------------------------------------

def search_places(state: AgentState):
    """Google Maps APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜"""
    if state.places is None:
        state.places = []
    try:
        # ì„œìš¸ì‹œì²­ ê¸°ì¤€(37.5665,126.9780) ë°˜ê²½ 10km ê²€ìƒ‰
        res = gmaps.places(query=state.query, language="ko", location="37.5665,126.9780", radius=10000)
        state.places = res.get('results', [])[:5]
    except Exception as e:
        st.error(f"Google Maps ì¥ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("Google Maps API í‚¤ê°€ ìœ íš¨í•œì§€, ë˜ëŠ” Places APIê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        state.places = []
    return state.dict()

def analyze_reviews(state: AgentState):
    """SBERT + Sentiment ê¸°ë°˜ ì •ëŸ‰ í‰ê°€ + LLM í•´ì„"""
    if state.places is None:
        state.places = []

    place_infos = []
    
    SIMILARITY_THRESHOLD = 0.35
    ALPHA, BETA = 0.75, 0.25  # ìœ ì‚¬ë„ ë¹„ì¤‘ ì¶”ê°€ ìƒí–¥
    
    # factors.jsonì€ í•œ ë²ˆë§Œ ë¡œë“œ (ì†ë„ ê°œì„ )
    with open("factors.json", "r", encoding="utf-8") as f:
        factor_definitions = json.load(f)
    
    for place in state.places:
        place_id = place.get("place_id")
        if not place_id:
            continue

        details = gmaps.place(place_id=place_id, language="ko").get('result', {})
        reviews = details.get('reviews', [])[:10]
        review_texts = [r['text'] for r in reviews if r.get('text')]
        if not review_texts:
            continue

        # 1) ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬ (LLM ì‚¬ìš©)
        review_text = "\n".join(review_texts)
        review_units = cached_semantic_split(review_text)
        if not review_units:
            review_units = cached_semantic_split(" ".join(review_texts))

        # 2) SBERT + ê°ì„±ëª¨ë¸ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        factor_sentiments = {f: [] for f in category_embeddings.keys()}
        # ë¦¬ë·° ìš”ì•½/í‚¤ì›Œë“œ (summary)ì™€ ì ìˆ˜ í•´ì„¤(explanation)ì€ ë¶„ë¦¬ ìƒì„±
        summary = "ë¦¬ë·° ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        positive_keywords: List[str] = []
        negative_keywords: List[str] = []

        # 2-1) ë¦¬ë·° ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ (summary ì „ìš©)
        try:
            cached = cached_unified_summary(review_text)
            positive_keywords = cached.get("positive_keywords", []) or []
            negative_keywords = cached.get("negative_keywords", []) or []
            summary = cached.get("summary", "") or "ë¦¬ë·° ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ LLM ìš”ì•½ì´ ì–´ë µìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            summary = "LLM ìš”ì•½ ì‹¤íŒ¨. NLP ë¶„ì„ë§Œ ì§„í–‰ë¨."
            positive_keywords, negative_keywords = [], []

        # ê°ì„±(0~1) ìŠ¤ì½”ì–´ ë° ë°°ì¹˜ ì„ë² ë”©/ìœ ì‚¬ë„
        sentiment_scores = sentiment_model(review_units)
        unit_embs = embed_model.encode(review_units, normalize_embeddings=True)
        subcat_list = list(category_embeddings.keys())
        factor_mat = np.stack([category_embeddings[s] for s in subcat_list], axis=0)
        sim_mat = np.matmul(unit_embs, factor_mat.T)

        for i, unit in enumerate(review_units):
            raw_sent = float(sentiment_scores[i]) if i < len(sentiment_scores) else 0.5
            # ê°ì„± ë³´ì •: í•˜í•œ 0.3 ê¸°ì¤€ìœ¼ë¡œ ì¶”ê°€ ì™„í™”
            sent_adj = np.clip((raw_sent - 0.3) / 0.7, 0, 1)
            sims = sim_mat[i]
            for j, sim in enumerate(sims):
                # ìœ ì‚¬ë„ ë³´ì •: 0.3 ê¸°ì¤€ìœ¼ë¡œ ì¶”ê°€ ì™„í™” (ë” ë§ì€ ë¬¸ì¥ í¬í•¨)
                sim_adj = np.clip((float(sim) - 0.3) / 0.5, 0, 1)
                if sim_adj > 0:
                    f_name = subcat_list[j]
                    combined = ALPHA * sim_adj + BETA * sent_adj
                    # ì‹œê·¸ëª¨ì´ë“œ: ì¤‘ì‹¬ 0.4, ê¸°ìš¸ê¸° 2.2ë¡œ ìƒí•œ í™•ì¥
                    score_scaled = 1 / (1 + np.exp(-2.2 * (combined - 0.4)))
                    factor_sentiments[f_name].append(float(score_scaled))

        # 3) í‚¤ì›Œë“œ ê¸°ë°˜ ë¶€ìŠ¤íŒ… (ì„ë² ë”© í•œê³„ ë³´ì™„)
        keyword_boosts = {
            "ê³ ìœ ì„±": ["ë…íŠ¹", "ìœ ë‹ˆí¬", "ì°¨ë³„", "ì»¨ì…‰", "í…Œë§ˆ", "íŠ¹ìƒ‰", "ê°œì„±", "íŠ¹ë³„í•œ", "ì•„ì´ë´í‹°í‹°"],
            "ë¬¸í™”ì  ë§¥ë½": ["ì „í†µ", "ì—­ì‚¬", "ë…„", "ì˜¤ë˜", "ì˜›", "ê³ í’", "ë¬¸í™”", "ë°°ê²½", "ìŠ¤í† ë¦¬"],
            "ê¸°ì–µ/ê²½í—˜": ["ì¶”ì–µ", "ê°ë™", "ì¸ìƒ", "íŠ¹ë³„", "ìŠì„ ìˆ˜", "ê¸°ì–µ", "íšŒìƒ"],
            "ì‹¬ë¯¸ì„±": ["ì˜ˆì˜", "ì•„ë¦„", "ë©‹ì§€", "ì„¸ë ¨", "ì•¼ê²½", "ë·°", "ì¸í…Œë¦¬ì–´", "ë””ìì¸", "ì¡°ëª…", "ì•„ëŠ‘"],
            "ê°ê°ì  ê²½í—˜": ["ìŒì•…", "í–¥", "ëƒ„ìƒˆ", "ì§ˆê°", "ë§›", "ì˜¤ê°", "ê°ê°"],
            "ì¾Œì ì„±": ["ì²­ê²°", "ê¹¨ë—", "ë°", "í†µí’", "í™”ì¥ì‹¤", "ìœ„ìƒ", "ì •ëˆ"],
            "ì ‘ê·¼ì„±": ["ê°€ê¹", "ì ‘ê·¼", "ì—­", "ì •ë¥˜ì¥", "ë„ë³´", "ë¶„ ê±°ë¦¬", "í¸ë¦¬"],
            "í™œë™ì„±": ["ëŒ€í™”", "ì—…ë¬´", "ì‘ì—…", "íšŒì˜", "ê³µë¶€", "í™œë™"],
            "ì‚¬íšŒì„±": ["ì¹œì ˆ", "ì„œë¹„ìŠ¤", "êµë¥˜", "ì†Œí†µ", "ì¹œê·¼"],
            "í˜•íƒœì„±": ["ë„“", "ê³µê°„", "êµ¬ì¡°", "ë°°ì¹˜", "ê°œë°©", "ë™ì„ "],
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì§ì ‘ ê³ ì ìˆ˜ í• ë‹¹
        for factor, keywords in keyword_boosts.items():
            matched_kws = [kw for kw in keywords if kw in review_text]
            match_count = len(matched_kws)
            if match_count > 0:
                # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ìˆ˜ì— ë¹„ë¡€í•´ 0.75~0.95 í• ë‹¹
                boosted_score = min(0.75 + (match_count * 0.05), 0.95)
                # ì—¬ëŸ¬ ë²ˆ ì¶”ê°€í•´ í‰ê· ì—ì„œë„ ë†’ì€ ê°€ì¤‘ì¹˜ ìœ ì§€
                for _ in range(3):
                    factor_sentiments[factor].append(boosted_score)
                print(f"[BOOST] {factor}: {match_count}ê°œ í‚¤ì›Œë“œ ë§¤ì¹­ ({', '.join(matched_kws[:3])}) â†’ {boosted_score:.2f}")
        
        # 3-1) ì„¸ë¶€ìš”ì¸ë³„ í‰ê·  ì ìˆ˜ (ì •ê·œí™” í¬í•¨)
        scores = json.loads(json.dumps(new_score_structure_template))
        all_vals = []
        for vals in factor_sentiments.values():
            all_vals.extend(vals)
        if all_vals:
            vmin, vmax = float(np.min(all_vals)), float(np.max(all_vals))
        else:
            vmin, vmax = 0.5, 0.5

        for main_cat, subcats in scores.items():
            for subcat in subcats.keys():
                vals = factor_sentiments.get(subcat, [])
                if vals and vmax > vmin:
                    raw = float(np.mean(vals))
                    # 0.30~1.0 ë²”ìœ„ë¡œ min-max ì •ê·œí™”
                    normed = 0.30 + 0.70 * ((raw - vmin) / (vmax - vmin + 1e-8))
                    scores[main_cat][subcat] = float(np.clip(normed, 0.30, 1.0))
                elif vals:
                    scores[main_cat][subcat] = float(np.clip(vals[0], 0.30, 1.0))
                else:
                    scores[main_cat][subcat] = 0.5

        # 4) LLM ê¸°ë°˜ ì ìˆ˜ ê²€ì¦ ë° ë³´ì • (GPT-4o ì¶”ë¡ )
        corrected_scores = json.loads(json.dumps(scores))  # ë³´ì • ì „ ë³µì‚¬
        correction_log = []
        try:
            sample_reviews = "\n".join(review_texts[:3])  # ë¦¬ë·° 5ê°œâ†’3ê°œë¡œ ì¶•ì†Œ
            
            validation_prompt = f"""
ë‹¹ì‹ ì€ ì¥ì†Œì„± í‰ê°€ ê°ì‚¬ìì…ë‹ˆë‹¤.
ì…ë ¥ëœ ì ìˆ˜ëŠ” SBERT + ê°ì„± íšŒê·€ëª¨ë¸ë¡œ ì‚°ì¶œëœ ê°’ì…ë‹ˆë‹¤.
ê° ìš”ì¸ë³„ ì ìˆ˜ì˜ íƒ€ë‹¹ì„±ì„ **ìš”ì¸ì˜ ì •ì˜ì— ë”°ë¼** ì •í™•íˆ ê²€í† í•˜ì„¸ìš”.

## ìš”ì¸ ì •ì˜ (ë°˜ë“œì‹œ ì°¸ê³ )
{json.dumps(factor_definitions, ensure_ascii=False, indent=2)}

## í˜„ì¬ ì ìˆ˜
{json.dumps(scores, ensure_ascii=False, indent=2)}

## ë¦¬ë·° ë‚´ìš©
{sample_reviews}

## ê²€í†  ê·œì¹™
1. ê° ìš”ì¸ì˜ ì •ì˜ì™€ í‚¤ì›Œë“œë¥¼ **ì •í™•íˆ** í™•ì¸í•˜ì„¸ìš”.
   ì˜ˆ: "ê°ê°ì  ê²½í—˜"ì€ ìŒì•…, í–¥ê¸°, ì§ˆê° ë“± ì˜¤ê° ìê·¹ / "ë¬¸í™”ì  ë§¥ë½"ì€ ì—­ì‚¬, ì „í†µ, ì§€ì—­ ë°°ê²½
2. ë¦¬ë·°ì—ì„œ í•´ë‹¹ ìš”ì¸ ì •ì˜ì— ë§ëŠ” ì–¸ê¸‰ì´ ìˆëŠ”ë° ì ìˆ˜ê°€ ë‚®ê±°ë‚˜, ì–¸ê¸‰ì´ ì—†ëŠ”ë° ì ìˆ˜ê°€ ë†’ìœ¼ë©´ delta ì œì•ˆ
3. deltaëŠ” -0.3 ~ +0.3 ë²”ìœ„
4. ê·¼ê±°ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±

## ì¶œë ¥ í˜•ì‹ (JSONë§Œ)
{{
  "corrections": [
    {{"factor": "ì¾Œì ì„±", "delta": 0.15, "reason": "ì²­ê²°, í™”ì¥ì‹¤, ì¶©ì „ì‹œì„¤ ê¸ì • ì–¸ê¸‰ ë§ìŒ"}},
    {{"factor": "ê°ê°ì  ê²½í—˜", "delta": 0.12, "reason": "ë””ì €íŠ¸ ë§›ê³¼ ë‹¤ì–‘ì„± ê°•ì¡°"}}
  ]
}}

ë³´ì • ë¶ˆí•„ìš” ì‹œ: {{"corrections": []}}
"""
            resp = client.chat.completions.create(
                model="gpt-4o",  # ë³´ì •ì€ ì •í™•í•œ ì¶”ë¡ ì´ í•„ìš”í•˜ë¯€ë¡œ gpt-4o ì‚¬ìš©
                messages=[{"role": "user", "content": validation_prompt}],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=500,  # ì¶©ë¶„í•œ í† í°ìœ¼ë¡œ ì •í™•í•œ ë³´ì •
            )
            correction_result = json.loads(resp.choices[0].message.content)
            corrections = correction_result.get("corrections", [])
            
            print(f"[DEBUG] GPT-4o ì‘ë‹µ: {correction_result}")  # ë””ë²„ê¹…ìš©
            
            # ë³´ì • ì ìš©
            for correction in corrections:
                if isinstance(correction, dict):
                    factor_name = correction.get("factor", "")
                    delta = float(correction.get("delta", 0))
                    reason = correction.get("reason", "")
                    
                    # ìš”ì¸ëª… ë§¤ì¹­ í›„ ì ìˆ˜ ë³´ì •
                    for main_cat, subcats in corrected_scores.items():
                        if factor_name in subcats:
                            old_val = subcats[factor_name]
                            new_val = np.clip(old_val + delta, 0.30, 1.0)
                            corrected_scores[main_cat][factor_name] = float(new_val)
                            correction_log.append({
                                "factor": factor_name,
                                "original": round(old_val, 2),
                                "adjusted": round(new_val, 2),
                                "delta": round(delta, 2),
                                "reason": reason
                            })
                            break
            
            # ë³´ì •ëœ ì ìˆ˜ë¥¼ ìµœì¢… ì ìˆ˜ë¡œ ì‚¬ìš©
            scores = corrected_scores
            
            if correction_log:
                print(f"[INFO] {len(correction_log)}ê°œ ìš”ì¸ ë³´ì •ë¨")
            else:
                print(f"[INFO] ë³´ì • í•„ìš” ì—†ìŒ")
            
        except Exception as e:
            print(f"[ERROR] LLM ì ìˆ˜ ë³´ì • ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            correction_log = []

        # 5) LLM ê¸°ë°˜ í•´ì„(explanation) - ë³´ì •ëœ ì ìˆ˜ ê¸°ì¤€ (ê°„ëµí™”)
        try:
            # ëŒ€í‘œ ì ìˆ˜ë§Œ ì¶”ì¶œ (ìƒìœ„ 3ê°œ + í•˜ìœ„ 2ê°œ)
            flat_scores = [(f"{mc}/{sc}", v) for mc, subs in scores.items() for sc, v in subs.items()]
            flat_scores.sort(key=lambda x: x[1], reverse=True)
            top_factors = flat_scores[:3]
            low_factors = flat_scores[-2:]
            
            explanation_prompt = f"""
ì•„ë˜ ì ìˆ˜ì—ì„œ ìƒìœ„/í•˜ìœ„ ìš”ì¸ì˜ ì´ìœ ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
ìƒìœ„: {", ".join([f"{f}({v:.2f})" for f, v in top_factors])}
í•˜ìœ„: {", ".join([f"{f}({v:.2f})" for f, v in low_factors])}
ë¦¬ë·°: {sample_reviews[:500]}
"""
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": explanation_prompt}],
                temperature=0.2,
                max_tokens=300,  # í† í° ì œí•œ
            )
            explanation = resp.choices[0].message.content.strip()
        except Exception as e:
            explanation = f"LLM í•´ì„ ì‹¤íŒ¨: {e}"

        # 6) ê²°ê³¼ ì €ì¥
        place_infos.append({
            'name': place.get('name', 'ì´ë¦„ ì—†ìŒ'), 
            'summary': summary,
            'address': place.get('formatted_address', place.get('vicinity', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')),
            'scores': scores, 
            'geometry': place.get('geometry', {}), 
            'place_id': place.get('place_id', ''),
            'positive_keywords': positive_keywords, 
            'negative_keywords': negative_keywords, 
            'explanation': explanation,
            'corrections': correction_log,  # ë³´ì • ë‚´ì—­ ì¶”ê°€
        })

    state.places = place_infos
    return state.dict()

# ----------------------------------------------------
# 5. LangGraph êµ¬ì„±
# ----------------------------------------------------

graph = StateGraph(AgentState)
graph.add_node("search_places", search_places)
graph.add_node("analyze_reviews", analyze_reviews)
graph.set_entry_point("search_places")
graph.add_edge("search_places", "analyze_reviews")
graph.add_edge("analyze_reviews", END)
agent = graph.compile()


# ----------------------------------------------------
# 6. Streamlit UI
# ----------------------------------------------------

st.title("ì¥ì†Œì„± ìš”ì¸ ê¸°ë°˜ ê³µê°„ ì •ëŸ‰ í‰ê°€ ë„êµ¬")

# CSSë¡œ í…ìŠ¤íŠ¸ ìƒ‰ìƒ ê°•ì œ (ê°€ë…ì„± ê°œì„ )
st.markdown("""
<style>
    .stMarkdown, .stCaption, p, div {
        color: #000000 !important;
    }
    h1, h2, h3, h4, h5, h6 {
        color: #000000 !important;
    }
    /* ì˜ˆì‹œ í…ìŠ¤íŠ¸ëŠ” íšŒìƒ‰ ìœ ì§€ */
    .example-text {
        color: #888888 !important;
    }
</style>
""", unsafe_allow_html=True)

st.markdown("ë¶„ì„í•  ê³µê°„ì˜ ìœ„ì¹˜ì™€ ê°ì„±/ê¸°ëŠ¥ì  íŠ¹ì„±ì„ ì…ë ¥í•˜ì‹­ì‹œì˜¤. "
             "<span class='example-text'>(ì˜ˆ: ì‹ ì´Œ ì¡°ìš©í•œ ì¹´í˜, ì¢…ë¡œêµ¬ ì „í†µì ì¸ ìŒì‹ì , ë§ˆí¬êµ¬ ì‚°ì±…ë¡œ ê³µì›)</span>", 
             unsafe_allow_html=True)
query = st.text_input("", placeholder="ì˜ˆ: ì‹ ì´Œ ì¡°ìš©í•œ ì¹´í˜")

if st.button("ì¥ì†Œì„± ì •ëŸ‰ ë¶„ì„ ì‹œì‘"):
    if not query.strip():
        st.warning("ì¥ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("NLP ë° LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ë¦¬ë·°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¥ì†Œì„± ìš”ì¸ì„ ì •ëŸ‰ í‰ê°€í•˜ëŠ” ì¤‘..."):
            result = agent.invoke({"query": query, "places": [], "answer": ""})
            places = result.get('places', [])
            st.session_state.history.append((query, places))
            st.rerun()

# ê²°ê³¼ ì¶œë ¥
if st.session_state.history:
    latest_query, latest_places = st.session_state.history[-1]
    st.markdown(f"---")
    st.markdown(f"### '{latest_query}'ì— ëŒ€í•œ ì¥ì†Œì„± í‰ê°€ ê²°ê³¼")

    for i, place in enumerate(latest_places):
        with st.container(border=True):
            st.subheader(place.get('name', 'ì´ë¦„ ì •ë³´ ì—†ìŒ'))
            st.markdown(f"**ğŸ“ ì£¼ì†Œ:** {place.get('address', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')}")
            
            # 2ì—´ ë ˆì´ì•„ì›ƒ: ì™¼ìª½(ì‹œê°í™”), ì˜¤ë¥¸ìª½(ë³´ì •/í•´ì„¤)
            col_left, col_right = st.columns([1.2, 1])
            
            scores = place.get('scores')
            
            # ========== ì™¼ìª½ ì—´: ë¦¬ë·° ìš”ì•½ + ì‹œê°í™” ==========
            with col_left:
                st.markdown(f"**ğŸ“ ë¦¬ë·° ìš”ì•½**")
                st.markdown(place.get('summary', 'ìš”ì•½ ì •ë³´ ì—†ìŒ'))
                
            with col_left:
                if scores:
                    st.markdown(f"**ğŸ“Š ì¥ì†Œì„± ì¢…í•© í‰ê°€**")

                    # Sunburst ì°¨íŠ¸ ë°ì´í„° ìƒì„±
                labels = []
                parents = []
                values = []
                colors = []

                # ë¶€ë“œëŸ¬ìš´ íŒŒìŠ¤í…”í†¤ ìƒ‰ìƒ ë§µ (factors.json êµ¬ì¡°ì™€ ë™ì¼í•œ ëŒ€ë¶„ë¥˜ ë¼ë²¨)
                color_map = {
                    "ë¬¼ë¦¬ì  íŠ¹ì„±": "rgb(173, 216, 230)",     # ì—°í•œ íŒŒë€ìƒ‰ (Light Blue)
                    "í™œë™ì  íŠ¹ì„±": "rgb(152, 251, 152)",   # ì—°í•œ ì—°ë‘ìƒ‰ (Light Lime Green)
                    "ì˜ë¯¸ì  íŠ¹ì„±": "rgb(255, 182, 193)" # ì—°í•œ ë¶„í™ìƒ‰ (Light Pink)
                }

                # ë£¨íŠ¸ ë…¸ë“œ ì¶”ê°€ (ì „ì²´ ì ìˆ˜ì˜ í‰ê· ìœ¼ë¡œ ì„¤ì •)
                all_scores = [s for main_cat, sub_scores in scores.items() for s in sub_scores.values() if s is not None]
                total_score = sum(all_scores)
                score_count = len(all_scores)
                root_value = total_score / score_count if score_count > 0 else 0.5

                labels.append(place['name'])
                parents.append("")
                values.append(root_value)
                colors.append("#FFFFFF")

                # ëŒ€ë¶„ë¥˜ì™€ ì„¸ë¶€ ë¶„ë¥˜ ì¶”ê°€
                for main_cat, sub_scores in scores.items():
                    main_scores = [s for s in sub_scores.values() if s is not None]
                    main_avg = sum(main_scores) / len(main_scores) if main_scores else 0
                    
                    labels.append(main_cat)
                    parents.append(place['name'])
                    values.append(main_avg)
                    colors.append(color_map.get(main_cat, "#CCCCCC"))
                    
                    for sub_cat, score in sub_scores.items():
                        if score is not None:
                            labels.append(f"{sub_cat}: {score:.2f}") # ì ìˆ˜ë¥¼ ë¼ë²¨ì— í¬í•¨
                            parents.append(main_cat)
                            values.append(float(score))
                            colors.append(color_map.get(main_cat, "#CCCCCC"))
                
                # Sunburst ì°¨íŠ¸ ìƒì„±
                try:
                    fig_sunburst = go.Figure(go.Sunburst(
                        labels=labels,
                        parents=parents,
                        values=values,
                        branchvalues="remainder",
                        marker=dict(colors=colors),
                        hovertemplate='<b>%{customdata[0]}</b><br>ì ìˆ˜: %{value:.2f}', # customdataëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ valueë§Œ í‘œì‹œ
                        maxdepth=2,
                        insidetextorientation='radial'
                    ))
                    
                    fig_sunburst.update_layout(
                        margin=dict(t=20, l=10, r=10, b=10),
                        height=400,
                        title_text=f"{place['name']} ì¥ì†Œì„± ì¢…í•© í‰ê°€",
                        font=dict(size=12, family="NotoSansKR, sans-serif")
                    )
                    
                    st.plotly_chart(fig_sunburst, use_container_width=True, key=f"sunburst_{i}_{place.get('place_id','')}")
                    
                except Exception as e:
                    # Sunburst ì‹¤íŒ¨ ì‹œ Treemap ì‹œë„
                    st.error(f"Sunburst ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    pass

                # Radar Chart ìƒì„± í•¨ìˆ˜ ì •ì˜
                def make_radar_chart(scores_dict, title="ì¥ì†Œì„± ìš”ì¸ íŠ¹ì„± ë¶„í¬"):
                    # ìƒ‰ìƒ ë§¤í•‘ (ëŒ€ë¶„ë¥˜ ê¸°ì¤€)
                    fill_color_map = {
                        "ë¬¼ë¦¬ì  íŠ¹ì„±": "rgba(173, 216, 230, 0.5)",  # ì—°í•œ íŒŒë€ìƒ‰
                        "í™œë™ì  íŠ¹ì„±": "rgba(152, 251, 152, 0.5)",  # ì—°í•œ ì´ˆë¡ìƒ‰
                        "ì˜ë¯¸ì  íŠ¹ì„±": "rgba(255, 182, 193, 0.5)"   # ì—°í•œ ë¶„í™ìƒ‰
                    }
                    
                    # ì „ì²´ ìš”ì¸ì„ ìˆœì„œëŒ€ë¡œ ë°°ì¹˜ + ìƒ‰ìƒ ë§¤í•‘
                    categories = []
                    values = []
                    colors = []
                    
                    for main_cat, subcats in scores_dict.items():
                        for subcat, val in subcats.items():
                            categories.append(subcat)
                            values.append(val if val is not None else 0.5)
                            colors.append(fill_color_map.get(main_cat, "rgba(200,200,200,0.5)"))
                    
                    fig = go.Figure()
                    
                    # Barpolarë¡œ ê° ì¶•ë³„ ìƒ‰ìƒ êµ¬ë¶„
                    fig.add_trace(go.Barpolar(
                        r=values,
                        theta=categories,
                        marker=dict(
                            color=colors,
                            line=dict(color="rgba(80,80,80,0.3)", width=1)
                        ),
                        hovertemplate='<b>%{theta}</b><br>ì ìˆ˜: %{r:.2f}<extra></extra>',
                        name="ìš”ì¸ë³„ ì ìˆ˜"
                    ))
                    
                    # ìœ¤ê³½ì„ ì„ ìœ„í•œ Scatterpolar ì¶”ê°€
                    categories_closed = categories + categories[:1]
                    values_closed = values + values[:1]
                    
                    fig.add_trace(go.Scatterpolar(
                        r=values_closed,
                        theta=categories_closed,
                        mode='lines',
                        line=dict(color="rgba(60, 60, 60, 0.8)", width=2.5),
                        showlegend=False,
                        hoverinfo='skip'
                    ))
                    
                    fig.update_layout(
                        polar=dict(
                            radialaxis=dict(
                                visible=True, 
                                range=[0, 1], 
                                tickvals=[0.2, 0.4, 0.6, 0.8, 1.0],
                                showline=True,
                                gridcolor="rgba(200,200,200,0.5)"
                            ),
                            angularaxis=dict(rotation=90, direction="clockwise")
                        ),
                        showlegend=False,
                        height=580,
                        margin=dict(l=140, r=140, t=110, b=110),  # ì—¬ë°± ìµœëŒ€ í™•ëŒ€
                        title=dict(text=title, x=0.5, font=dict(size=14, family="NotoSansKR"))
                    )
                    
                    return fig
                
                st.markdown(f"**ğŸ“Š ì¥ì†Œì„± ìš”ì¸ íŠ¹ì„± ë¶„í¬ë„**")
                # Radar Chart ì¶œë ¥
                fig_radar = make_radar_chart(scores, title=f"{place['name']} ì¥ì†Œì„± íŠ¹ì„± ë¶„í¬")
                st.plotly_chart(fig_radar, use_container_width=True, key=f"radar_{i}_{place.get('place_id','')}")

            
            # ========== ì˜¤ë¥¸ìª½ ì—´: LLM ë³´ì • + í•´ì„¤ ==========
            with col_right:
                # LLM ë³´ì • ë‚´ì—­ í‘œì‹œ
                corrections = place.get('corrections', [])
                if corrections:
                    st.markdown("**âš™ï¸ LLM ì ìˆ˜ ë³´ì •**")
                    st.caption("GPT-4o ê²€ì¦ ê²°ê³¼")
                    
                    correction_df = pd.DataFrame(corrections)
                    correction_df = correction_df.rename(columns={
                        "factor": "ìš”ì¸",
                        "original": "ì›ì ìˆ˜",
                        "adjusted": "ë³´ì •",
                        "delta": "Î”",
                        "reason": "ê·¼ê±°"
                    })
                    st.dataframe(
                        correction_df,
                        use_container_width=True,
                        hide_index=True,
                        column_config={
                            "ìš”ì¸": st.column_config.TextColumn(width="small"),
                            "ì›ì ìˆ˜": st.column_config.NumberColumn(format="%.2f", width="small"),
                            "ë³´ì •": st.column_config.NumberColumn(format="%.2f", width="small"),
                            "Î”": st.column_config.NumberColumn(format="%+.2f", width="small"),
                            "ê·¼ê±°": st.column_config.TextColumn(width="medium"),
                        }
                    )
                else:
                    st.markdown("**âš™ï¸ LLM ì ìˆ˜ ë³´ì •**")
                    st.caption("ë³´ì • í•„ìš” ì—†ìŒ")
                
                # ì ìˆ˜ í•´ì„¤ (ë³´ì • í›„ ìµœì¢… ì ìˆ˜ ê¸°ì¤€)
                if place.get('explanation'):
                    st.markdown("**ğŸ” ìµœì¢… ì ìˆ˜ í•´ì„¤**")
                    st.markdown(place.get('explanation'))
                
                # ì›Œë“œ í´ë¼ìš°ë“œ ì‹œê°í™” (ì˜¤ë¥¸ìª½ ì—´ í•˜ë‹¨, ì¢Œìš° ë°°ì¹˜)
                if place.get('positive_keywords') or place.get('negative_keywords'):
                    st.markdown("**ğŸ“ í‚¤ì›Œë“œ ë¶„ì„**")
                    
                    wc_col1, wc_col2 = st.columns(2)
                    
                    # ê¸ì • ì›Œë“œ í´ë¼ìš°ë“œ
                    if place.get('positive_keywords'):
                        with wc_col1:
                            st.caption("âœ… ê¸ì •")
                            text = " ".join(place['positive_keywords'])
                            if text:
                                img = generate_wordcloud(text, font_path, colormap="Greens")
                                if img is not None:
                                    st.image(img, use_container_width=True)
                    
                    # ë¶€ì • ì›Œë“œ í´ë¼ìš°ë“œ
                    if place.get('negative_keywords'):
                        with wc_col2:
                            st.caption("âŒ ë¶€ì •")
                            text = " ".join(place['negative_keywords'])
                            if text:
                                img = generate_wordcloud(text, font_path, colormap="Reds")
                                if img is not None:
                                    st.image(img, use_container_width=True)
                
            # ì§€ë„ ë° ë¡œë“œë·° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            if place.get('geometry') and place['geometry'].get('location'):
                lat, lng = place['geometry']['location']['lat'], place['geometry']['location']['lng']
                
                map_key = f"map_{i}_{place['place_id']}"
                streetview_key = f"street_{i}_{place['place_id']}"
                
                if map_key not in st.session_state:
                    st.session_state[map_key] = False
                if streetview_key not in st.session_state:
                    st.session_state[streetview_key] = False
                
                col1, col2 = st.columns(2)
                
                # ë²„íŠ¼ í´ë¦­ ì‹œ ìƒíƒœ í† ê¸€ í›„ ì¬ì‹¤í–‰í•˜ì—¬ ì§€ë„ í‘œì‹œ
                if col1.button("ğŸ—ºï¸ ì§€ë„ ë³´ê¸°", key=f"btn_{map_key}"):
                    st.session_state[map_key] = not st.session_state[map_key]
                    st.rerun()
                
                if col2.button("ğŸš— ë¡œë“œë·° ë³´ê¸°", key=f"btn_{streetview_key}"):
                    st.session_state[streetview_key] = not st.session_state[streetview_key]
                    st.rerun()
                
                if st.session_state[map_key] or st.session_state[streetview_key]:
                    st.markdown("**ğŸ“ ìœ„ì¹˜ ì •ë³´**")
                    
                    map_col1, map_col2 = st.columns(2)
                    
                    if st.session_state[map_key]:
                        with map_col1:
                            st.markdown("**ğŸ—ºï¸ ì§€ë„**")
                            # Google Maps Embed API (ì „ì²´ í­ ì‚¬ìš©)
                            map_url = f"https://www.google.com/maps/embed/v1/place?key={st.session_state.gmaps_key}&q={lat},{lng}"
                            st.markdown(
                                f'<iframe src="{map_url}" width="100%" height="450" style="border:0;" allowfullscreen="" loading="lazy"></iframe>',
                                unsafe_allow_html=True
                            )
                    
                    if st.session_state[streetview_key]:
                        with map_col2:
                            st.markdown("**ğŸš— ë¡œë“œë·°**")
                            # Google Maps Street View Embed API (ì „ì²´ í­ ì‚¬ìš©)
                            streetview_url = f"https://www.google.com/maps/embed/v1/streetview?key={st.session_state.gmaps_key}&location={lat},{lng}"
                            st.markdown(
                                f'<iframe src="{streetview_url}" width="100%" height="450" style="border:0;" allowfullscreen="" loading="lazy"></iframe>',
                                unsafe_allow_html=True
                            )
            else:
                st.info("ğŸ“ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ì–´ ì§€ë„ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
