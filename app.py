import streamlit as st
import googlemaps
from openai import OpenAI
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Tuple
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import os
from pathlib import Path
from dotenv import load_dotenv
import json
import re
from wordcloud import WordCloud
import numpy as np
import warnings # ê²½ê³  ë©”ì‹œì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€
import folium
from folium.plugins import HeatMap, MarkerCluster
from streamlit_folium import st_folium
import time
import random
import tab4_analysis
from scipy import stats

# NLP ëª¨ë¸ ê´€ë ¨ ì„í¬íŠ¸ (Hugging Face Transformers)
try:
    from sentence_transformers import SentenceTransformer
    from transformers import pipeline
    from transformers import logging as transformers_logging # íŠ¸ëœìŠ¤í¬ë¨¸ ë¡œê¹… ì„í¬íŠ¸
    # ëª…ì‹œì  ë¡œë“œë¥¼ ìœ„í•œ ì¶”ê°€ import
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except ImportError:
    st.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(sentence-transformers, transformers)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `pip install sentence-transformers transformers` ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    st.stop()


# ----------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •, ìºì‹± ë° ìœ í‹¸ë¦¬í‹°
# ----------------------------------------------------

# í°íŠ¸ ê²½ë¡œ ìºì‹± (WordCloudìš©)
@st.cache_resource(show_spinner=False)
def get_font_path():
    # ì‹¤ì œ í™˜ê²½ì— ë§ì¶° í°íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆì‹œ: NotoSansKR-Regular.ttf íŒŒì¼ì´ 'fonts' í´ë”ì— ìˆë‹¤ê³  ê°€ì •
    # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜, ì‹œìŠ¤í…œ í°íŠ¸ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # í˜„ì¬ëŠ” ë”ë¯¸ ê²½ë¡œë¡œ ì„¤ì •í•˜ë©°, ì‹¤í–‰ í™˜ê²½ì— ë”°ë¼ ìˆ˜ì • í•„ìš”
    try:
        # Streamlit í°íŠ¸ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì‹¤ì œ í°íŠ¸ ê²½ë¡œë¥¼ ì§€ì •
        return os.path.join(os.path.dirname(__file__), 'fonts', 'NotoSansKR-Regular.ttf')
    except:
        return None # í°íŠ¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°

font_path = get_font_path()

# í‘œë³¸ CSV ê²½ë¡œ
BASE_DIR = Path(__file__).resolve().parent
SAMPLED_CAFE_CSV = BASE_DIR / "ì„œìš¸ì‹œ_ìƒê¶Œ_ì¹´í˜ë¹µ_í‘œë³¸.csv"
GOOGLE_REVIEW_SAMPLE_CSV = BASE_DIR / "google_reviews_sample.csv"  # ì‚¬ì „ ìˆ˜ì§‘(2500ê°œ) íŒŒì¼
GOOGLE_REVIEW_LIVE_CSV = BASE_DIR / "google_reviews_live.csv"      # íƒ­3ì—ì„œ ìƒˆë¡œ ìˆ˜ì§‘í•œ ê²°ê³¼
FULL_CAFE_CSV = Path(__file__).resolve().parent / "ì„œìš¸ì‹œ_ìƒê¶Œ_ì¹´í˜ë¹µ.csv"
SAMPLED_CAFE_WITH_TRANSIT_CSV = BASE_DIR / "ì„œìš¸ì‹œ_ìƒê¶Œ_ì¹´í˜ë¹µ_í‘œë³¸_with_transit.csv"


@st.cache_data(show_spinner="í‘œë³¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
def load_sampled_cafes(csv_path: Path) -> pd.DataFrame:
    if not csv_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

    try:
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
    except UnicodeDecodeError:
        df = pd.read_csv(csv_path, encoding="cp949")
    return df

@st.cache_data(show_spinner="ì „ì²´ ì¹´í˜ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
def load_full_cafes(csv_path: Path) -> pd.DataFrame:
    if not csv_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

    try:
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
    except UnicodeDecodeError:
        df = pd.read_csv(csv_path, encoding="cp949")
    return df


@st.cache_data(show_spinner="Google ë¦¬ë·° ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
def load_google_reviews_csv(csv_path: Path) -> pd.DataFrame:
    if not csv_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

    try:
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
    except UnicodeDecodeError:
        df = pd.read_csv(csv_path, encoding="cp949")
    return df


@st.cache_data(show_spinner=False)
def load_csv_generic(csv_path: Path) -> pd.DataFrame:
    if not csv_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
    try:
        return pd.read_csv(csv_path, encoding="utf-8-sig")
    except UnicodeDecodeError:
        return pd.read_csv(csv_path, encoding="cp949")

# WordCloud ì´ë¯¸ì§€ ìºì‹± (PIL ì´ë¯¸ì§€ ë°˜í™˜)
@st.cache_data(show_spinner=False)
def generate_wordcloud(text: str, font_path: str, colormap: str = "Greens", **kwargs):
    if not text or not text.strip():
        return None
    
    # í°íŠ¸ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ê²½ê³ 
    if font_path and os.path.exists(font_path):
        font_kwarg = {'font_path': font_path}
    else:
        # st.warning("WordCloud í°íŠ¸ íŒŒì¼ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font_kwarg = {}
        
    wc = WordCloud(
        **font_kwarg,
        background_color="white",
        width=600,
        height=300,
        scale=2,
        max_words=180,
        prefer_horizontal=0.9,
        colormap=colormap,
        collocations=True,
        normalize_plurals=False,
        relative_scaling=0.35,
        min_font_size=8,
        max_font_size=90,
        random_state=42,
        regexp=r"[ê°€-í£a-zA-Z]+" # í•œê¸€, ì˜ë¬¸ë§Œ í¬í•¨
    ).generate(text)
    return wc.to_image()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="Seoul Place Recommendation", page_icon="ğŸ—ºï¸", layout="wide")

# ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜ (OpenAI ê¸°ë°˜)
def semantic_split(text: str) -> list[str]:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•´ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    """
    try:
        prompt = f"""
        ì•„ë˜ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„(í•˜ë‚˜ì˜ ê°ì •ì´ë‚˜ í‰ê°€ë¥¼ ë‹´ì€ ë‹¨ë½)ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.
        ê° ë¬¸ì¥ì€ ë…ë¦½ì ì¸ íŒë‹¨ì´ ê°€ëŠ¥í•œ ë‹¨ìœ„ì—¬ì•¼ í•˜ë©°, ë¶ˆí•„ìš”í•œ ì ‘ì†ì‚¬ëŠ” ì œê±°í•˜ì„¸ìš”.
        ì¶œë ¥ì€ JSON ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.

        ì˜ˆì‹œ:
        ì…ë ¥: "ì¹´í˜ ë¶„ìœ„ê¸°ê°€ ì¢‹ê³  ì»¤í”¼ëŠ” ë§›ìˆì§€ë§Œ ì¢Œì„ì´ ì¢ì•„ìš”."
        ì¶œë ¥: ["ì¹´í˜ ë¶„ìœ„ê¸°ê°€ ì¢‹ë‹¤.", "ì»¤í”¼ê°€ ë§›ìˆë‹¤.", "ì¢Œì„ì´ ì¢ë‹¤."]

        ë¦¬ë·° í…ìŠ¤íŠ¸:
        {text}
        """

        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "semantic_split_schema",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "sentences": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["sentences"],
                        "additionalProperties": False
                    }
                }
            },
            temperature=0
        )
        content = resp.choices[0].message.content
        parsed = json.loads(content) if content else {"sentences": []}
        if isinstance(parsed, list):
            candidates = parsed
        else:
            candidates = parsed.get("sentences", [])
        cleaned = [c.strip() for c in candidates if isinstance(c, str) and len(c.strip()) > 2]
        if cleaned:
            return cleaned
    except Exception as e:
        print(f"Semantic split error: {e}")
        # í´ë°±ì€ ì•„ë˜ ì¼ë°˜ ë¶„ê¸°ì—ì„œ ìˆ˜í–‰
        pass
    # í´ë°±: ë¬¸ì¥ë¶€í˜¸ â†’ ì ‘ì† í‘œí˜„ 2ë‹¨ê³„ ë¶„í• 
    fallback_units = []
    for s in re.split(r'[.!?]\s*', text):
        if not s or not s.strip():
            continue
        parts = re.split(r'(?:í•˜ì§€ë§Œ|ê·¸ëŸ¬ë‚˜|ê·¸ëŸ°ë°|ì¸ë°|ì§€ë§Œ|ëŠ”ë°)', s)
        for p in parts:
            p = p.strip()
            if len(p) > 2:
                fallback_units.append(p)
    return fallback_units


# ìºì‹œëœ ì˜ë¯¸ ë¶„í•  ë˜í¼
@st.cache_data(show_spinner=False)
def cached_semantic_split(text: str) -> List[str]:
    return semantic_split(text)


# ìºì‹œëœ LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ë˜í¼
@st.cache_data(show_spinner=False)
def cached_unified_summary(review_text: str):
    try:
        sample = review_text[:1200]
        unified_prompt = f"""
        ë‹¤ìŒ ë¦¬ë·°ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¥ì†Œë¥¼ ë¶„ì„í•˜ì—¬ í•œ ë²ˆì— ê²°ê³¼ë§Œ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
        1) positive_keywords: ì¥ì†Œ/ì¥ì†Œì„± ê´€ë ¨ ê¸ì • í•µì‹¬ ë‹¨ì–´ ìµœëŒ€ 10ê°œ (ë¬¸ìì—´ ë°°ì—´)
        2) negative_keywords: ì¥ì†Œ/ì¥ì†Œì„± ê´€ë ¨ ë¶€ì • í•µì‹¬ ë‹¨ì–´ ìµœëŒ€ 10ê°œ (ë¬¸ìì—´ ë°°ì—´)
        3) summary: ì „ë°˜ì  ë¶„ìœ„ê¸°/ê³µê°„ íŠ¹ì„±/ì£¼ìš” ê²½í—˜ ì¤‘ì‹¬ì˜ 5~8ë¬¸ì¥ ìš”ì•½ (ë¬¸ìì—´)

        ë¦¬ë·° í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì •ì˜ êµ¬ê°„ ì•ˆì˜ í…ìŠ¤íŠ¸ë§Œ ì°¸ê³ í•˜ì„¸ìš”:
        ```
        {sample}
        ```
        """
        unified_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": unified_prompt}],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "unified_summary_schema",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "positive_keywords": {"type": "array", "items": {"type": "string"}},
                            "negative_keywords": {"type": "array", "items": {"type": "string"}},
                            "summary": {"type": "string"}
                        },
                        "required": ["positive_keywords", "negative_keywords", "summary"],
                        "additionalProperties": False
                    }
                }
            },
            temperature=0
        )
        content = unified_response.choices[0].message.content
        parsed = json.loads(content) if content else {"positive_keywords": [], "negative_keywords": [], "summary": ""}
        return {
            "positive_keywords": parsed.get("positive_keywords", []) or [],
            "negative_keywords": parsed.get("negative_keywords", []) or [],
            "summary": (parsed.get("summary") or "").strip() or "ë¦¬ë·° ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ LLM ìš”ì•½ì´ ì–´ë µìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        print(f"LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "positive_keywords": [],
            "negative_keywords": [],
            "summary": "LLM ìš”ì•½ ì‹¤íŒ¨. NLP ë¶„ì„ë§Œ ì§„í–‰ë¨."
        }

# ----------------------------------------------------
# 2. API í‚¤ ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ----------------------------------------------------

if "history" not in st.session_state:
    st.session_state.history = []

# API í‚¤ ë¡œë“œ
gmaps_key = os.getenv("Maps_API_KEY") or st.secrets.get("Maps_API_KEY", "")
openai_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")

if "gmaps_key" not in st.session_state:
    st.session_state.gmaps_key = gmaps_key or ""
if "openai_key" not in st.session_state:
    st.session_state.openai_key = openai_key or ""

# API í‚¤ ì…ë ¥ UI (í‚¤ê°€ ì—†ëŠ” ê²½ìš°)
if not st.session_state.gmaps_key or not st.session_state.openai_key:
    st.title("ğŸ—ºï¸ Seoul Place Recommendation and Spatial Evaluation System")

    st.info("API í‚¤ë¥¼ `.env` íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
    st.markdown("---")
    gmaps_input = st.text_input("Google Maps API Key", type="password")
    openai_input = st.text_input("OpenAI API Key", type="password")

    if st.button("Start"):
        if gmaps_input and openai_input:
            st.session_state.gmaps_key = gmaps_input
            st.session_state.openai_key = openai_input
            st.rerun()
        else:
            st.warning("Please enter both API keys.")
    st.stop()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OpenAI clientëŠ” í´ë°± ë¡œì§ì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ ì „ì—­ì ìœ¼ë¡œ ìœ ì§€)
try:
    gmaps = googlemaps.Client(key=st.session_state.gmaps_key)
    # LLM í´ë¼ì´ì–¸íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    client = OpenAI(api_key=st.session_state.openai_key)
except Exception as e:
    st.error(f"API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()


# ----------------------------------------------------
# 3. LangGraph State ì •ì˜ ë° ëª¨ë¸ ë¡œë”© (ìºì‹œ)
# ----------------------------------------------------

class AgentState(BaseModel):
    query: str
    places: Optional[List[Dict[str, Any]]] = Field(default_factory=list)
    answer: Optional[str] = ""

# ì¥ì†Œì„± ìš”ì¸ ì„ë² ë”© ë° ëª¨ë¸ ë¡œë“œ (Sentence-BERT)
def _compute_factors_hash(path: str = "factors.json") -> str:
    try:
        with open(path, "rb") as f:
            import hashlib
            return hashlib.md5(f.read()).hexdigest()
    except Exception:
        return ""

@st.cache_resource(show_spinner="ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_category_embeddings(factors_hash: str):
    try:
        # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©ì— ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©
        model = SentenceTransformer("jhgan/ko-sroberta-multitask")
        # factors.json íŒŒì¼ ë¡œë“œ
        with open("factors.json", "r", encoding="utf-8") as f:
            factors = json.load(f)
    except FileNotFoundError:
        st.error("ì˜¤ë¥˜: 'factors.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— íŒŒì¼ì„ ìƒì„±í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    except Exception as e:
        st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    embeddings = {}
    score_structure = {}
    
    # 11ê°œ ì„¸ë¶€ ìš”ì¸ì˜ ì •ì˜ ë¬¸ì¥ì„ ì„ë² ë”©
    for main_cat, subcats in factors.items():
        score_structure[main_cat] = {}
        for subcat, definition in subcats.items():
            emb = model.encode(definition, normalize_embeddings=True)
            embeddings[subcat] = emb
            score_structure[main_cat][subcat] = None
            
    return embeddings, model, score_structure

factors_hash = _compute_factors_hash()
category_embeddings, embed_model, new_score_structure_template = load_category_embeddings(factors_hash)

@st.cache_resource(show_spinner="ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_sentiment_model_tabularis():
    """
    ê³µê°œ ê°ì • ë¶„í¬í˜• ëª¨ë¸ (tabularisai/multilingual-sentiment-analysis) ê¸°ë°˜
    - ì¶œë ¥: 0.0 ~ 1.0 ì—°ì† ì ìˆ˜ (ë¶€ì •â†’ê¸ì •)
    """
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    import numpy as np

    model_name = "tabularisai/multilingual-sentiment-analysis"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    pipe = pipeline(
        "text-classification",
        model=model,
        tokenizer=tokenizer,
        return_all_scores=True,
        truncation=True,
        max_length=512,
    )
    weights = np.linspace(0, 1, 5)  # [0.0, 0.25, 0.5, 0.75, 1.0]

    def predict_score(sentences: List[str]):
        if not sentences:
            return []
        results = pipe(sentences, truncation=True, max_length=512)
        scores = []
        for res in results:
            probs = np.array([r['score'] for r in res])
            score = float(np.dot(probs, weights))
            scores.append(score)
        return scores

    return predict_score


sentiment_model = load_sentiment_model_tabularis()


# ----------------------------------------------------
# 4. LangGraph Node ì •ì˜
# ----------------------------------------------------

def search_places(state: AgentState):
    """Google Maps APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜"""
    if state.places is None:
        state.places = []
    try:
        # ì„œìš¸ì‹œì²­ ê¸°ì¤€(37.5665,126.9780) ë°˜ê²½ 10km ê²€ìƒ‰
        res = gmaps.places(query=state.query, language="ko", location="37.5665,126.9780", radius=10000)
        state.places = res.get('results', [])[:5]
    except Exception as e:
        st.error(f"Google Maps ì¥ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("Google Maps API í‚¤ê°€ ìœ íš¨í•œì§€, ë˜ëŠ” Places APIê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        state.places = []
    return state.dict()

def analyze_reviews(state: AgentState):
    """SBERT + Sentiment ê¸°ë°˜ ì •ëŸ‰ í‰ê°€ + LLM í•´ì„"""
    if state.places is None:
        state.places = []

    place_infos = []
    
    SIMILARITY_THRESHOLD = 0.35
    ALPHA, BETA = 0.75, 0.25  # ìœ ì‚¬ë„ ë¹„ì¤‘ ì¶”ê°€ ìƒí–¥
    
    # factors.jsonì€ í•œ ë²ˆë§Œ ë¡œë“œ (ì†ë„ ê°œì„ )
    with open("factors.json", "r", encoding="utf-8") as f:
        factor_definitions = json.load(f)
    
    for place in state.places:
        place_id = place.get("place_id")
        if not place_id:
            continue

        details = gmaps.place(place_id=place_id, language="ko").get('result', {})
        reviews = details.get('reviews', [])[:10]
        review_texts = [r['text'] for r in reviews if r.get('text')]
        if not review_texts:
            continue

        # 1) ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬ (LLM ì‚¬ìš©)
        review_text = "\n".join(review_texts)
        review_units = cached_semantic_split(review_text)
        if not review_units:
            review_units = cached_semantic_split(" ".join(review_texts))

        # 2) SBERT + ê°ì„±ëª¨ë¸ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        factor_sentiments = {f: [] for f in category_embeddings.keys()}
        # ë¦¬ë·° ìš”ì•½/í‚¤ì›Œë“œ (summary)ì™€ ì ìˆ˜ í•´ì„¤(explanation)ì€ ë¶„ë¦¬ ìƒì„±
        summary = "ë¦¬ë·° ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        positive_keywords: List[str] = []
        negative_keywords: List[str] = []

        # 2-1) ë¦¬ë·° ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ (summary ì „ìš©)
        try:
            cached = cached_unified_summary(review_text)
            positive_keywords = cached.get("positive_keywords", []) or []
            negative_keywords = cached.get("negative_keywords", []) or []
            summary = cached.get("summary", "") or "ë¦¬ë·° ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ LLM ìš”ì•½ì´ ì–´ë µìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            summary = "LLM ìš”ì•½ ì‹¤íŒ¨. NLP ë¶„ì„ë§Œ ì§„í–‰ë¨."
            positive_keywords, negative_keywords = [], []

        # ê°ì„±(0~1) ìŠ¤ì½”ì–´ ë° ë°°ì¹˜ ì„ë² ë”©/ìœ ì‚¬ë„
        sentiment_scores = sentiment_model(review_units)
        unit_embs = embed_model.encode(review_units, normalize_embeddings=True)
        subcat_list = list(category_embeddings.keys())
        factor_mat = np.stack([category_embeddings[s] for s in subcat_list], axis=0)
        sim_mat = np.matmul(unit_embs, factor_mat.T)

        for i, unit in enumerate(review_units):
            raw_sent = float(sentiment_scores[i]) if i < len(sentiment_scores) else 0.5
            # ê°ì„± ë³´ì •: í•˜í•œì„ 0.2ê¹Œì§€ ë‚®ì¶”ê³  ë²”ìœ„ë¥¼ 0.6ìœ¼ë¡œ í™•ì¥
            sent_adj = np.clip((raw_sent - 0.2) / 0.6, 0, 1)
            sims = sim_mat[i]
            for j, sim in enumerate(sims):
                # ìœ ì‚¬ë„ ë³´ì •: 0.2 ê¸°ì¤€ìœ¼ë¡œ ì™„í™”í•˜ê³  ìƒí•œ í­ í™•ì¥
                sim_adj = np.clip((float(sim) - 0.2) / 0.4, 0, 1)
                if sim_adj > 0:
                    f_name = subcat_list[j]
                    combined = ALPHA * sim_adj + BETA * sent_adj
                    # ì‹œê·¸ëª¨ì´ë“œ: ì¤‘ì‹¬ 0.3, ê¸°ìš¸ê¸° 1.6ë¡œ ë” ë¶€ë“œëŸ½ê²Œ
                    score_scaled = 1 / (1 + np.exp(-1.6 * (combined - 0.3)))
                    factor_sentiments[f_name].append(float(score_scaled))

        # 3) í‚¤ì›Œë“œ ê¸°ë°˜ ë¶€ìŠ¤íŒ… (ì„ë² ë”© í•œê³„ ë³´ì™„)
        keyword_boosts = {
            "ê³ ìœ ì„±": ["ë…íŠ¹", "ìœ ë‹ˆí¬", "ì°¨ë³„", "ì»¨ì…‰", "í…Œë§ˆ", "íŠ¹ìƒ‰", "ê°œì„±", "íŠ¹ë³„í•œ", "ì•„ì´ë´í‹°í‹°", "ìœ ì¼", "ë…ì°½"],
            "ë¬¸í™”ì  ë§¥ë½": ["ì „í†µ", "ì—­ì‚¬", "ë…„", "ì˜¤ë˜", "ì˜›", "ê³ í’", "ë¬¸í™”", "ë°°ê²½", "ìŠ¤í† ë¦¬", "ì„¸ì›”", "ë‚´ë ¥", "ìœ ì„œ", "ë ˆíŠ¸ë¡œ", "ë¹ˆí‹°ì§€", "í´ë˜ì‹", "ì•¤í‹°í¬", "ê³¼ê±°", "ì˜›ë‚ "],
            "ì§€ì—­ ì •ì²´ì„±": ["ì§€ì—­", "ë™ë„¤", "ë§ˆì„", "ê·¼ì²˜", "ì£¼ë³€", "ëª…ì†Œ", "ëœë“œë§ˆí¬", "ìƒì§•", "ëŒ€í‘œ", "ì‹ ì´Œ", "í™ëŒ€", "ê°•ë‚¨", "ì´íƒœì›", "ì—°ë‚¨", "ì„±ìˆ˜", "ì„ì§€ë¡œ", "ìµì„ ë™", "ë¶ì´Œ", "ì‚¼ì²­ë™", "ì¢…ë¡œ", "ëª…ë™"],
            "ê¸°ì–µ/ê²½í—˜": ["ì¶”ì–µ", "ê°ë™", "ì¸ìƒ", "íŠ¹ë³„", "ìŠì„ ìˆ˜", "ê¸°ì–µ", "íšŒìƒ", "ê²½í—˜", "ëŠë‚Œ"],
            "ì‹¬ë¯¸ì„±": ["ì˜ˆì˜", "ì•„ë¦„", "ë©‹ì§€", "ì„¸ë ¨", "ì•¼ê²½", "ë·°", "ì¸í…Œë¦¬ì–´", "ë””ìì¸", "ì¡°ëª…", "ì•„ëŠ‘", "ë¶„ìœ„ê¸°", "ê°ì„±"],
            "ê°ê°ì  ê²½í—˜": ["ìŒì•…", "í–¥", "ëƒ„ìƒˆ", "ì§ˆê°", "ë§›", "ì˜¤ê°", "ê°ê°", "ì†Œë¦¬", "ì´‰ê°"],
            "ì¾Œì ì„±": ["ì²­ê²°", "ê¹¨ë—", "ë°", "í†µí’", "í™”ì¥ì‹¤", "ìœ„ìƒ", "ì •ëˆ", "ì¾Œì "],
            "ì ‘ê·¼ì„±": ["ê°€ê¹", "ì ‘ê·¼", "ì—­", "ì •ë¥˜ì¥", "ë„ë³´", "ë¶„ ê±°ë¦¬", "í¸ë¦¬", "ì§€í•˜ì² ì—­", "ë²„ìŠ¤ì •ë¥˜ì¥", "ì—­ì—ì„œ", "ì—­ê¹Œì§€", "ì •ë¥˜ì¥ì—ì„œ", "ì •ë¥˜ì¥ê¹Œì§€", "ëŒ€ì¤‘êµí†µ", "êµí†µí¸", "ì˜¤ê¸° ì‰¬", "ì°¾ê¸° ì‰¬", "ìœ„ì¹˜ ì¢‹", "êµí†µ ì¢‹"],
            "í™œë™ì„±": ["ëŒ€í™”", "ì—…ë¬´", "ì‘ì—…", "íšŒì˜", "ê³µë¶€", "í™œë™", "ëª¨ì„", "ìŠ¤í„°ë””"],
            "ì‚¬íšŒì„±": ["ì¹œì ˆ", "ì„œë¹„ìŠ¤", "êµë¥˜", "ì†Œí†µ", "ì¹œê·¼", "ì¸ì‚¬", "ë°°ë ¤"],
            "í˜•íƒœì„±": ["ë„“", "ê³µê°„", "êµ¬ì¡°", "ë°°ì¹˜", "ê°œë°©", "ë™ì„ ", "ì¸µ", "ë£¸"],
        }
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì§ì ‘ ê³ ì ìˆ˜ í• ë‹¹
        for factor, keywords in keyword_boosts.items():
            matched_kws = [kw for kw in keywords if kw in review_text]
            match_count = len(matched_kws)
            if match_count > 0:
                # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ìˆ˜ì— ë¹„ë¡€í•´ 0.75~0.95 í• ë‹¹
                boosted_score = min(0.75 + (match_count * 0.05), 0.95)
                # ì—¬ëŸ¬ ë²ˆ ì¶”ê°€í•´ í‰ê· ì—ì„œë„ ë†’ì€ ê°€ì¤‘ì¹˜ ìœ ì§€
                for _ in range(3):
                    factor_sentiments[factor].append(boosted_score)
                print(f"[BOOST] {factor}: {match_count}ê°œ í‚¤ì›Œë“œ ë§¤ì¹­ ({', '.join(matched_kws[:3])}) â†’ {boosted_score:.2f}")
        
        # 3-0) ì ‘ê·¼ì„± ì „ìš© íŒ¨í„´ ë§¤ì¹­ (ì‹œê°„+ê±°ë¦¬ í‘œí˜„ ê°•í™”)
        accessibility_patterns = [
            # ë„ë³´ ì‹œê°„ í‘œí˜„: "ë„ë³´ë¡œ 5ë¶„", "ë„ë³´ 10ë¶„", "5ë¶„ ë„ë³´" ë“±
            r'ë„ë³´\s*(?:ë¡œ\s*)?(\d+)\s*ë¶„',
            r'(\d+)\s*ë¶„\s*ë„ë³´',
            r'ë„ë³´\s*ë¡œ\s*(\d+)\s*ë¶„\s*(?:ì´ë©´|ë§Œì—|ê±¸ë¦¼|ê±¸ë ¤|ê°€ëŠ¥)',
            # ê±°ë¦¬ í‘œí˜„: "5ë¶„ ê±°ë¦¬", "10ë¶„ ê±°ë¦¬"
            r'(\d+)\s*ë¶„\s*ê±°ë¦¬',
            # ì—­/ì •ë¥˜ì¥ + ì‹œê°„: "ì§€í•˜ì² ì—­ì—ì„œ 5ë¶„", "ì—­ê¹Œì§€ 10ë¶„"
            r'(?:ì§€í•˜ì² ì—­|ì—­|ë²„ìŠ¤ì •ë¥˜ì¥|ì •ë¥˜ì¥)(?:ì—ì„œ|ê¹Œì§€)\s*(\d+)\s*ë¶„',
            r'(\d+)\s*ë¶„\s*(?:ì´ë©´|ë§Œì—|ê±¸ë¦¼|ê±¸ë ¤)\s*(?:ê°ˆ\s*ìˆ˜|ë„ì°©|ê°€ëŠ¥)',
            # "5ë¶„ì´ë©´ ê°ˆ ìˆ˜ ìˆë‹¤" ê°™ì€ í‘œí˜„
            r'(\d+)\s*ë¶„\s*ì´ë©´\s*(?:ê°ˆ\s*ìˆ˜|ë„ì°©|ê°€ëŠ¥)',
        ]
        
        accessibility_score_boost = 0.0
        matched_patterns = []
        for pattern in accessibility_patterns:
            matches = re.finditer(pattern, review_text, re.IGNORECASE)
            for match in matches:
                time_str = match.group(1) if match.groups() else None
                if time_str:
                    try:
                        time_minutes = int(time_str)
                        # 5ë¶„ ì´í•˜: ë§¤ìš° ë†’ì€ ì ìˆ˜ (0.90~0.95)
                        # 10ë¶„ ì´í•˜: ë†’ì€ ì ìˆ˜ (0.85~0.90)
                        # 15ë¶„ ì´í•˜: ë³´í†µ ì ìˆ˜ (0.75~0.85)
                        if time_minutes <= 5:
                            boost = 0.95
                        elif time_minutes <= 10:
                            boost = 0.90
                        elif time_minutes <= 15:
                            boost = 0.85
                        else:
                            boost = 0.80
                        
                        if boost > accessibility_score_boost:
                            accessibility_score_boost = boost
                        matched_patterns.append(f"{time_minutes}ë¶„ ({match.group(0)})")
                    except ValueError:
                        pass
        
        if matched_patterns:
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë§¤ìš° ë†’ì€ ì ìˆ˜ ë¶€ì—¬ (5ë²ˆ ì¶”ê°€ë¡œ í‰ê·  ê°€ì¤‘ì¹˜ ê°•í™”)
            for _ in range(5):
                factor_sentiments["ì ‘ê·¼ì„±"].append(accessibility_score_boost)
            print(f"[ACCESSIBILITY PATTERN] ì ‘ê·¼ì„±: {len(matched_patterns)}ê°œ íŒ¨í„´ ë§¤ì¹­ ({', '.join(matched_patterns[:3])}) â†’ {accessibility_score_boost:.2f}")
        
        # 3-1) ì„¸ë¶€ìš”ì¸ë³„ í‰ê·  ì ìˆ˜ (ì •ê·œí™” í¬í•¨)
        scores = json.loads(json.dumps(new_score_structure_template))
        all_vals = []
        for vals in factor_sentiments.values():
            all_vals.extend(vals)
        if all_vals:
            vmin, vmax = float(np.min(all_vals)), float(np.max(all_vals))
        else:
            vmin, vmax = 0.5, 0.5

        for main_cat, subcats in scores.items():
            for subcat in subcats.keys():
                vals = factor_sentiments.get(subcat, [])
                if vals and vmax > vmin:
                    raw = float(np.mean(vals))
                    # 0.20~1.0 ë²”ìœ„ë¡œ min-max ì •ê·œí™”
                    normed = 0.20 + 0.80 * ((raw - vmin) / (vmax - vmin + 1e-8))
                    scores[main_cat][subcat] = float(np.clip(normed, 0.20, 1.0))
                elif vals:
                    scores[main_cat][subcat] = float(np.clip(vals[0], 0.20, 1.0))
                else:
                    scores[main_cat][subcat] = 0.5

        # 4) LLM ê¸°ë°˜ ì ìˆ˜ ê²€ì¦ ë° ë³´ì • (GPT-4o ì¶”ë¡ )
        corrected_scores = json.loads(json.dumps(scores))  # ë³´ì • ì „ ë³µì‚¬
        correction_log = []
        try:
            sample_reviews = "\n".join(review_texts[:3])  # ë¦¬ë·° 5ê°œâ†’3ê°œë¡œ ì¶•ì†Œ
            
            validation_prompt = f"""
ë‹¹ì‹ ì€ ì¥ì†Œì„± í‰ê°€ ê°ì‚¬ìì…ë‹ˆë‹¤.
ì…ë ¥ëœ ì ìˆ˜ëŠ” SBERT + ê°ì„± íšŒê·€ëª¨ë¸ë¡œ ì‚°ì¶œëœ ê°’ì…ë‹ˆë‹¤.
ê° ìš”ì¸ë³„ ì ìˆ˜ì˜ íƒ€ë‹¹ì„±ì„ **ìš”ì¸ì˜ ì •ì˜ì— ë”°ë¼** ì •í™•íˆ ê²€í† í•˜ì„¸ìš”.

## ìš”ì¸ ì •ì˜ (ë°˜ë“œì‹œ ì°¸ê³ )
{json.dumps(factor_definitions, ensure_ascii=False, indent=2)}

## í˜„ì¬ ì ìˆ˜
{json.dumps(scores, ensure_ascii=False, indent=2)}

## ë¦¬ë·° ë‚´ìš©
{sample_reviews}

## ê²€í†  ê·œì¹™
1. ê° ìš”ì¸ì˜ ì •ì˜ì™€ í‚¤ì›Œë“œë¥¼ **ì •í™•íˆ** í™•ì¸í•˜ì„¸ìš”.
   ì˜ˆ: "ê°ê°ì  ê²½í—˜"ì€ ìŒì•…, í–¥ê¸°, ì§ˆê° ë“± ì˜¤ê° ìê·¹ / "ë¬¸í™”ì  ë§¥ë½"ì€ ì—­ì‚¬, ì „í†µ, ì§€ì—­ ë°°ê²½
2. ë¦¬ë·°ì—ì„œ í•´ë‹¹ ìš”ì¸ ì •ì˜ì— ë§ëŠ” ì–¸ê¸‰ì´ ìˆëŠ”ë° ì ìˆ˜ê°€ ë‚®ê±°ë‚˜, ì–¸ê¸‰ì´ ì—†ëŠ”ë° ì ìˆ˜ê°€ ë†’ìœ¼ë©´ delta ì œì•ˆ
3. deltaëŠ” -0.5 ~ +0.5 ë²”ìœ„
4. ê·¼ê±°ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±

## ì ‘ê·¼ì„± íŠ¹ë³„ ê²€í†  ê°€ì´ë“œ
**ì ‘ê·¼ì„±**ì€ ëŒ€ì¤‘êµí†µ ì ‘ê·¼, ë„ë³´ ê°€ëŠ¥ì„± ë“± ì¥ì†Œë¥¼ ì‰½ê²Œ ì°¾ì•„ì˜¤ê³  ì´ìš©í•  ìˆ˜ ìˆëŠ” ì •ë„ì…ë‹ˆë‹¤.
ë‹¤ìŒê³¼ ê°™ì€ í‘œí˜„ì´ ë¦¬ë·°ì— ìˆìœ¼ë©´ ì ‘ê·¼ì„± ì ìˆ˜ê°€ ë†’ì•„ì•¼ í•©ë‹ˆë‹¤:
- "ì§€í•˜ì² ì—­ì—ì„œ ë„ë³´ë¡œ 5ë¶„ì´ë©´ ê°ˆ ìˆ˜ ìˆë‹¤" â†’ ì ‘ê·¼ì„± ë§¤ìš° ë†’ìŒ (0.9 ì´ìƒ)
- "ë²„ìŠ¤ì •ë¥˜ì¥ì—ì„œ 10ë¶„ ê±°ë¦¬" â†’ ì ‘ê·¼ì„± ë†’ìŒ (0.85 ì´ìƒ)
- "ì—­ê¹Œì§€ 5ë¶„", "ë„ë³´ 5ë¶„", "5ë¶„ ê±°ë¦¬" â†’ ì ‘ê·¼ì„± ë†’ìŒ
- "ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì´ í¸ë¦¬í•˜ë‹¤", "êµí†µí¸ì´ ì¢‹ë‹¤" â†’ ì ‘ê·¼ì„± ë†’ìŒ
- "ê°€ê¹Œìš´ ê³³", "ì ‘ê·¼í•˜ê¸° ì‰¬ìš´ ìœ„ì¹˜" â†’ ì ‘ê·¼ì„± ë†’ìŒ

ë§Œì•½ ë¦¬ë·°ì— ìœ„ì™€ ê°™ì€ í‘œí˜„ì´ ìˆëŠ”ë° ì ‘ê·¼ì„± ì ìˆ˜ê°€ ë‚®ë‹¤ë©´(0.7 ì´í•˜), deltaë¥¼ +0.2~+0.3ìœ¼ë¡œ ì œì•ˆí•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSONë§Œ)
{{
  "corrections": [
    {{"factor": "ì¾Œì ì„±", "delta": 0.15, "reason": "ì²­ê²°, í™”ì¥ì‹¤, ì¶©ì „ì‹œì„¤ ê¸ì • ì–¸ê¸‰ ë§ìŒ"}},
    {{"factor": "ê°ê°ì  ê²½í—˜", "delta": 0.12, "reason": "ë””ì €íŠ¸ ë§›ê³¼ ë‹¤ì–‘ì„± ê°•ì¡°"}}
  ]
}}

ë³´ì • ë¶ˆí•„ìš” ì‹œ: {{"corrections": []}}
"""
            resp = client.chat.completions.create(
                model="gpt-4o",  # ë³´ì •ì€ ì •í™•í•œ ì¶”ë¡ ì´ í•„ìš”í•˜ë¯€ë¡œ gpt-4o ì‚¬ìš©
                messages=[{"role": "user", "content": validation_prompt}],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=500,  # ì¶©ë¶„í•œ í† í°ìœ¼ë¡œ ì •í™•í•œ ë³´ì •
            )
            correction_result = json.loads(resp.choices[0].message.content)
            corrections = correction_result.get("corrections", [])
            
            print(f"[DEBUG] GPT-4o ì‘ë‹µ: {correction_result}")  # ë””ë²„ê¹…ìš©
            
            # ë³´ì • ì ìš©
            for correction in corrections:
                if isinstance(correction, dict):
                    factor_name = correction.get("factor", "")
                    delta = float(correction.get("delta", 0))
                    reason = correction.get("reason", "")
                    
                    # ìš”ì¸ëª… ë§¤ì¹­ í›„ ì ìˆ˜ ë³´ì •
                    for main_cat, subcats in corrected_scores.items():
                        if factor_name in subcats:
                            old_val = subcats[factor_name]
                            new_val = np.clip(old_val + delta, 0.20, 1.0)
                            corrected_scores[main_cat][factor_name] = float(new_val)
                            correction_log.append({
                                "factor": factor_name,
                                "original": round(old_val, 2),
                                "adjusted": round(new_val, 2),
                                "delta": round(delta, 2),
                                "reason": reason
                            })
                            break
            
            # ë³´ì •ëœ ì ìˆ˜ë¥¼ ìµœì¢… ì ìˆ˜ë¡œ ì‚¬ìš©
            scores = corrected_scores
            
            if correction_log:
                print(f"[INFO] {len(correction_log)}ê°œ ìš”ì¸ ë³´ì •ë¨")
            else:
                print(f"[INFO] ë³´ì • í•„ìš” ì—†ìŒ")
            
        except Exception as e:
            print(f"[ERROR] LLM ì ìˆ˜ ë³´ì • ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            correction_log = []

        # 5) LLM ê¸°ë°˜ í•´ì„(explanation) - ë³´ì •ëœ ì ìˆ˜ ê¸°ì¤€ (ê°„ëµí™”)
        try:
            # ëŒ€í‘œ ì ìˆ˜ë§Œ ì¶”ì¶œ (ìƒìœ„ 3ê°œ + í•˜ìœ„ 2ê°œ)
            flat_scores = [(f"{mc}/{sc}", v) for mc, subs in scores.items() for sc, v in subs.items()]
            flat_scores.sort(key=lambda x: x[1], reverse=True)
            top_factors = flat_scores[:3]
            low_factors = flat_scores[-2:]
            
            explanation_prompt = f"""
ì•„ë˜ ì ìˆ˜ì—ì„œ ìƒìœ„/í•˜ìœ„ ìš”ì¸ì˜ ì´ìœ ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
ìƒìœ„: {", ".join([f"{f}({v:.2f})" for f, v in top_factors])}
í•˜ìœ„: {", ".join([f"{f}({v:.2f})" for f, v in low_factors])}
ë¦¬ë·°: {sample_reviews[:500]}
"""
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": explanation_prompt}],
                temperature=0.2,
                max_tokens=300,  # í† í° ì œí•œ
            )
            explanation = resp.choices[0].message.content.strip()
        except Exception as e:
            explanation = f"LLM í•´ì„ ì‹¤íŒ¨: {e}"

        # 6) ê²°ê³¼ ì €ì¥
        place_infos.append({
            'name': place.get('name', 'ì´ë¦„ ì—†ìŒ'), 
            'summary': summary,
            'address': place.get('formatted_address', place.get('vicinity', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')),
            'scores': scores, 
            'geometry': place.get('geometry', {}), 
            'place_id': place.get('place_id', ''),
            'positive_keywords': positive_keywords, 
            'negative_keywords': negative_keywords, 
            'explanation': explanation,
            'corrections': correction_log,  # ë³´ì • ë‚´ì—­ ì¶”ê°€
        })

    state.places = place_infos
    return state.dict()

# ----------------------------------------------------
# 5. LangGraph êµ¬ì„±
# ----------------------------------------------------

graph = StateGraph(AgentState)
graph.add_node("search_places", search_places)
graph.add_node("analyze_reviews", analyze_reviews)
graph.set_entry_point("search_places")
graph.add_edge("search_places", "analyze_reviews")
graph.add_edge("analyze_reviews", END)
agent = graph.compile()


# ----------------------------------------------------
# 6. ì‹¤í—˜ìš© ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ í•¨ìˆ˜
# ----------------------------------------------------

# ì„œìš¸ì‹œ 25ê°œ êµ¬ ëª©ë¡
SEOUL_DISTRICTS = [
    "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ìš©ì‚°êµ¬", "ì„±ë™êµ¬", "ê´‘ì§„êµ¬", 
    "ë™ëŒ€ë¬¸êµ¬", "ì¤‘ë‘êµ¬", "ì„±ë¶êµ¬", "ê°•ë¶êµ¬", "ë„ë´‰êµ¬",
    "ë…¸ì›êµ¬", "ì€í‰êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ë§ˆí¬êµ¬", "ì–‘ì²œêµ¬",
    "ê°•ì„œêµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬", "ì˜ë“±í¬êµ¬", "ë™ì‘êµ¬",
    "ê´€ì•…êµ¬", "ì„œì´ˆêµ¬", "ê°•ë‚¨êµ¬", "ì†¡íŒŒêµ¬", "ê°•ë™êµ¬"
]

# ì„œìš¸ì‹œ ì¤‘ì‹¬ ì¢Œí‘œ
SEOUL_CENTER = (37.5665, 126.9780)

def collect_cafes_in_district(district: str, max_results: int = 50) -> List[Dict]:
    """
    íŠ¹ì • êµ¬ì˜ ì¹´í˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    try:
        query = f"ì„œìš¸íŠ¹ë³„ì‹œ {district} ì¹´í˜"
        # Places APIë¡œ ê²€ìƒ‰
        results = gmaps.places(
            query=query,
            language="ko",
            type="cafe"
        ).get('results', [])
        
        # place_id, ì´ë¦„, ìœ„ì¹˜, ì£¼ì†Œ ë“± ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        cafes = []
        for place in results[:max_results]:
            if place.get('geometry') and place.get('place_id'):
                cafes.append({
                    'place_id': place['place_id'],
                    'name': place.get('name', ''),
                    'lat': place['geometry']['location']['lat'],
                    'lng': place['geometry']['location']['lng'],
                    'address': place.get('formatted_address', place.get('vicinity', '')),
                    'district': district,
                    'rating': place.get('rating', None),
                    'user_ratings_total': place.get('user_ratings_total', 0)
                })
        
        print(f"[INFO] {district}: {len(cafes)}ê°œ ì¹´í˜ ìˆ˜ì§‘")
        return cafes
    
    except Exception as e:
        print(f"[ERROR] {district} ì¹´í˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

@st.cache_data(ttl=3600*24, show_spinner="ì„œìš¸ ì „ì—­ ì¹´í˜ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
def collect_all_cafes_seoul(_gmaps_client, max_per_district: int = 30) -> pd.DataFrame:
    """
    ì„œìš¸ ì „ì²´ 25ê°œ êµ¬ì˜ ì¹´í˜ ë°ì´í„°ë¥¼ ë³‘ë ¬ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    all_cafes = []
    
    # ìˆœì°¨ ìˆ˜ì§‘ (API quota ê³ ë ¤)
    for district in SEOUL_DISTRICTS:
        cafes = collect_cafes_in_district(district, max_per_district)
        all_cafes.extend(cafes)
        time.sleep(0.5)  # API rate limit ë°©ì§€
    
    df = pd.DataFrame(all_cafes)
    
    # ì¤‘ë³µ ì œê±° (place_id ê¸°ì¤€)
    if not df.empty:
        df = df.drop_duplicates(subset=['place_id']).reset_index(drop=True)
        print(f"[INFO] ì´ {len(df)}ê°œ ì¹´í˜ ìˆ˜ì§‘ ì™„ë£Œ")
    
    return df

def calculate_transit_accessibility(lat: float, lng: float, max_distance: int = 600) -> Tuple[float, str, str, float, float]:
    """
    íŠ¹ì • ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì§€í•˜ì² ì—­/ë²„ìŠ¤ì •ë¥˜ì¥ê¹Œì§€ì˜ ë„ë³´ ì‹œê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Returns:
        (ë„ë³´_ë¶„, ìµœê·¼ì ‘_ì—­ëª…, íƒ€ì…, ë„ë³´_ê±°ë¦¬(m), ì§ì„ _ê±°ë¦¬(m))
    """
    try:
        print(f"[DEBUG] ì ‘ê·¼ì„± ê³„ì‚° ì‹œì‘: lat={lat}, lng={lng}")
        
        # ë°˜ê²½ 600m ë‚´ ì§€í•˜ì² ì—­ ê²€ìƒ‰
        try:
            subway_results = gmaps.places_nearby(
                location=(lat, lng),
                radius=max_distance,
                type='subway_station',
                language='ko'
            ).get('results', [])
            print(f"[DEBUG] ì§€í•˜ì² ì—­ ê²€ìƒ‰ ê²°ê³¼: {len(subway_results)}ê°œ")
        except Exception as e:
            print(f"[ERROR] ì§€í•˜ì² ì—­ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            subway_results = []
        
        # ë°˜ê²½ 600m ë‚´ ë²„ìŠ¤ ì •ë¥˜ì¥ ê²€ìƒ‰
        try:
            bus_results = gmaps.places_nearby(
                location=(lat, lng),
                radius=max_distance,
                type='bus_station',
                language='ko'
            ).get('results', [])
            print(f"[DEBUG] ë²„ìŠ¤ì •ë¥˜ì¥ ê²€ìƒ‰ ê²°ê³¼: {len(bus_results)}ê°œ")
        except Exception as e:
            print(f"[ERROR] ë²„ìŠ¤ì •ë¥˜ì¥ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            bus_results = []
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¡°ê¸° ë°˜í™˜
        if not subway_results and not bus_results:
            print(f"[WARN] 600m ë‚´ ì—­/ì •ë¥˜ì¥ ì—†ìŒ")
            return None, "ì •ë³´ ì—†ìŒ", "ì—†ìŒ", float("nan"), float("nan")
        
        from math import radians, cos, sin, asin, sqrt

        def haversine(lat1, lon1, lat2, lon2):
            lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
            dlon = lon2 - lon1
            dlat = lat2 - lat1
            a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2
            c = 2 * asin(sqrt(a))
            return 6371 * c * 1000  # meters

        def compute_walking_time(target_lat, target_lng, target_name, target_type):
            attempts = [
                ((lat, lng), (target_lat, target_lng), "ì¹´í˜â†’ëŒ€ì¤‘êµí†µ"),
                ((target_lat, target_lng), (lat, lng), "ëŒ€ì¤‘êµí†µâ†’ì¹´í˜"),
            ]
            for origin, destination, attempt_label in attempts:
                try:
                    result = gmaps.distance_matrix(
                        origins=[origin],
                        destinations=[destination],
                        mode="walking",
                        language="ko",
                        region="kr",
                    )
                    status = result["rows"][0]["elements"][0]["status"]
                    print(f"[DEBUG] Distance Matrix ì‘ë‹µ status ({attempt_label}): {status}")
                    if status == "OK":
                        duration_sec = result["rows"][0]["elements"][0]["duration"]["value"]
                        distance_m = result["rows"][0]["elements"][0]["distance"]["value"]
                        print(
                            f"[DEBUG] âœ“ {target_name} ({target_type}) - Distance Matrix({attempt_label}): "
                            f"{duration_sec/60:.1f}ë¶„ ({distance_m}m)"
                        )
                        return duration_sec / 60.0, distance_m, "distance_matrix"
                except Exception as e:
                    print(f"[WARN] Distance Matrix í˜¸ì¶œ ì‹¤íŒ¨ ({attempt_label}): {e}")

            try:
                directions = gmaps.directions(
                    origin=(lat, lng),
                    destination=(target_lat, target_lng),
                    mode="walking",
                    language="ko",
                    region="kr",
                )
                if directions:
                    leg = directions[0]["legs"][0]
                    duration_sec = leg["duration"]["value"]
                    distance_m = leg["distance"]["value"]
                    print(
                        f"[DEBUG] âœ“ {target_name} ({target_type}) - Directions API: "
                        f"{duration_sec/60:.1f}ë¶„ ({distance_m}m)"
                    )
                    return duration_sec / 60.0, distance_m, "directions"
                print(f"[WARN] Directions API ê²°ê³¼ ì—†ìŒ: {target_name} ({target_type})")
            except Exception as e:
                print(f"[WARN] Directions API í˜¸ì¶œ ì‹¤íŒ¨: {target_name} ({target_type}) â†’ {e}")
            distance_m = haversine(lat, lng, target_lat, target_lng)
            actual_distance_m = distance_m * 1.4
            duration = actual_distance_m / 67.0
            print(
                f"[DEBUG] âš  {target_name} ({target_type}): {duration:.1f}ë¶„ "
                f"(ì§ì„  {distance_m:.0f}m â†’ ì‹¤ì œê²½ë¡œ ì¶”ì • {actual_distance_m:.0f}m)"
            )
            return duration, actual_distance_m, "fallback"
        
        # ê°€ì¥ ê°€ê¹Œìš´ ì—­/ì •ë¥˜ì¥ ì°¾ê¸°
        min_walk_time = 999
        min_walk_distance = float("inf")
        min_straight_distance = float("inf")
        nearest_name = "ì •ë³´ ì—†ìŒ"
        nearest_type = "ì—†ìŒ"
        distance_matrix_success = False
        
        # ì§€í•˜ì² ì—­ ì²˜ë¦¬
        for idx, station in enumerate(subway_results[:3]):  # ìƒìœ„ 3ê°œë§Œ ê²€ì‚¬
            try:
                station_loc = station["geometry"]["location"]
                station_name = station.get("name", "ì§€í•˜ì² ì—­")
                print(f"[DEBUG] [{idx+1}/3] ì§€í•˜ì² ì—­ ë„ë³´ ì‹œê°„ ê³„ì‚°: {station_name}")

                straight_distance = haversine(lat, lng, station_loc["lat"], station_loc["lng"])
                duration, distance_m, _ = compute_walking_time(
                    station_loc["lat"], station_loc["lng"], station_name, "ì§€í•˜ì² ì—­"
                )
                distance_matrix_success = True
                if duration < min_walk_time:
                    min_walk_time = duration
                    min_walk_distance = distance_m
                    min_straight_distance = straight_distance
                    nearest_name = station_name
                    nearest_type = "ì§€í•˜ì² ì—­"

            except Exception as e:
                print(f"[ERROR] ì§€í•˜ì² ì—­ '{station_name}' ì²˜ë¦¬ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                import traceback

                traceback.print_exc()

        # ë²„ìŠ¤ì •ë¥˜ì¥ ì²˜ë¦¬
        for idx, bus in enumerate(bus_results[:3]):
            try:
                bus_loc = bus["geometry"]["location"]
                bus_name = bus.get("name", "ë²„ìŠ¤ì •ë¥˜ì¥")
                print(f"[DEBUG] [{idx+1}/3] ë²„ìŠ¤ì •ë¥˜ì¥ ë„ë³´ ì‹œê°„ ê³„ì‚°: {bus_name}")

                straight_distance = haversine(lat, lng, bus_loc["lat"], bus_loc["lng"])
                duration, distance_m, _ = compute_walking_time(
                    bus_loc["lat"], bus_loc["lng"], bus_name, "ë²„ìŠ¤ì •ë¥˜ì¥"
                )
                distance_matrix_success = True
                if duration < min_walk_time:
                    min_walk_time = duration
                    min_walk_distance = distance_m
                    min_straight_distance = straight_distance
                    nearest_name = bus_name
                    nearest_type = "ë²„ìŠ¤ì •ë¥˜ì¥"

            except Exception as e:
                print(f"[ERROR] ë²„ìŠ¤ì •ë¥˜ì¥ '{bus_name}' ì²˜ë¦¬ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                import traceback

                traceback.print_exc()
        
        # Distance Matrix APIê°€ í•œ ë²ˆë„ ì„±ê³µí•˜ì§€ ëª»í–ˆë‹¤ë©´
        if not distance_matrix_success:
            print(f"[ERROR] Distance Matrix API í˜¸ì¶œì´ ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            print(f"[INFO] ê°€ëŠ¥í•œ ì›ì¸:")
            print(f"  1. Distance Matrix APIê°€ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
            print(f"  2. API í‚¤ì— Distance Matrix API ê¶Œí•œì´ ì—†ìŒ")
            print(f"  3. Billingì´ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
            print(f"  4. API quota ì´ˆê³¼")
            return None, "Distance Matrix ì‹¤íŒ¨", "ì˜¤ë¥˜", float("nan"), float("nan")
        
        # ê²°ê³¼ ë°˜í™˜
        if min_walk_time < 999 and min_walk_distance != float("inf"):
            print(f"[SUCCESS] ìµœê·¼ì ‘: {nearest_name} ({nearest_type}), ë„ë³´ {min_walk_time:.1f}ë¶„")
            return (
                round(min_walk_time, 1),
                nearest_name,
                nearest_type,
                float(min_walk_distance),
                float(min_straight_distance),
            )
        else:
            print(f"[WARN] Distance Matrix í˜¸ì¶œì€ ì„±ê³µí–ˆì§€ë§Œ ìœ íš¨í•œ ê²½ë¡œë¥¼ ì°¾ì§€ ëª»í•¨")
            return None, "ê²½ë¡œ ì—†ìŒ", "ì—†ìŒ", float("nan"), float("nan")
    
    except Exception as e:
        print(f"[CRITICAL ERROR] ì ‘ê·¼ì„± ê³„ì‚° ì¹˜ëª…ì  ì˜¤ë¥˜: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return None, "ì¹˜ëª…ì  ì˜¤ë¥˜", "ì˜¤ë¥˜", float("nan"), float("nan")

def calculate_placeness_batch(df: pd.DataFrame, sample_size: int = None, progress_callback=None) -> pd.DataFrame:
    """
    ì¹´í˜ ë°ì´í„°í”„ë ˆì„ì— ëŒ€í•´ ì¥ì†Œì„± ì ìˆ˜ë¥¼ ì¼ê´„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    if sample_size and len(df) > sample_size:
        df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)
    
    results = []
    total = len(df)
    
    for idx, row in df.iterrows():
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        if progress_callback:
            progress_callback(idx + 1, total, row.get('name', '?'))
        try:
            # Place Details APIë¡œ ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
            details = gmaps.place(place_id=row['place_id'], language='ko').get('result', {})
            reviews = details.get('reviews', [])[:10]
            review_texts = [r['text'] for r in reviews if r.get('text')]
            
            if not review_texts:
                # ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                continue
            
            # ë¦¬ë·° í…ìŠ¤íŠ¸ ë³‘í•©
            review_text = "\n".join(review_texts)
            
            # ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬
            review_units = cached_semantic_split(review_text)
            if not review_units:
                continue
            
            # SBERT + ê°ì„± ëª¨ë¸ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° (analyze_reviews ë¡œì§ ì¬ì‚¬ìš©)
            factor_sentiments = {f: [] for f in category_embeddings.keys()}
            
            sentiment_scores = sentiment_model(review_units)
            unit_embs = embed_model.encode(review_units, normalize_embeddings=True)
            subcat_list = list(category_embeddings.keys())
            factor_mat = np.stack([category_embeddings[s] for s in subcat_list], axis=0)
            sim_mat = np.matmul(unit_embs, factor_mat.T)
            
            ALPHA, BETA = 0.75, 0.25
            
            for i, unit in enumerate(review_units):
                raw_sent = float(sentiment_scores[i]) if i < len(sentiment_scores) else 0.5
                sent_adj = np.clip((raw_sent - 0.3) / 0.7, 0, 1)
                sims = sim_mat[i]
                for j, sim in enumerate(sims):
                    sim_adj = np.clip((float(sim) - 0.3) / 0.5, 0, 1)
                    if sim_adj > 0:
                        f_name = subcat_list[j]
                        combined = ALPHA * sim_adj + BETA * sent_adj
                        score_scaled = 1 / (1 + np.exp(-2.2 * (combined - 0.4)))
                        factor_sentiments[f_name].append(float(score_scaled))
            
            # í‚¤ì›Œë“œ ë¶€ìŠ¤íŒ… (ì ‘ê·¼ì„± í¬í•¨)
            keyword_boosts = {
                "ê³ ìœ ì„±": ["ë…íŠ¹", "ìœ ë‹ˆí¬", "ì°¨ë³„", "ì»¨ì…‰", "í…Œë§ˆ", "íŠ¹ìƒ‰", "ê°œì„±", "ìœ ì¼", "ë…ì°½"],
                "ë¬¸í™”ì  ë§¥ë½": ["ì „í†µ", "ì—­ì‚¬", "ë…„", "ì˜¤ë˜", "ì˜›", "ê³ í’", "ë¬¸í™”", "ë°°ê²½", "ìŠ¤í† ë¦¬", "ì„¸ì›”", "ë‚´ë ¥", "ìœ ì„œ", "ë ˆíŠ¸ë¡œ", "ë¹ˆí‹°ì§€", "í´ë˜ì‹", "ì•¤í‹°í¬", "ê³¼ê±°", "ì˜›ë‚ "],
                "ì§€ì—­ ì •ì²´ì„±": ["ì§€ì—­", "ë™ë„¤", "ë§ˆì„", "ê·¼ì²˜", "ì£¼ë³€", "ëª…ì†Œ", "ëœë“œë§ˆí¬", "ìƒì§•", "ëŒ€í‘œ", "ì‹ ì´Œ", "í™ëŒ€", "ê°•ë‚¨", "ì´íƒœì›", "ì—°ë‚¨", "ì„±ìˆ˜", "ì„ì§€ë¡œ", "ìµì„ ë™", "ë¶ì´Œ", "ì‚¼ì²­ë™", "ì¢…ë¡œ", "ëª…ë™"],
                "ì‹¬ë¯¸ì„±": ["ì˜ˆì˜", "ì•„ë¦„", "ë©‹ì§€", "ì„¸ë ¨", "ì•¼ê²½", "ë·°", "ì¸í…Œë¦¬ì–´", "ë””ìì¸", "ì¡°ëª…", "ì•„ëŠ‘", "ë¶„ìœ„ê¸°", "ê°ì„±"],
                "ì ‘ê·¼ì„±": ["ê°€ê¹", "ì ‘ê·¼", "ì—­", "ì •ë¥˜ì¥", "ë„ë³´", "í¸ë¦¬"],
            }
            
            for factor, keywords in keyword_boosts.items():
                matched_kws = [kw for kw in keywords if kw in review_text]
                if matched_kws:
                    boosted_score = min(0.75 + (len(matched_kws) * 0.05), 0.95)
                    for _ in range(2):
                        factor_sentiments[factor].append(boosted_score)
            
            # ì ‘ê·¼ì„± íŒ¨í„´ ë§¤ì¹­
            accessibility_patterns = [
                r'ë„ë³´\s*(?:ë¡œ\s*)?(\d+)\s*ë¶„',
                r'(\d+)\s*ë¶„\s*ë„ë³´',
                r'(\d+)\s*ë¶„\s*ê±°ë¦¬',
            ]
            
            for pattern in accessibility_patterns:
                matches = re.finditer(pattern, review_text)
                for match in matches:
                    time_str = match.group(1)
                    if time_str:
                        try:
                            time_minutes = int(time_str)
                            if time_minutes <= 5:
                                boost = 0.95
                            elif time_minutes <= 10:
                                boost = 0.90
                            else:
                                boost = 0.85
                            factor_sentiments["ì ‘ê·¼ì„±"].append(boost)
                        except:
                            pass
            
            # ì ìˆ˜ ì •ê·œí™”
            scores = json.loads(json.dumps(new_score_structure_template))
            all_vals = []
            for vals in factor_sentiments.values():
                all_vals.extend(vals)
            
            if all_vals:
                vmin, vmax = float(np.min(all_vals)), float(np.max(all_vals))
            else:
                vmin, vmax = 0.5, 0.5
            
            for main_cat, subcats in scores.items():
                for subcat in subcats.keys():
                    vals = factor_sentiments.get(subcat, [])
                    if vals and vmax > vmin:
                        raw = float(np.mean(vals))
                        normed = 0.30 + 0.70 * ((raw - vmin) / (vmax - vmin + 1e-8))
                        scores[main_cat][subcat] = float(np.clip(normed, 0.30, 1.0))
                    elif vals:
                        scores[main_cat][subcat] = float(np.clip(vals[0], 0.30, 1.0))
                    else:
                        scores[main_cat][subcat] = 0.5
            
            # Overall ì ìˆ˜ ê³„ì‚° (ì „ì²´ í‰ê· )
            all_scores = [s for main_cat, sub_scores in scores.items() for s in sub_scores.values() if s is not None]
            overall_score = np.mean(all_scores) if all_scores else 0.5
            
            # ê²°ê³¼ ì €ì¥
            result = row.to_dict()
            result['overall_score'] = round(overall_score, 3)
            result['accessibility_score'] = round(scores.get('ë¬¼ë¦¬ì  íŠ¹ì„±', {}).get('ì ‘ê·¼ì„±', 0.5), 3)
            result['scores'] = scores
            results.append(result)
            
            print(f"[{idx+1}/{len(df)}] {row['name']}: overall={overall_score:.2f}, ì ‘ê·¼ì„±={result['accessibility_score']:.2f}")
            
        except Exception as e:
            print(f"[ERROR] {row.get('name', '?')} ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            continue
    
    return pd.DataFrame(results)


# ----------------------------------------------------
# 7. Streamlit UI
# ----------------------------------------------------

st.title("ì¥ì†Œì„± ìš”ì¸ ê¸°ë°˜ ê³µê°„ ì •ëŸ‰ í‰ê°€ ë„êµ¬")

# CSSë¡œ í…ìŠ¤íŠ¸ ìƒ‰ìƒ ê°•ì œ (ê°€ë…ì„± ê°œì„ )
st.markdown("""
<style>
    .stMarkdown, .stCaption, p, div {
        color: #000000 !important;
    }
    h1, h2, h3, h4, h5, h6 {
        color: #000000 !important;
    }
    /* ì˜ˆì‹œ í…ìŠ¤íŠ¸ëŠ” íšŒìƒ‰ ìœ ì§€ */
    .example-text {
        color: #888888 !important;
    }
</style>
""", unsafe_allow_html=True)

# íƒ­ êµ¬ì„±
tab1, tab2, tab4 = st.tabs(["ğŸ” ê°œë³„ ì¥ì†Œ ë¶„ì„", "ğŸ—ºï¸ ì„œìš¸ ì „ì—­ ì‹¤í—˜", "ğŸ“ Google ë¦¬ë·° ë¶„ì„"])

# ========================================
# íƒ­ 1: ê°œë³„ ì¥ì†Œ ë¶„ì„ (ê¸°ì¡´ ê¸°ëŠ¥)
# ========================================
with tab1:
    st.markdown("ë¶„ì„í•  ê³µê°„ì˜ ìœ„ì¹˜ì™€ ê°ì„±/ê¸°ëŠ¥ì  íŠ¹ì„±ì„ ì…ë ¥í•˜ì‹­ì‹œì˜¤. "
                 "<span class='example-text'>(ì˜ˆ: ì‹ ì´Œ ì¡°ìš©í•œ ì¹´í˜, ì¢…ë¡œêµ¬ ì „í†µì ì¸ ìŒì‹ì , ë§ˆí¬êµ¬ ì‚°ì±…ë¡œ ê³µì›)</span>", 
                 unsafe_allow_html=True)
    query = st.text_input("", placeholder="ì˜ˆ: ì‹ ì´Œ ì¡°ìš©í•œ ì¹´í˜", key="query_tab1")

    if st.button("ì¥ì†Œì„± ì •ëŸ‰ ë¶„ì„ ì‹œì‘", key="btn_analyze_tab1"):
        if not query.strip():
            st.warning("ì¥ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("NLP ë° LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ë¦¬ë·°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¥ì†Œì„± ìš”ì¸ì„ ì •ëŸ‰ í‰ê°€í•˜ëŠ” ì¤‘..."):
                result = agent.invoke({"query": query, "places": [], "answer": ""})
                places = result.get('places', [])
                st.session_state.history.append((query, places))
                st.rerun()

    # ê²°ê³¼ ì¶œë ¥
    if st.session_state.history:
        latest_query, latest_places = st.session_state.history[-1]
        st.markdown(f"---")
        st.markdown(f"### '{latest_query}'ì— ëŒ€í•œ ì¥ì†Œì„± í‰ê°€ ê²°ê³¼")

        for i, place in enumerate(latest_places):
            with st.container(border=True):
                st.subheader(place.get('name', 'ì´ë¦„ ì •ë³´ ì—†ìŒ'))
                st.markdown(f"**ğŸ“ ì£¼ì†Œ:** {place.get('address', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')}")
                
                # 2ì—´ ë ˆì´ì•„ì›ƒ: ì™¼ìª½(ì‹œê°í™”), ì˜¤ë¥¸ìª½(ë³´ì •/í•´ì„¤)
                col_left, col_right = st.columns([1.2, 1])
                
                scores = place.get('scores')
                
                # ========== ì™¼ìª½ ì—´: ë¦¬ë·° ìš”ì•½ + ì‹œê°í™” ==========
                with col_left:
                    st.markdown(f"**ğŸ“ ë¦¬ë·° ìš”ì•½**")
                    st.markdown(place.get('summary', 'ìš”ì•½ ì •ë³´ ì—†ìŒ'))
                
                with col_left:
                    if scores:
                        st.markdown(f"**ğŸ“Š ì¥ì†Œì„± ì¢…í•© í‰ê°€**")

                        # Sunburst ì°¨íŠ¸ ë°ì´í„° ìƒì„±
                        labels = []
                        parents = []
                        values = []
                        colors = []

                        # ë¶€ë“œëŸ¬ìš´ íŒŒìŠ¤í…”í†¤ ìƒ‰ìƒ ë§µ (factors.json êµ¬ì¡°ì™€ ë™ì¼í•œ ëŒ€ë¶„ë¥˜ ë¼ë²¨)
                        color_map = {
                            "ë¬¼ë¦¬ì  íŠ¹ì„±": "rgb(173, 216, 230)",     # ì—°í•œ íŒŒë€ìƒ‰ (Light Blue)
                            "í™œë™ì  íŠ¹ì„±": "rgb(152, 251, 152)",   # ì—°í•œ ì—°ë‘ìƒ‰ (Light Lime Green)
                            "ì˜ë¯¸ì  íŠ¹ì„±": "rgb(255, 182, 193)" # ì—°í•œ ë¶„í™ìƒ‰ (Light Pink)
                        }

                        # ë£¨íŠ¸ ë…¸ë“œ ì¶”ê°€ (ì „ì²´ ì ìˆ˜ì˜ í‰ê· ìœ¼ë¡œ ì„¤ì •)
                        all_scores = [s for main_cat, sub_scores in scores.items() for s in sub_scores.values() if s is not None]
                        total_score = sum(all_scores)
                        score_count = len(all_scores)
                        root_value = total_score / score_count if score_count > 0 else 0.5

                        labels.append(place['name'])
                        parents.append("")
                        values.append(root_value)
                        colors.append("#FFFFFF")

                        # ëŒ€ë¶„ë¥˜ì™€ ì„¸ë¶€ ë¶„ë¥˜ ì¶”ê°€
                        for main_cat, sub_scores in scores.items():
                            main_scores = [s for s in sub_scores.values() if s is not None]
                            main_avg = sum(main_scores) / len(main_scores) if main_scores else 0
                
                            labels.append(main_cat)
                            parents.append(place['name'])
                            values.append(main_avg)
                            colors.append(color_map.get(main_cat, "#CCCCCC"))
                
                            for sub_cat, score in sub_scores.items():
                                if score is not None:
                                    labels.append(f"{sub_cat}: {score:.2f}") # ì ìˆ˜ë¥¼ ë¼ë²¨ì— í¬í•¨
                                    parents.append(main_cat)
                                    values.append(float(score))
                                    colors.append(color_map.get(main_cat, "#CCCCCC"))
                
                        # Sunburst ì°¨íŠ¸ ìƒì„±
                        try:
                            fig_sunburst = go.Figure(go.Sunburst(
                                labels=labels,
                                parents=parents,
                                values=values,
                                branchvalues="remainder",
                                marker=dict(colors=colors),
                                hovertemplate='<b>%{customdata[0]}</b><br>ì ìˆ˜: %{value:.2f}', # customdataëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ valueë§Œ í‘œì‹œ
                                maxdepth=2,
                                insidetextorientation='radial'
                            ))
                
                            fig_sunburst.update_layout(
                                margin=dict(t=20, l=10, r=10, b=10),
                                height=400,
                                title_text=f"{place['name']} ì¥ì†Œì„± ì¢…í•© í‰ê°€",
                                font=dict(size=12, family="NotoSansKR, sans-serif")
                            )
                
                            st.plotly_chart(fig_sunburst, use_container_width=True, key=f"sunburst_{i}_{place.get('place_id','')}")
                
                        except Exception as e:
                            # Sunburst ì‹¤íŒ¨ ì‹œ Treemap ì‹œë„
                            st.error(f"Sunburst ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                            pass

                        # Radar Chart ìƒì„± í•¨ìˆ˜ ì •ì˜
                        def make_radar_chart(scores_dict, title="ì¥ì†Œì„± ìš”ì¸ íŠ¹ì„± ë¶„í¬"):
                            # ìƒ‰ìƒ ë§¤í•‘ (ëŒ€ë¶„ë¥˜ ê¸°ì¤€)
                            fill_color_map = {
                                "ë¬¼ë¦¬ì  íŠ¹ì„±": "rgba(173, 216, 230, 0.5)",  # ì—°í•œ íŒŒë€ìƒ‰
                                "í™œë™ì  íŠ¹ì„±": "rgba(152, 251, 152, 0.5)",  # ì—°í•œ ì´ˆë¡ìƒ‰
                                "ì˜ë¯¸ì  íŠ¹ì„±": "rgba(255, 182, 193, 0.5)"   # ì—°í•œ ë¶„í™ìƒ‰
                            }
                
                            # ì „ì²´ ìš”ì¸ì„ ìˆœì„œëŒ€ë¡œ ë°°ì¹˜ + ìƒ‰ìƒ ë§¤í•‘
                            categories = []
                            values = []
                            colors = []
                
                            for main_cat, subcats in scores_dict.items():
                                for subcat, val in subcats.items():
                                    categories.append(subcat)
                                    values.append(val if val is not None else 0.5)
                                    colors.append(fill_color_map.get(main_cat, "rgba(200,200,200,0.5)"))
                
                            fig = go.Figure()
                
                            # Barpolarë¡œ ê° ì¶•ë³„ ìƒ‰ìƒ êµ¬ë¶„
                            fig.add_trace(go.Barpolar(
                                r=values,
                                theta=categories,
                                marker=dict(
                                    color=colors,
                                    line=dict(color="rgba(80,80,80,0.3)", width=1)
                                ),
                                hovertemplate='<b>%{theta}</b><br>ì ìˆ˜: %{r:.2f}<extra></extra>',
                                name="ìš”ì¸ë³„ ì ìˆ˜"
                            ))
                
                            # ìœ¤ê³½ì„ ì„ ìœ„í•œ Scatterpolar ì¶”ê°€
                            categories_closed = categories + categories[:1]
                            values_closed = values + values[:1]
                
                            fig.add_trace(go.Scatterpolar(
                                r=values_closed,
                                theta=categories_closed,
                                mode='lines',
                                line=dict(color="rgba(60, 60, 60, 0.8)", width=2.5),
                                showlegend=False,
                                hoverinfo='skip'
                            ))
                
                            fig.update_layout(
                                polar=dict(
                                    radialaxis=dict(
                                        visible=True, 
                                        range=[0, 1], 
                                        tickvals=[0.2, 0.4, 0.6, 0.8, 1.0],
                                        showline=True,
                                        gridcolor="rgba(200,200,200,0.5)"
                                    ),
                                    angularaxis=dict(rotation=90, direction="clockwise")
                                ),
                                showlegend=False,
                                height=580,
                                margin=dict(l=140, r=140, t=110, b=110),  # ì—¬ë°± ìµœëŒ€ í™•ëŒ€
                                title=dict(text=title, x=0.5, font=dict(size=14, family="NotoSansKR"))
                            )
                
                            return fig
                
                        st.markdown(f"**ğŸ“Š ì¥ì†Œì„± ìš”ì¸ íŠ¹ì„± ë¶„í¬ë„**")
                        # Radar Chart ì¶œë ¥
                        fig_radar = make_radar_chart(scores, title=f"{place['name']} ì¥ì†Œì„± íŠ¹ì„± ë¶„í¬")
                    st.plotly_chart(fig_radar, use_container_width=True, key=f"radar_{i}_{place.get('place_id','')}")

            
                # ========== ì˜¤ë¥¸ìª½ ì—´: LLM ë³´ì • + í•´ì„¤ ==========
                with col_right:
                    # LLM ë³´ì • ë‚´ì—­ í‘œì‹œ
                    corrections = place.get('corrections', [])
                    if corrections:
                        st.markdown("**âš™ï¸ LLM ì ìˆ˜ ë³´ì •**")
                        st.caption("GPT-4o ê²€ì¦ ê²°ê³¼")
                
                        correction_df = pd.DataFrame(corrections)
                        correction_df = correction_df.rename(columns={
                            "factor": "ìš”ì¸",
                            "original": "ì›ì ìˆ˜",
                            "adjusted": "ë³´ì •",
                            "delta": "Î”",
                            "reason": "ê·¼ê±°"
                        })
                        st.dataframe(
                            correction_df,
                            use_container_width=True,
                            hide_index=True,
                            column_config={
                                "ìš”ì¸": st.column_config.TextColumn(width="small"),
                                "ì›ì ìˆ˜": st.column_config.NumberColumn(format="%.2f", width="small"),
                                "ë³´ì •": st.column_config.NumberColumn(format="%.2f", width="small"),
                                "Î”": st.column_config.NumberColumn(format="%+.2f", width="small"),
                                "ê·¼ê±°": st.column_config.TextColumn(width="medium"),
                            }
                        )
                    else:
                        st.markdown("**âš™ï¸ LLM ì ìˆ˜ ë³´ì •**")
                        st.caption("ë³´ì • í•„ìš” ì—†ìŒ")
                
                    # ì ìˆ˜ í•´ì„¤ (ë³´ì • í›„ ìµœì¢… ì ìˆ˜ ê¸°ì¤€)
                    if place.get('explanation'):
                        st.markdown("**ğŸ” ìµœì¢… ì ìˆ˜ í•´ì„¤**")
                        st.markdown(place.get('explanation'))
                
                    # ì›Œë“œ í´ë¼ìš°ë“œ ì‹œê°í™” (ì˜¤ë¥¸ìª½ ì—´ í•˜ë‹¨, ì¢Œìš° ë°°ì¹˜)
                    if place.get('positive_keywords') or place.get('negative_keywords'):
                        st.markdown("**ğŸ“ í‚¤ì›Œë“œ ë¶„ì„**")
                
                        wc_col1, wc_col2 = st.columns(2)
                
                        # ê¸ì • ì›Œë“œ í´ë¼ìš°ë“œ
                        if place.get('positive_keywords'):
                            with wc_col1:
                                st.caption("âœ… ê¸ì •")
                                text = " ".join(place['positive_keywords'])
                                if text:
                                    img = generate_wordcloud(text, font_path, colormap="Greens")
                                    if img is not None:
                                        st.image(img, use_container_width=True)
                
                        # ë¶€ì • ì›Œë“œ í´ë¼ìš°ë“œ
                        if place.get('negative_keywords'):
                            with wc_col2:
                                st.caption("âŒ ë¶€ì •")
                                text = " ".join(place['negative_keywords'])
                                if text:
                                    img = generate_wordcloud(text, font_path, colormap="Reds")
                                    if img is not None:
                                        st.image(img, use_container_width=True)
                
                # ì§€ë„ ë° ë¡œë“œë·° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                if place.get('geometry') and place['geometry'].get('location'):
                    lat, lng = place['geometry']['location']['lat'], place['geometry']['location']['lng']
                
                    map_key = f"map_{i}_{place['place_id']}"
                    streetview_key = f"street_{i}_{place['place_id']}"
                
                    if map_key not in st.session_state:
                        st.session_state[map_key] = False
                    if streetview_key not in st.session_state:
                        st.session_state[streetview_key] = False
                
                    col1, col2 = st.columns(2)
                
                    # ë²„íŠ¼ í´ë¦­ ì‹œ ìƒíƒœ í† ê¸€ í›„ ì¬ì‹¤í–‰í•˜ì—¬ ì§€ë„ í‘œì‹œ
                    if col1.button("ğŸ—ºï¸ ì§€ë„ ë³´ê¸°", key=f"btn_{map_key}"):
                        st.session_state[map_key] = not st.session_state[map_key]
                        st.rerun()
                
                    if col2.button("ğŸš— ë¡œë“œë·° ë³´ê¸°", key=f"btn_{streetview_key}"):
                        st.session_state[streetview_key] = not st.session_state[streetview_key]
                        st.rerun()
                
                    if st.session_state[map_key] or st.session_state[streetview_key]:
                        st.markdown("**ğŸ“ ìœ„ì¹˜ ì •ë³´**")
                
                        map_col1, map_col2 = st.columns(2)
                
                        if st.session_state[map_key]:
                            with map_col1:
                                st.markdown("**ğŸ—ºï¸ ì§€ë„**")
                                # Google Maps Embed API (ì „ì²´ í­ ì‚¬ìš©)
                                map_url = f"https://www.google.com/maps/embed/v1/place?key={st.session_state.gmaps_key}&q={lat},{lng}"
                                st.markdown(
                                    f'<iframe src="{map_url}" width="100%" height="450" style="border:0;" allowfullscreen="" loading="lazy"></iframe>',
                                    unsafe_allow_html=True
                                )
                
                        if st.session_state[streetview_key]:
                            with map_col2:
                                st.markdown("**ğŸš— ë¡œë“œë·°**")
                                # Google Maps Street View Embed API (ì „ì²´ í­ ì‚¬ìš©)
                                streetview_url = f"https://www.google.com/maps/embed/v1/streetview?key={st.session_state.gmaps_key}&location={lat},{lng}"
                                st.markdown(
                                    f'<iframe src="{streetview_url}" width="100%" height="450" style="border:0;" allowfullscreen="" loading="lazy"></iframe>',
                                    unsafe_allow_html=True
                                )
                else:
                    st.info("ğŸ“ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ì–´ ì§€ë„ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ========================================
# íƒ­ 2: ì„œìš¸ ì „ì—­ ì‹¤í—˜ (ë…¼ë¬¸ìš© ë°ì´í„° ìˆ˜ì§‘ ë° ê²€ì¦)
# ========================================
with tab2:
    st.markdown("### ğŸ—ºï¸ ì„œìš¸ ì¹´í˜ ì¥ì†Œì„± ì‹¤í—˜ ë° ê²€ì¦")
    st.markdown("""
    ì´ íƒ­ì—ì„œëŠ” ì„œìš¸ ì „ì—­ì˜ ì¹´í˜ë¥¼ ëŒ€ìƒìœ¼ë¡œ ëŒ€ê·œëª¨ ì¥ì†Œì„± í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³ ,
    ì ‘ê·¼ì„± ì ìˆ˜ì™€ ì‹¤ì œ ëŒ€ì¤‘êµí†µ ë„ë³´ ì‹œê°„ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤.
    """)
    
    # ë°ì´í„° ìˆ˜ì§‘ ì˜µì…˜
    st.subheader("1ï¸âƒ£ ë°ì´í„° ìˆ˜ì§‘")
    
    col_config1, col_config2 = st.columns(2)
    with col_config1:
        sample_per_district = st.slider(
            "êµ¬ë‹¹ ìµœëŒ€ ì¹´í˜ ìˆ˜",
            min_value=10,
            max_value=50,
            value=20,
            help="ê° êµ¬ì—ì„œ ìˆ˜ì§‘í•  ìµœëŒ€ ì¹´í˜ ê°œìˆ˜"
        )
    
    with col_config2:
        score_sample_size = st.slider(
            "ì ìˆ˜ ê³„ì‚° ìƒ˜í”Œ ìˆ˜",
            min_value=10,
            max_value=300,
            value=30,
            help="ì¥ì†Œì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•  ì¹´í˜ ìƒ˜í”Œ ìˆ˜ (API quota ê³ ë ¤)"
        )
        st.caption(f"â±ï¸ ì˜ˆìƒ ì‹œê°„: ì•½ {score_sample_size * 30 / 60:.0f}ë¶„ (ì¹´í˜ë‹¹ ~30ì´ˆ)")
        if score_sample_size > 50:
            st.warning("âš ï¸ 50ê°œ ì´ìƒì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤ (25ë¶„+)")
    
    if st.button("ğŸ”„ ì„œìš¸ ì „ì—­ ì¹´í˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", key="btn_collect_cafes"):
        with st.spinner(f"ì„œìš¸ 25ê°œ êµ¬ì—ì„œ ì¹´í˜ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (êµ¬ë‹¹ ìµœëŒ€ {sample_per_district}ê°œ)"):
            # ì¹´í˜ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            cafes_df = collect_all_cafes_seoul(gmaps, max_per_district=sample_per_district)
            
            if not cafes_df.empty:
                st.session_state['cafes_df'] = cafes_df
                st.success(f"âœ… ì´ {len(cafes_df)}ê°œ ì¹´í˜ ìˆ˜ì§‘ ì™„ë£Œ!")
                
                # ê°„ë‹¨í•œ í†µê³„
                district_counts = cafes_df['district'].value_counts()
                st.dataframe(district_counts.head(10), use_container_width=True)
            else:
                st.error("ì¹´í˜ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
    
    # ì¥ì†Œì„± ì ìˆ˜ ê³„ì‚°
    if 'cafes_df' in st.session_state and not st.session_state['cafes_df'].empty:
        st.markdown("---")
        st.subheader("2ï¸âƒ£ ì¥ì†Œì„± ì ìˆ˜ ê³„ì‚°")
        
        if st.button("ğŸ“Š ì¥ì†Œì„± ì ìˆ˜ ì¼ê´„ ê³„ì‚°", key="btn_calc_scores"):
            cafes_df = st.session_state['cafes_df']
            
            # ì˜ˆìƒ ì‹œê°„ í‘œì‹œ
            estimated_time = score_sample_size * 30 / 60  # ì¹´í˜ë‹¹ 30ì´ˆ ê°€ì •
            st.info(f"â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {estimated_time:.1f}ë¶„ (ì¹´í˜ë‹¹ ~30ì´ˆ)")
            
            # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ë° ìƒíƒœ í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()
            time_text = st.empty()
            
            import time
            start_time = time.time()
            
            def update_progress(current, total, cafe_name):
                progress = current / total
                progress_bar.progress(progress)
                
                elapsed = time.time() - start_time
                avg_time = elapsed / current if current > 0 else 30
                remaining = (total - current) * avg_time
                
                status_text.text(f"ì§„í–‰ ì¤‘: {current}/{total} - {cafe_name}")
                time_text.text(f"â±ï¸ ê²½ê³¼: {elapsed/60:.1f}ë¶„ | ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„ | í‰ê· : {avg_time:.1f}ì´ˆ/ì¹´í˜")
            
            scored_df = calculate_placeness_batch(
                cafes_df, 
                sample_size=score_sample_size,
                progress_callback=update_progress
            )
            
            # ì •ë¦¬
            progress_bar.empty()
            status_text.empty()
            time_text.empty()
            
            if not scored_df.empty:
                st.session_state['scored_df'] = scored_df
                total_time = time.time() - start_time
                st.success(f"âœ… {len(scored_df)}ê°œ ì¹´í˜ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„)")
                
                # ê¸°ë³¸ í†µê³„
                st.markdown("**ì ìˆ˜ ë¶„í¬ í†µê³„**")
                stats_df = scored_df[['overall_score', 'accessibility_score']].describe()
                st.dataframe(stats_df, use_container_width=True)
            else:
                st.error("ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨")
    
    # ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„± ê³„ì‚°
    if 'scored_df' in st.session_state and not st.session_state['scored_df'].empty:
        st.markdown("---")
        st.subheader("3ï¸âƒ£ ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„± ë°ì´í„° ìˆ˜ì§‘")
        
        st.info("""
        ğŸ’¡ **ì´ ë‹¨ê³„ëŠ” ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤**
        - ì§€ë„ ë§ˆì»¤ì— **ìµœê·¼ì ‘ ì—­/ì •ë¥˜ì¥, ë„ë³´ ì‹œê°„** ì •ë³´ ì¶”ê°€
        - H1 ê°€ì„¤ ê²€ì¦ (ì ‘ê·¼ì„± ì ìˆ˜ vs ì‹¤ì œ ë„ë³´ ì‹œê°„ ìƒê´€ê´€ê³„)ì— í•„ìš”
        """)
        
        transit_sample = st.number_input(
            "ì ‘ê·¼ì„± ê³„ì‚° ìƒ˜í”Œ ìˆ˜ (Distance Matrix API quota ê³ ë ¤)",
            min_value=10,
            max_value=100,
            value=30,
            help="ìµœê·¼ì ‘ ì—­/ì •ë¥˜ì¥ê¹Œì§€ì˜ ë„ë³´ ì‹œê°„ì„ ê³„ì‚°í•  ì¹´í˜ ìˆ˜"
        )
        st.caption(f"â±ï¸ ì˜ˆìƒ ì‹œê°„: ì•½ {transit_sample * 0.5:.0f}ë¶„ (ì¹´í˜ë‹¹ ~30ì´ˆ)")
        
        if st.button("ğŸš‡ ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„± ê³„ì‚°", key="btn_calc_transit"):
            scored_df = st.session_state['scored_df']
            sample_df = scored_df.sample(n=min(transit_sample, len(scored_df)), random_state=42)
            
            results = []
            error_log = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            error_text = st.empty()
            
            for counter, (idx, row) in enumerate(sample_df.iterrows(), start=1):
                status_text.text(f"ê³„ì‚° ì¤‘: {row['name']} ({counter}/{len(sample_df)})")
                
                try:
                    walk_time, nearest_name, transit_type, walk_distance, straight_distance = calculate_transit_accessibility(
                        row['lat'], row['lng']
                    )
                    results.append({
                        'place_id': row['place_id'],
                        'name': row['name'],
                        'lat': row['lat'],
                        'lng': row['lng'],
                        'district': row['district'],
                        'overall_score': row['overall_score'],
                        'accessibility_score': row['accessibility_score'],
                        'walk_time_minutes': walk_time,
                        'nearest_station': nearest_name,
                        'transit_type': transit_type,
                        'walk_distance_m': walk_distance,
                        'straight_distance_m': straight_distance,
                    })
                    
                    if walk_time is None:
                        error_log.append(f"{row['name']}: ì£¼ë³€ì— ì—­/ì •ë¥˜ì¥ ì—†ìŒ")
                    
                except Exception as e:
                    error_log.append(f"{row['name']}: {str(e)}")
                    results.append({
                        'place_id': row['place_id'],
                        'name': row['name'],
                        'lat': row['lat'],
                        'lng': row['lng'],
                        'district': row['district'],
                        'overall_score': row['overall_score'],
                        'accessibility_score': row['accessibility_score'],
                        'walk_time_minutes': None,
                        'nearest_station': 'ì˜¤ë¥˜',
                        'transit_type': 'ì˜¤ë¥˜',
                        'walk_distance_m': None,
                        'straight_distance_m': None,
                    })
                
                progress_bar.progress(counter / len(sample_df))
                
                # ì—ëŸ¬ ë¡œê·¸ í‘œì‹œ
                if error_log:
                    error_text.text(f"âš ï¸ ì˜¤ë¥˜: {len(error_log)}ê°œ | ë§ˆì§€ë§‰: {error_log[-1]}")
                
                time.sleep(0.3)  # API rate limit
            
            transit_df = pd.DataFrame(results)
            # None ê°’ í•„í„°ë§ (ì ‘ê·¼ì„± ê³„ì‚° ì‹¤íŒ¨í•œ ê²½ìš°)
            valid_transit_df = transit_df[transit_df['walk_time_minutes'].notna()].reset_index(drop=True)
            
            if not valid_transit_df.empty:
                st.session_state['transit_df'] = valid_transit_df
                st.success(f"âœ… {len(valid_transit_df)}ê°œ ì¹´í˜ ì ‘ê·¼ì„± ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
                
                if error_log:
                    st.warning(f"âš ï¸ {len(error_log)}ê°œ ì¹´í˜ëŠ” ì ‘ê·¼ì„± ê³„ì‚° ì‹¤íŒ¨ (600m ë‚´ ì—­/ì •ë¥˜ì¥ ì—†ìŒ)")
                    with st.expander("ì˜¤ë¥˜ ìƒì„¸ ë³´ê¸°"):
                        for err in error_log:
                            st.text(err)
                
                st.dataframe(valid_transit_df.head(10), use_container_width=True)
            else:
                st.error("âŒ ì ‘ê·¼ì„± ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: ëª¨ë“  ì¹´í˜ì—ì„œ 600m ë‚´ ì—­/ì •ë¥˜ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                st.info("""
                **ê°€ëŠ¥í•œ ì›ì¸:**
                1. Google Maps APIì˜ Places Nearby APIê°€ ë¹„í™œì„±í™”ë¨
                2. API í‚¤ì˜ quota ì´ˆê³¼
                3. ì„ íƒëœ ì¹´í˜ë“¤ì´ ëŒ€ì¤‘êµí†µì—ì„œ ë„ˆë¬´ ë©€ë¦¬ ìœ„ì¹˜
                
                **í•´ê²° ë°©ë²•:**
                - Google Cloud Consoleì—ì„œ Places API, Distance Matrix API í™œì„±í™” í™•ì¸
                - ë‹¤ë¥¸ êµ¬ì—­ì˜ ì¹´í˜ë¡œ ì¬ì‹œë„
                """)
                
                if error_log:
                    with st.expander("ì˜¤ë¥˜ ìƒì„¸ ë³´ê¸°"):
                        for err in error_log:
                            st.text(err)
            
            status_text.empty()
            progress_bar.empty()
            error_text.empty()
    
    # ì‹œê°í™” ë° ë¶„ì„
    if 'scored_df' in st.session_state and not st.session_state['scored_df'].empty:
        st.markdown("---")
        st.subheader("4ï¸âƒ£ ì‹œê°í™” ë° ë¶„ì„")
        
        scored_df = st.session_state['scored_df']
        
        # E1: êµ¬ ë‹¨ìœ„ Choropleth
        st.markdown("**E1: êµ¬ ë‹¨ìœ„ ì¥ì†Œì„± ê°•ë„**")
        district_scores = scored_df.groupby('district')['overall_score'].mean().sort_values(ascending=False)
        
        # Choropleth ì§€ë„
        st.markdown("**í–‰ì •êµ¬ë³„ ì¥ì†Œì„± íˆíŠ¸ë§µ**")
        
        # GeoJSON URL (ì„œìš¸ì‹œ êµ¬ ê²½ê³„)
        seoul_geo_url = "https://raw.githubusercontent.com/southkorea/seoul-maps/master/kostat/2013/json/seoul_municipalities_geo_simple.json"
        
        try:
            import requests
            geo_response = requests.get(seoul_geo_url)
            seoul_geo = geo_response.json()
            
            # Folium Choropleth ì§€ë„ ìƒì„±
            m_choropleth = folium.Map(location=SEOUL_CENTER, zoom_start=11, tiles='OpenStreetMap')
            
            # district_scoresë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            choropleth_data = district_scores.reset_index()
            choropleth_data.columns = ['district', 'score']
            
            # Choropleth ë ˆì´ì–´ ì¶”ê°€
            folium.Choropleth(
                geo_data=seoul_geo,
                name='choropleth',
                data=choropleth_data,
                columns=['district', 'score'],
                key_on='feature.properties.name',
                fill_color='YlOrRd',
                fill_opacity=0.7,
                line_opacity=0.2,
                legend_name='í‰ê·  ì¥ì†Œì„± ì ìˆ˜',
                highlight=True
            ).add_to(m_choropleth)
            
            # êµ¬ë³„ í‰ê·  ì ìˆ˜ í‘œì‹œ (íˆ´íŒ)
            style_function = lambda x: {'fillColor': '#ffffff', 'color':'#000000', 'fillOpacity': 0.1, 'weight': 0.1}
            highlight_function = lambda x: {'fillColor': '#000000', 'color':'#000000', 'fillOpacity': 0.50, 'weight': 0.1}
            
            folium.GeoJson(
                seoul_geo,
                style_function=style_function,
                control=False,
                highlight_function=highlight_function,
                tooltip=folium.features.GeoJsonTooltip(
                    fields=['name'],
                    aliases=['êµ¬:'],
                    style=("background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;")
                )
            ).add_to(m_choropleth)
            
            folium.LayerControl().add_to(m_choropleth)
            st_folium(m_choropleth, width=None, height=500)
            
        except Exception as e:
            st.error(f"Choropleth ì§€ë„ ìƒì„± ì‹¤íŒ¨: {e}")
            # ë§‰ëŒ€ ì°¨íŠ¸ë¡œ ëŒ€ì²´
            fig_district = go.Figure(go.Bar(
                x=district_scores.values,
                y=district_scores.index,
                orientation='h',
                marker=dict(color=district_scores.values, colorscale='Viridis', showscale=True),
                text=[f"{v:.2f}" for v in district_scores.values],
                textposition='auto'
            ))
            fig_district.update_layout(
                title="ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ í‰ê·  ì¥ì†Œì„± ì ìˆ˜",
                xaxis_title="í‰ê·  ì¥ì†Œì„± ì ìˆ˜",
                yaxis_title="ìì¹˜êµ¬",
                height=600,
                showlegend=False
            )
            st.plotly_chart(fig_district, use_container_width=True)
        
        # E2: í¬ì¸íŠ¸ íˆíŠ¸ë§µ (Folium)
        st.markdown("**E2: ì¹´í˜ ìœ„ì¹˜ ë° ì¥ì†Œì„± íˆíŠ¸ë§µ**")
        
        # transit_dfê°€ ìˆìœ¼ë©´ ëŒ€ì¤‘êµí†µ ì •ë³´ í¬í•¨, ì—†ìœ¼ë©´ ê¸°ë³¸ ì •ë³´ë§Œ
        use_transit_data = 'transit_df' in st.session_state and not st.session_state['transit_df'].empty
        
        if use_transit_data:
            st.info("âœ… ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„± ì •ë³´ í¬í•¨")
            display_df = st.session_state['transit_df']
        else:
            st.info("â„¹ï¸ ê¸°ë³¸ ì •ë³´ë§Œ í‘œì‹œ (ëŒ€ì¤‘êµí†µ ì •ë³´ëŠ” 3ë‹¨ê³„ì—ì„œ ê³„ì‚° ê°€ëŠ¥)")
            display_df = scored_df
        
        # Folium ì§€ë„ ìƒì„±
        m = folium.Map(location=SEOUL_CENTER, zoom_start=11, tiles='OpenStreetMap')
        
        # íˆíŠ¸ë§µ ë°ì´í„° ì¤€ë¹„
        heat_data = [[row['lat'], row['lng'], row['overall_score']] 
                     for _, row in display_df.iterrows() if row['overall_score'] > 0]
        
        HeatMap(heat_data, radius=15, blur=25, max_zoom=13).add_to(m)
        
        # ë§ˆì»¤ í´ëŸ¬ìŠ¤í„°
        marker_cluster = MarkerCluster().add_to(m)
        
        for _, row in display_df.head(100).iterrows():
            has_transit_info = (use_transit_data and 'walk_time_minutes' in row and 
                               row.get('walk_time_minutes') is not None and pd.notna(row.get('walk_time_minutes')))
            
            if has_transit_info:
                popup_html = f"""
                <div style="font-family: NotoSansKR, sans-serif; min-width: 200px;">
                    <h4 style="margin: 0 0 10px 0; color: #1f77b4;">{row['name']}</h4>
                    <hr style="margin: 5px 0;">
                    <b>ğŸ“Š ì¥ì†Œì„± ì ìˆ˜</b>
                    <ul style="margin: 5px 0; padding-left: 20px;">
                        <li>ì „ì²´: <b>{row['overall_score']:.2f}</b></li>
                        <li>ì ‘ê·¼ì„±: <b>{row['accessibility_score']:.2f}</b></li>
                    </ul>
                    <b>ğŸš‡ ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„±</b>
                    <ul style="margin: 5px 0; padding-left: 20px;">
                        <li>ìµœê·¼ì ‘: <b>{row['nearest_station']}</b></li>
                        <li>ë„ë³´: <b style="color: {'green' if row['walk_time_minutes'] <= 5 else 'orange' if row['walk_time_minutes'] <= 10 else 'red'};">{row['walk_time_minutes']:.1f}ë¶„</b></li>
                        <li>ìœ í˜•: {row['transit_type']}</li>
                    </ul>
                    <b>ğŸ“ ìœ„ì¹˜</b><br>
                    <span style="color: #666;">{row['district']}</span>
                </div>
                """
                marker_color = 'green' if row['walk_time_minutes'] <= 5 else 'blue' if row['walk_time_minutes'] <= 10 else 'orange'
            else:
                popup_html = f"""
                <div style="font-family: NotoSansKR, sans-serif; min-width: 180px;">
                    <h4 style="margin: 0 0 10px 0; color: #1f77b4;">{row['name']}</h4>
                    <hr style="margin: 5px 0;">
                    <b>ğŸ“Š ì ìˆ˜</b>
                    <ul style="margin: 5px 0; padding-left: 20px;">
                        <li>ì¥ì†Œì„±: <b>{row['overall_score']:.2f}</b></li>
                        <li>ì ‘ê·¼ì„±: <b>{row['accessibility_score']:.2f}</b></li>
                    </ul>
                    <b>ğŸ“ êµ¬</b><br>
                    <span style="color: #666;">{row['district']}</span>
                    <hr style="margin: 5px 0;">
                    <small style="color: #999;">ğŸ’¡ ëŒ€ì¤‘êµí†µ ì •ë³´ëŠ”<br>3ë‹¨ê³„ì—ì„œ ì¶”ê°€ ê°€ëŠ¥</small>
                </div>
                """
                marker_color = 'blue' if row['overall_score'] > 0.7 else 'gray'
            
            folium.Marker(
                location=[row['lat'], row['lng']],
                popup=folium.Popup(popup_html, max_width=300),
                tooltip=row['name'],
                icon=folium.Icon(color=marker_color, icon='coffee', prefix='fa')
            ).add_to(marker_cluster)
        
        st_folium(m, width=None, height=600)
    
    # H1 ê²€ì¦: ì ‘ê·¼ì„± ì ìˆ˜ vs ë„ë³´ ì‹œê°„ ìƒê´€ê´€ê³„
    if 'transit_df' in st.session_state and not st.session_state['transit_df'].empty:
        st.markdown("---")
        st.subheader("5ï¸âƒ£ H1 ê²€ì¦: ì ‘ê·¼ì„± ì ìˆ˜ vs ëŒ€ì¤‘êµí†µ ë„ë³´ ì‹œê°„")
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        transit_df = st.session_state['transit_df'].dropna(subset=['walk_time_minutes', 'accessibility_score'])
        
        st.markdown("""
        **ê°€ì„¤ H1**: "ìµœê·¼ì ‘ ëŒ€ì¤‘êµí†µ ë„ë³´ ì‹œê°„ì´ ì§§ì„ìˆ˜ë¡ ì ‘ê·¼ì„± ì ìˆ˜ê°€ ë†’ë‹¤."
        
        ìƒê´€ê³„ìˆ˜ê°€ ìŒìˆ˜(-) ê°’ì„ ê°€ì§€ë©´ ê°€ì„¤ì´ ì§€ì§€ë©ë‹ˆë‹¤.
        (ë„ë³´ ì‹œê°„ â†“ â†’ ì ‘ê·¼ì„± ì ìˆ˜ â†‘)
        """)
        
        # â‘  ì •ê·œì„± ê²€ì •
        shapiro_walk = stats.shapiro(transit_df['walk_time_minutes'])
        shapiro_acc = stats.shapiro(transit_df['accessibility_score'])
        is_normal = shapiro_walk.pvalue > 0.05 and shapiro_acc.pvalue > 0.05
    
        # â‘¡ ìƒê´€ê³„ìˆ˜ ê³„ì‚° (ì •ê·œë¶„í¬ë©´ Pearson, ì•„ë‹ˆë©´ Spearman)
        if is_normal:
            corr_type = "Pearson"
            correlation, p_value = stats.pearsonr(
                transit_df['walk_time_minutes'], transit_df['accessibility_score']
            )
        else:
            corr_type = "Spearman"
            correlation, p_value = stats.spearmanr(
                transit_df['walk_time_minutes'], transit_df['accessibility_score']
            )
    
        col_stat1, col_stat2 = st.columns(2)
        with col_stat1:
            st.metric(f"{corr_type} ìƒê´€ê³„ìˆ˜ (r)", f"{correlation:.3f}")
        with col_stat2:
            st.metric("p-value", f"{p_value:.4f}")
        
        if p_value < 0.05:
            if correlation < 0:
                st.success(f"âœ… ê°€ì„¤ ì§€ì§€: ì ‘ê·¼ì„± ì ìˆ˜ì™€ ë„ë³´ ì‹œê°„ ê°„ ìœ ì˜ë¯¸í•œ ìŒì˜ ìƒê´€ê´€ê³„ (r={correlation:.3f}, p<0.05)")
            else:
                st.warning(f"âš ï¸ ê°€ì„¤ ê¸°ê°: ì–‘ì˜ ìƒê´€ê´€ê³„ ë°œê²¬ (r={correlation:.3f}, p<0.05)")
        else:
            st.info(f"ğŸ“Š ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ì—†ìŒ (p={p_value:.4f} > 0.05)")
        
        # ì‚°ì ë„
        fig_scatter = go.Figure()
        
        fig_scatter.add_trace(go.Scatter(
            x=transit_df['walk_time_minutes'],
            y=transit_df['accessibility_score'],
            mode='markers',
            marker=dict(
                size=10,
                color=transit_df['accessibility_score'],
                colorscale='RdYlGn',
                showscale=True,
                colorbar=dict(title="ì ‘ê·¼ì„±<br>ì ìˆ˜"),
                line=dict(width=1, color='DarkSlateGrey')
            ),
            text=[f"{row['name']}<br>{row['nearest_station']}" for _, row in transit_df.iterrows()],
            hovertemplate='<b>%{text}</b><br>ë„ë³´: %{x:.1f}ë¶„<br>ì ‘ê·¼ì„±: %{y:.2f}<extra></extra>'
        ))
        
        # íšŒê·€ì„  ì¶”ê°€
        from scipy.stats import linregress
        slope, intercept, r_value, p_value_reg, std_err = linregress(
            transit_df['walk_time_minutes'],
            transit_df['accessibility_score']
        )
        
        x_range = np.linspace(
            transit_df['walk_time_minutes'].min(),
            transit_df['walk_time_minutes'].max(),
            100
        )
        y_pred = slope * x_range + intercept
        
        fig_scatter.add_trace(go.Scatter(
            x=x_range,
            y=y_pred,
            mode='lines',
            name=f'íšŒê·€ì„  (y={slope:.3f}x+{intercept:.3f})',
            line=dict(color='red', width=2, dash='dash')
        ))
        
        fig_scatter.update_layout(
            title=f"ì ‘ê·¼ì„± ì ìˆ˜ vs ëŒ€ì¤‘êµí†µ ë„ë³´ ì‹œê°„ (r={correlation:.3f}, p={p_value:.4f})",
            xaxis_title="ìµœê·¼ì ‘ ì—­/ì •ë¥˜ì¥ê¹Œì§€ ë„ë³´ ì‹œê°„ (ë¶„)",
            yaxis_title="ì ‘ê·¼ì„± ì ìˆ˜ (ëª¨ë¸ ì˜ˆì¸¡)",
            height=500,
            hovermode='closest',
            showlegend=True
        )
        
        st.plotly_chart(fig_scatter, use_container_width=True)
        
        # ë°ì´í„° í…Œì´ë¸”
        st.markdown("**ìƒì„¸ ë°ì´í„°**")
        display_df_table = transit_df[['name', 'district', 'overall_score', 'accessibility_score', 
                                 'walk_time_minutes', 'nearest_station', 'transit_type']].copy()
        display_df_table.columns = ['ì¹´í˜ëª…', 'êµ¬', 'ì „ì²´ ì¥ì†Œì„±', 'ì ‘ê·¼ì„± ì ìˆ˜', 
                              'ë„ë³´(ë¶„)', 'ìµœê·¼ì ‘ ì—­/ì •ë¥˜ì¥', 'ìœ í˜•']
        st.dataframe(display_df_table, use_container_width=True, height=300)


# ========================================
# íƒ­ 4: Google ë¦¬ë·° CSV ê¸°ë°˜ ë¶„ì„
# ========================================
with tab4:
    st.markdown("### ğŸ“š ë°ì´í„°ì…‹ í™•ì¸")
    dataset_targets = [
        ("ì„œìš¸ì‹œ ì¹´í˜ í‘œë³¸ (ì ‘ê·¼ì„± í¬í•¨)", SAMPLED_CAFE_WITH_TRANSIT_CSV),
        ("Google ë¦¬ë·° í‘œë³¸", GOOGLE_REVIEW_SAMPLE_CSV),
    ]
    for dataset_title, dataset_path in dataset_targets:
        st.markdown(f"#### {dataset_title}")
        if dataset_path.exists():
            try:
                dataset_df = load_csv_generic(dataset_path)
                preview_height = min(520, max(240, 35 * min(len(dataset_df), 50)))
                st.dataframe(
                    dataset_df,
                    use_container_width=True,
                    hide_index=True,
                    height=preview_height,
                )
                st.download_button(
                    f"ğŸ“¥ {dataset_path.name} ë‹¤ìš´ë¡œë“œ",
                    data=dataset_df.to_csv(index=False).encode("utf-8-sig"),
                    file_name=dataset_path.name,
                    mime="text/csv",
                    key=f"download_{dataset_path.stem}",
                )
            except Exception as dataset_err:
                st.error(f"ë°ì´í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {dataset_err}")
        else:
            st.info(f"`{dataset_path.name}` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    st.divider()
    st.markdown("### ğŸ“ Google ë¦¬ë·° ë°ì´í„° ë¶„ì„")
    st.caption("ë¦¬ë·° CSV(í‘œë³¸/ìµœê·¼ ìˆ˜ì§‘)ë¥¼ ë¶ˆëŸ¬ì™€ ê°ì„± ë¶„ì„ê³¼ í‰ì  ìƒê´€ ê´€ê³„ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.")

    if not GOOGLE_REVIEW_SAMPLE_CSV.exists() and not GOOGLE_REVIEW_LIVE_CSV.exists():
        st.warning("Google ë¦¬ë·° CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íƒ­ 3ì—ì„œ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•˜ê±°ë‚˜ CSVë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
    else:
        available_sources = {}
        if GOOGLE_REVIEW_SAMPLE_CSV.exists():
            available_sources[f"í‘œë³¸ (ì „ì²´ 2500ê°œ) - {GOOGLE_REVIEW_SAMPLE_CSV.name}"] = GOOGLE_REVIEW_SAMPLE_CSV
        if GOOGLE_REVIEW_LIVE_CSV.exists():
            available_sources[f"ìµœê·¼ ìˆ˜ì§‘ (íƒ­3) - {GOOGLE_REVIEW_LIVE_CSV.name}"] = GOOGLE_REVIEW_LIVE_CSV

        selected_label = st.selectbox(
            "ë¶„ì„í•  ë¦¬ë·° CSV ì„ íƒ",
            options=list(available_sources.keys()),
            index=0,
        )
        selected_path = available_sources[selected_label]

        try:
            base_review_df = load_google_reviews_csv(selected_path)
        except Exception as e:
            st.error(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")
            base_review_df = None

        if base_review_df is not None:
            if "ë¦¬ë·°" not in base_review_df.columns:
                st.error("CSVì— 'ë¦¬ë·°' ì»¬ëŸ¼ì´ ì—†ì–´ ê°ì„± ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.success(f"âœ… {len(base_review_df):,}ê°œ ë¦¬ë·° ë¡œë“œ ì™„ë£Œ ({selected_path.name})")

                with st.expander("ğŸ” í•„í„°", expanded=False):
                    district_options = (
                        sorted(base_review_df["ì‹œêµ°êµ¬ëª…"].dropna().unique().tolist())
                        if "ì‹œêµ°êµ¬ëª…" in base_review_df.columns
                        else []
                    )
                    selected_districts = st.multiselect(
                        "ì‹œêµ°êµ¬ ì„ íƒ",
                        district_options,
                        placeholder="ì „ì²´ ì‹œêµ°êµ¬",
                        key="tab4_filter_district",
                    )

                    keyword = st.text_input(
                        "ìƒí˜¸ëª…/ë¦¬ë·° ê²€ìƒ‰ì–´",
                        value="",
                        placeholder="ì˜ˆ: í™ëŒ€, ë¶„ìœ„ê¸°, ì¹œì ˆ",
                        key="tab4_filter_keyword",
                    ).strip()
                    keyword_lower = keyword.lower()

                    min_review_per_place = st.slider(
                        "ìµœì†Œ ë¦¬ë·° ìˆ˜ (ì§‘ê³„ ëŒ€ìƒ)",
                        min_value=1,
                        max_value=50,
                        value=3,
                        step=1,
                        key="tab4_min_reviews",
                    )

                filtered_reviews = base_review_df.copy()
                if selected_districts and "ì‹œêµ°êµ¬ëª…" in filtered_reviews.columns:
                    filtered_reviews = filtered_reviews[filtered_reviews["ì‹œêµ°êµ¬ëª…"].isin(selected_districts)]
                if keyword_lower:
                    filtered_reviews = filtered_reviews[
                        filtered_reviews.apply(
                            lambda row: any(
                                keyword_lower in str(row.get(col, "")).lower()
                                for col in ["ìƒí˜¸ëª…", "ë¦¬ë·°", "í–‰ì •ë™ëª…", "ë„ë¡œëª…ì£¼ì†Œ"]
                            ),
                            axis=1,
                        )
                    ]

                st.info(f"í•„í„°ë§ ê²°ê³¼ ë¦¬ë·° ìˆ˜: {len(filtered_reviews):,}ê±´")

                current_config = {
                    "source": selected_path.resolve().as_posix(),
                    "districts": tuple(selected_districts),
                    "keyword": keyword_lower,
                    "min_reviews": min_review_per_place,
                }
                prev_config = st.session_state.get("tab4_analysis_config")
                if prev_config != current_config:
                    st.session_state["tab4_analysis_config"] = current_config
                    st.session_state.pop("tab4_grouped_df", None)
                    st.session_state.pop("tab4_analysis_results_raw", None)
                    st.session_state.pop("tab4_analysis_results_final", None)
                    st.session_state["tab4_analysis_active"] = False

                display_cols = [
                    col
                    for col in ["ìƒí˜¸ëª…", "ì‹œêµ°êµ¬ëª…", "í–‰ì •ë™ëª…", "place_id", "í‰ì ", "ë¦¬ë·°", "ì‘ì„±ì¼"]
                    if col in filtered_reviews.columns
                ]
                table_height = min(600, max(250, 35 * min(len(filtered_reviews), 50)))
                st.dataframe(
                    filtered_reviews[display_cols].head(1000),
                    use_container_width=True,
                    hide_index=True,
                    height=table_height,
                )

                group_cols = [col for col in ["place_id", "ìƒí˜¸ëª…", "ì‹œêµ°êµ¬ëª…", "í–‰ì •ë™ëª…"] if col in filtered_reviews.columns]
                if not group_cols:
                    st.error("ì§‘ê³„ë¥¼ ìœ„í•œ ì‹ë³„ ì»¬ëŸ¼(place_id ë˜ëŠ” ìƒí˜¸ëª…/ì‹œêµ°êµ¬ëª… ë“±)ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.markdown("#### âš™ï¸ ê°ì„± ë¶„ì„ ì‹¤í–‰")
                    analysis_active = st.session_state.get("tab4_analysis_active", False)
                    run_analysis = st.button("ğŸš€ ê°ì„± ë¶„ì„ ì‹¤í–‰", key="tab4_run_analysis")
                    if run_analysis:
                        st.session_state["tab4_analysis_active"] = True
                        analysis_active = True

                    if not analysis_active:
                        st.info("í•„í„°ë¥¼ ì„¤ì •í•œ ë’¤ â€˜ê°ì„± ë¶„ì„ ì‹¤í–‰â€™ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
                        st.stop()

                    grouped_state = st.session_state.get("tab4_grouped_df")
                    if run_analysis or grouped_state is None:
                        aggregated = filtered_reviews.copy()
                        aggregated["ë¦¬ë·°"] = aggregated["ë¦¬ë·°"].fillna("").astype(str)
                        aggregated["ë¦¬ë·°ì •ì œ"] = aggregated["ë¦¬ë·°"].str.strip()
                        aggregated["ë¦¬ë·°ê°ì„±ì ìˆ˜"] = np.nan

                        valid_mask = aggregated["ë¦¬ë·°ì •ì œ"] != ""
                        if not valid_mask.any():
                            st.info("ìœ íš¨í•œ ë¦¬ë·° í…ìŠ¤íŠ¸ê°€ ì—†ì–´ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            st.session_state["tab4_analysis_active"] = False
                            st.stop()

                        texts = aggregated.loc[valid_mask, "ë¦¬ë·°ì •ì œ"].tolist()
                        sentiment_scores = sentiment_model(texts)
                        aggregated.loc[valid_mask, "ë¦¬ë·°ê°ì„±ì ìˆ˜"] = sentiment_scores

                        grouped = (
                            aggregated.groupby(group_cols, dropna=False)
                            .agg(
                                í‰ê· í‰ì =("í‰ì ", "mean"),
                                ë¦¬ë·°ìˆ˜=("ë¦¬ë·°ì •ì œ", lambda s: int((s != "").sum())),
                                í‰ê· ê°ì„±ì ìˆ˜=("ë¦¬ë·°ê°ì„±ì ìˆ˜", "mean"),
                            )
                            .reset_index()
                        )

                        grouped = grouped[grouped["ë¦¬ë·°ìˆ˜"] >= min_review_per_place]

                        if grouped.empty:
                            st.warning("ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì§‘ê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            st.session_state["tab4_analysis_active"] = False
                            st.stop()

                        st.session_state["tab4_grouped_df"] = grouped.copy()
                    else:
                        grouped = grouped_state.copy()

                    st.markdown("#### ğŸ“Š ê°ì„± ë¶„ì„ ê²°ê³¼")
                    base_analysis_cols = [c for c in group_cols if c != "place_id"] + ["í‰ê· í‰ì ", "í‰ê· ê°ì„±ì ìˆ˜", "ë¦¬ë·°ìˆ˜"]
                    base_height = min(400, max(240, 38 * len(grouped)))
                    st.dataframe(
                        grouped[base_analysis_cols],
                        use_container_width=True,
                        hide_index=True,
                        height=base_height,
                    )

                    base_corr_df = grouped.dropna(subset=["í‰ê· í‰ì ", "í‰ê· ê°ì„±ì ìˆ˜"])
                    if len(base_corr_df) >= 2:
                        base_corr, base_p = stats.pearsonr(
                            base_corr_df["í‰ê· í‰ì "].astype(float),
                            base_corr_df["í‰ê· ê°ì„±ì ìˆ˜"].astype(float),
                        )
                        base_p_text = f"{base_p:.4f}" if base_p >= 1e-4 else f"{base_p:.2e}"
                        if base_p < 0.05:
                            if base_corr > 0:
                                base_msg = " â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ê°€ í™•ì¸ë©ë‹ˆë‹¤ (Î±=0.05)."
                            else:
                                base_msg = " â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ìŒì˜ ìƒê´€ê´€ê³„ê°€ í™•ì¸ë©ë‹ˆë‹¤ (Î±=0.05).\n   â€» ê¸°ëŒ€ì™€ ë‹¤ë¥¸ ë°©í–¥ì˜ ìƒê´€ê´€ê³„ì´ë¯€ë¡œ ë°ì´í„° ë¶„í¬ë‚˜ ëª¨ë¸ ê²°ê³¼ë¥¼ ì¶”ê°€ ê²€í† í•˜ì„¸ìš”."
                        else:
                            base_msg = " â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë¡œ ë³´ê¸° ì–´ë µìŠµë‹ˆë‹¤ (Î±=0.05)."
                        st.markdown("#### ğŸ“ˆ ë¦¬ë·° í‰ì  vs ê°ì„± ë¶„ì„ ì ìˆ˜ (ì¥ì†Œì„± ìš”ì¸ ë¶„ì„ ì „)")
                        st.write(f"ìƒê´€ê³„ìˆ˜(í”¼ì–´ìŠ¨ r): **{base_corr:.3f}** (p-value={base_p_text}){base_msg}")
                        base_slope, base_intercept, _, _, _ = stats.linregress(
                            base_corr_df["í‰ê· í‰ì "].astype(float),
                            base_corr_df["í‰ê· ê°ì„±ì ìˆ˜"].astype(float),
                        )
                        st.caption(f"íšŒê·€ì„ : y = {base_slope:.3f}x + {base_intercept:.3f}")
                        base_fig = px.scatter(
                            base_corr_df,
                            x="í‰ê· í‰ì ",
                            y="í‰ê· ê°ì„±ì ìˆ˜",
                            hover_data=[col for col in group_cols if col != "place_id" and col in base_corr_df.columns],
                            trendline="ols",
                            labels={"í‰ê· í‰ì ": "Google í‰ì  í‰ê· ", "í‰ê· ê°ì„±ì ìˆ˜": "ê°ì„± ì ìˆ˜ í‰ê· "},
                            title="í‰ê·  í‰ì  vs ê°ì„± ì ìˆ˜",
                        )
                        st.plotly_chart(base_fig, use_container_width=True, key="tab4_pre_factor_corr")
                    else:
                        st.info("ê¸°ë³¸ ê°ì„± ë¶„ì„ ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

                    st.markdown("---")
                    st.markdown("#### ğŸ§ª ì¥ì†Œì„± ìš”ì¸ë³„ ìƒì„¸ ë¶„ì„")
                    st.caption("ì¥ì†Œì„± ìš”ì¸ë³„ ë¶„ì„ì—ëŠ” ë‹¤ì†Œ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    with open("factors.json", "r", encoding="utf-8") as f:
                        factor_definitions_tab4 = json.load(f)

                    raw_results_state = st.session_state.get("tab4_analysis_results_raw")
                    final_results_state = st.session_state.get("tab4_analysis_results_final")
                    if run_analysis or raw_results_state is None:
                        progress_placeholder = st.empty()
                        status_placeholder = st.empty()
                        with progress_placeholder:
                            with st.spinner("ì¥ì†Œì„± ìš”ì¸ë³„ ë¶„ì„ ìˆ˜í–‰ ì¤‘..."):
                                raw_results = tab4_analysis.analyze_review_groups(
                                    review_df=filtered_reviews,
                                    group_cols=group_cols,
                                    sentiment_model=sentiment_model,
                                    embed_model=embed_model,
                                    category_embeddings=category_embeddings,
                                    score_template=new_score_structure_template,
                                    semantic_split_fn=cached_semantic_split,
                                    llm_client=client,
                                    factor_definitions=factor_definitions_tab4,
                                    llm_delta_limit=0.5,
                                    progress_callback=lambda current, total, context: (
                                        status_placeholder.text(f"ë¶„ì„ ì§„í–‰ ì¤‘: {current}/{total} - {context}")
                                    ),
                                )
                        progress_placeholder.empty()
                        status_placeholder.empty()
                        st.session_state["tab4_analysis_results_raw"] = raw_results.copy()
                    else:
                        raw_results = raw_results_state.copy()

                    if run_analysis or final_results_state is None:
                        analysis_results = raw_results.copy()

                        # ë¬¸ì¥ ë‹¨ìœ„ ê°ì„± í‰ê· ì„ ë³„ë„ ë³´ê´€í•˜ê³ , í…Œì´ë¸”ìš© í‰ê· ê°ì„±ì ìˆ˜ëŠ” ìƒë‹¨ ì§‘ê³„ê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸°
                        if "í‰ê· ê°ì„±ì ìˆ˜" in analysis_results.columns:
                            analysis_results = analysis_results.rename(
                                columns={"í‰ê· ê°ì„±ì ìˆ˜": "í‰ê· ê°ì„±ì ìˆ˜(ë¬¸ì¥ë‹¨ìœ„)"}
                            )

                        merge_keys = [col for col in group_cols if col in grouped.columns and col in analysis_results.columns]
                        if merge_keys:
                            base_sentiments = grouped[merge_keys + ["í‰ê· ê°ì„±ì ìˆ˜"]].copy()
                            analysis_results = analysis_results.merge(base_sentiments, on=merge_keys, how="left")
                        else:
                            analysis_results["í‰ê· ê°ì„±ì ìˆ˜"] = np.nan

                        analysis_results = analysis_results[analysis_results["ë¦¬ë·°ìˆ˜"] >= min_review_per_place]

                        if analysis_results.empty:
                            st.warning("ì¥ì†Œì„± ìš”ì¸ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                            st.stop()
                    else:
                        analysis_results = final_results_state.copy()

                    score_columns = []
                    accessibility_col = None
                    if not analysis_results["scores"].dropna().empty:
                        example_scores = analysis_results["scores"].dropna().iloc[0]
                        score_columns = [
                            f"{main}/{sub}"
                            for main, subdict in example_scores.items()
                            for sub in subdict.keys()
                        ]
                        expanded_scores = analysis_results["scores"].apply(
                            lambda score_dict: {
                                f"{main}/{sub}": subdict.get(sub)
                                for main, subdict in score_dict.items()
                                for sub in subdict.keys()
                            }
                            if isinstance(score_dict, dict)
                            else {}
                        )
                        expanded_df = pd.DataFrame(list(expanded_scores), index=analysis_results.index)
                        new_score_cols = [col for col in expanded_df.columns if col not in analysis_results.columns]
                        if new_score_cols:
                            analysis_results = pd.concat([analysis_results, expanded_df[new_score_cols]], axis=1)
                        if "ë¬¼ë¦¬ì  íŠ¹ì„±/ì ‘ê·¼ì„±" in analysis_results.columns:
                            accessibility_col = "ë¬¼ë¦¬ì  íŠ¹ì„±/ì ‘ê·¼ì„±"

                    st.markdown("#### ğŸ“Š ì¥ì†Œì„± ë¶„ì„ ê²°ê³¼ (ìš”ì¸ë³„ í‰ê· )")
                    analysis_cols = [
                        col
                        for col in (
                            [c for c in group_cols if c != "place_id"]
                            + ["í‰ê· í‰ì ", "í‰ê· ê°ì„±ì ìˆ˜", "í‰ê· ì¥ì†Œì„±ì ìˆ˜", "ë¦¬ë·°ìˆ˜", "ë¦¬ë·°ë¬¸ì¥ìˆ˜"]
                        )
                        if col in analysis_results.columns
                    ]
                    score_display_cols = [col for col in score_columns if col in analysis_results.columns]
                    display_cols = analysis_cols + score_display_cols
                    analysis_height = min(600, max(240, 38 * len(analysis_results)))
                    st.dataframe(
                        analysis_results[display_cols] if display_cols else analysis_results,
                    use_container_width=True,
                    hide_index=True,
                        height=analysis_height,
                    )

                    if "corrections" in analysis_results.columns:
                        total_corrections = analysis_results["corrections"].apply(lambda logs: len(logs) if isinstance(logs, list) else 0).sum()
                        st.caption(f"LLM ë³´ì • ê±´ìˆ˜: {total_corrections}ê±´")
                        with st.expander("LLM ë³´ì • ìƒì„¸", expanded=False):
                            correction_rows = []
                            for _, row in analysis_results.iterrows():
                                logs = row.get("corrections") or []
                                for log in logs:
                                    correction_rows.append(
                                        {
                                            "ìƒí˜¸ëª…": row.get("ìƒí˜¸ëª…"),
                                            "ì‹œêµ°êµ¬ëª…": row.get("ì‹œêµ°êµ¬ëª…"),
                                            "í–‰ì •ë™ëª…": row.get("í–‰ì •ë™ëª…"),
                                            **log,
                                        }
                                    )
                            if correction_rows:
                                correction_df = pd.DataFrame(correction_rows)
                                st.dataframe(correction_df, use_container_width=True, hide_index=True)
                            else:
                                st.write("ë³´ì •ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

                    corr_df = analysis_results.dropna(subset=["í‰ê· í‰ì ", "í‰ê· ì¥ì†Œì„±ì ìˆ˜"])
                    if len(corr_df) >= 2:
                        corr_value, corr_p = stats.pearsonr(
                            corr_df["í‰ê· í‰ì "].astype(float),
                            corr_df["í‰ê· ì¥ì†Œì„±ì ìˆ˜"].astype(float),
                        )
                        st.markdown("#### ğŸ“ˆ ë¦¬ë·° í‰ì  vs ì¥ì†Œì„± ì ìˆ˜ (ìš”ì¸ í‰ê· )")
                        p_text = f"{corr_p:.4f}" if corr_p >= 1e-4 else f"{corr_p:.2e}"
                        if corr_p < 0.05:
                            if corr_value > 0:
                                significance_msg = " â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ê°€ í™•ì¸ë©ë‹ˆë‹¤ (Î±=0.05)."
                            else:
                                significance_msg = " â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ìŒì˜ ìƒê´€ê´€ê³„ê°€ í™•ì¸ë©ë‹ˆë‹¤ (Î±=0.05).\n   â€» ê¸°ëŒ€ì™€ ë‹¤ë¥¸ ë°©í–¥ì˜ ìƒê´€ê´€ê³„ì´ë¯€ë¡œ ë°ì´í„° ë¶„í¬ë‚˜ ëª¨ë¸ ê²°ê³¼ë¥¼ ì¶”ê°€ ê²€í† í•˜ì„¸ìš”."
                        else:
                            significance_msg = " â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë¡œ ë³´ê¸° ì–´ë µìŠµë‹ˆë‹¤ (Î±=0.05)."
                        st.write(f"ìƒê´€ê³„ìˆ˜(í”¼ì–´ìŠ¨ r): **{corr_value:.3f}** (p-value={p_text}){significance_msg}")
                        slope, intercept, _, _, _ = stats.linregress(
                            corr_df["í‰ê· í‰ì "].astype(float),
                            corr_df["í‰ê· ì¥ì†Œì„±ì ìˆ˜"].astype(float),
                        )
                        st.caption(f"íšŒê·€ì„ : y = {slope:.3f}x + {intercept:.3f}")
                        fig_corr = px.scatter(
                            corr_df,
                            x="í‰ê· í‰ì ",
                            y="í‰ê· ì¥ì†Œì„±ì ìˆ˜",
                            hover_data=[col for col in group_cols if col != "place_id"],
                            trendline="ols",
                            labels={"í‰ê· í‰ì ": "Google í‰ì  í‰ê· ", "í‰ê· ì¥ì†Œì„±ì ìˆ˜": "ì¥ì†Œì„± ì ìˆ˜ í‰ê· "},
                            title="í‰ê·  í‰ì  vs ì¥ì†Œì„± ì ìˆ˜ (ìš”ì¸ í‰ê· )",
                        )
                        st.plotly_chart(fig_corr, use_container_width=True, key="tab4_sentiment_corr")
                    else:
                        st.info("ìƒê´€ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

                    # ì¢Œí‘œ/ì ‘ê·¼ì„± ë°ì´í„° ë³´ê°•
                    if (
                        ("lat" not in analysis_results.columns or analysis_results["lat"].isna().all())
                        and "place_id" in analysis_results.columns
                    ):
                        with st.spinner("ì§€ë„ ì¢Œí‘œ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                            coord_cache = {}
                            for pid in analysis_results["place_id"].dropna().unique():
                                try:
                                    detail = gmaps.place(place_id=pid, language="ko")
                                    loc = (
                                        detail.get("result", {})
                                        .get("geometry", {})
                                        .get("location", {})
                                    )
                                    if loc:
                                        coord_cache[pid] = (loc.get("lat"), loc.get("lng"))
                                except Exception:
                                    coord_cache[pid] = (np.nan, np.nan)
                            analysis_results["lat"] = analysis_results["place_id"].map(
                                lambda pid: coord_cache.get(pid, (np.nan, np.nan))[0]
                            )
                            analysis_results["lng"] = analysis_results["place_id"].map(
                                lambda pid: coord_cache.get(pid, (np.nan, np.nan))[1]
                            )

                    accessibility_required_cols = {"walk_time_minutes", "walk_distance_m", "straight_distance_m", "nearest_station", "transit_type"}
                    has_accessibility_cols = accessibility_required_cols.issubset(analysis_results.columns)
                    needs_accessibility = not has_accessibility_cols
                    if has_accessibility_cols:
                        needs_accessibility = analysis_results[
                            list({"walk_time_minutes", "walk_distance_m", "straight_distance_m"} & set(analysis_results.columns))
                        ].isna().any().any()

                    if needs_accessibility and {"lat", "lng"}.issubset(analysis_results.columns):
                        with st.spinner("ì ‘ê·¼ì„±(ë„ë³´ ì‹œê°„) ë°ì´í„°ë¥¼ ê³„ì‚° ì¤‘ì…ë‹ˆë‹¤..."):
                            # ì‚¬ì „ ê³„ì‚°ëœ ì ‘ê·¼ì„± CSV ë¡œë“œ ì‹œë„
                            if "precomputed_transit" not in st.session_state:
                                transit_csv = Path("ì„œìš¸ì‹œ_ìƒê¶Œ_ì¹´í˜ë¹µ_í‘œë³¸_with_transit.csv")
                                if transit_csv.exists():
                                    try:
                                        precomputed_df = pd.read_csv(transit_csv)
                                        required_cols = {"ìƒí˜¸ëª…", "í–‰ì •ë™ëª…", "nearest_station", "transit_type", "walk_time_minutes", "walk_distance_m", "straight_distance_m"}
                                        if required_cols.issubset(precomputed_df.columns):
                                            precomputed_df = precomputed_df.copy()
                                            st.session_state["precomputed_transit"] = precomputed_df
                                        else:
                                            st.warning("ì‚¬ì „ ê³„ì‚°ëœ ì ‘ê·¼ì„± CSVì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ëª¨ë‘ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. API í˜¸ì¶œì„ ì§„í–‰í•©ë‹ˆë‹¤.")
                                    except Exception as transit_load_err:
                                        st.warning(f"ì‚¬ì „ ê³„ì‚°ëœ ì ‘ê·¼ì„± CSV ë¡œë”© ì‹¤íŒ¨: {transit_load_err}")
                                else:
                                    st.info("ì‚¬ì „ ê³„ì‚°ëœ ì ‘ê·¼ì„± CSVê°€ ì—†ì–´ API ê¸°ë°˜ìœ¼ë¡œ ì ‘ê·¼ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.")

                            precomputed_df = st.session_state.get("precomputed_transit")

                            if precomputed_df is not None and not precomputed_df.empty:
                                analysis_results = analysis_results.copy()

                                # í–‰ì •ë™ëª… + ìƒí˜¸ëª…ìœ¼ë¡œ ë§¤ì¹­
                                merge_keys = ["ìƒí˜¸ëª…", "í–‰ì •ë™ëª…"]
                                if all(col in analysis_results.columns for col in merge_keys) and all(col in precomputed_df.columns for col in merge_keys):
                                    merged = analysis_results.merge(
                                        precomputed_df[
                                            [
                                                "ìƒí˜¸ëª…",
                                                "í–‰ì •ë™ëª…",
                                                "nearest_station",
                                                "transit_type",
                                                "walk_time_minutes",
                                                "walk_distance_m",
                                                "straight_distance_m",
                                            ]
                                        ],
                                        how="left",
                                        on=merge_keys,
                                        suffixes=("", "_precomputed"),
                                    )

                                    for col in ["walk_time_minutes", "walk_distance_m", "straight_distance_m", "nearest_station", "transit_type"]:
                                        pre_col = f"{col}_precomputed"
                                        if pre_col in merged.columns:
                                            merged[col] = merged[col].combine_first(merged[pre_col])
                                            merged = merged.drop(columns=[pre_col])

                                    analysis_results = merged
                                else:
                                    st.warning("ë§¤ì¹­ì„ ìœ„í•œ 'ìƒí˜¸ëª…' ë˜ëŠ” 'í–‰ì •ë™ëª…' ì»¬ëŸ¼ì´ ì—†ì–´ ì‚¬ì „ ê³„ì‚°ëœ ì ‘ê·¼ì„± ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                            missing_after_merge = analysis_results[
                                ["walk_time_minutes", "walk_distance_m", "straight_distance_m"]
                            ].isna().any(axis=1)

                            if missing_after_merge.any():
                                st.info("ì‚¬ì „ ê³„ì‚°ëœ ë°ì´í„°ê°€ ì—†ëŠ” ì¹´í˜ì— ëŒ€í•´ API í˜¸ì¶œë¡œ ì ‘ê·¼ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.")
                                walk_times, walk_path_distances, walk_straight_distances, stations, transit_types = [], [], [], [], []
                                for _, row in analysis_results.iterrows():
                                    lat_val, lng_val = row.get("lat"), row.get("lng")
                                    if pd.isna(lat_val) or pd.isna(lng_val):
                                        walk_times.append(np.nan)
                                        walk_path_distances.append(np.nan)
                                        walk_straight_distances.append(np.nan)
                                        stations.append(None)
                                        transit_types.append(None)
                                        continue

                                    if (
                                        row.get("walk_time_minutes") is not None
                                        and not pd.isna(row.get("walk_time_minutes"))
                                        and row.get("walk_distance_m") is not None
                                        and not pd.isna(row.get("walk_distance_m"))
                                    ):
                                        walk_times.append(row.get("walk_time_minutes"))
                                        walk_path_distances.append(row.get("walk_distance_m"))
                                        walk_straight_distances.append(row.get("straight_distance_m"))
                                        stations.append(row.get("nearest_station"))
                                        transit_types.append(row.get("transit_type"))
                                        continue

                                    try:
                                        (
                                            walk_time,
                                            station_name,
                                            transit_type,
                                            walk_distance,
                                            straight_distance,
                                        ) = calculate_transit_accessibility(lat_val, lng_val)
                                    except Exception:
                                        walk_time, station_name, transit_type, walk_distance, straight_distance = (
                                            np.nan,
                                            None,
                                            None,
                                            np.nan,
                                            np.nan,
                                        )
                                    walk_times.append(walk_time)
                                    walk_path_distances.append(walk_distance)
                                    walk_straight_distances.append(straight_distance)
                                    stations.append(station_name)
                                    transit_types.append(transit_type)
                                analysis_results["walk_time_minutes"] = walk_times
                                analysis_results["walk_distance_m"] = walk_path_distances
                                analysis_results["straight_distance_m"] = walk_straight_distances
                                analysis_results["nearest_station"] = stations
                                analysis_results["transit_type"] = transit_types
                                analysis_results = analysis_results.drop(columns=[col for col in ["lat_key", "lng_key"] if col in analysis_results.columns])

                    if run_analysis or final_results_state is None:
                        st.session_state["tab4_analysis_results_final"] = analysis_results.copy()

                    for col in ["walk_time_minutes", "walk_distance_m", "straight_distance_m"]:
                        if col in analysis_results.columns:
                            analysis_results[col] = pd.to_numeric(analysis_results[col], errors="coerce")
                    access_info_cols = [
                        col
                        for col in ["walk_time_minutes", "walk_distance_m", "straight_distance_m", "nearest_station", "transit_type", accessibility_col]
                        if col and col in analysis_results.columns
                    ]
                    if access_info_cols:
                        st.markdown("#### ğŸš‡ ì£¼ë³€ ëŒ€ì¤‘êµí†µ ì ‘ê·¼ì„± ìš”ì•½")
                        st.caption("Google Places Nearby + Distance Matrix API ê¸°ë°˜ìœ¼ë¡œ ìµœê·¼ì ‘ ì—­/ì •ë¥˜ì¥ê³¼ ì§ì„ ê±°ë¦¬ë¥¼ ê°€ì ¸ì™€ì„œ ì˜ˆìƒ ë„ë³´ ì‹œê°„ì„ ì¶”ì •í•©ë‹ˆë‹¤.")
                        access_display_cols = [c for c in group_cols if c != "place_id"] + access_info_cols
                        access_height = min(400, max(240, 38 * len(analysis_results)))
                        st.dataframe(
                            analysis_results[access_display_cols],
                            use_container_width=True,
                            hide_index=True,
                            height=access_height,
                        )

                    download_df = analysis_results.copy()
                    download_df["scores"] = download_df["scores"].apply(lambda s: json.dumps(s, ensure_ascii=False))
                    if "corrections" not in download_df.columns:
                        download_df["corrections"] = [[] for _ in range(len(download_df))]
                    download_df["corrections"] = download_df["corrections"].apply(
                        lambda logs: json.dumps(logs, ensure_ascii=False) if isinstance(logs, list) else "[]"
                    )
                    download_df["ì›ë³¸íŒŒì¼"] = selected_path.name
                    st.download_button(
                        "ğŸ“¥ ê°ì„± ë¶„ì„ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                        data=download_df.to_csv(index=False).encode("utf-8-sig"),
                        file_name="google_review_sentiment_summary.csv",
                        mime="text/csv",
                        key="tab4_download_summary",
                    )

                    # ì§€ë„ ì‹œê°í™”
                    valid_map_df = (
                        analysis_results.dropna(subset=["lat", "lng"])
                        if {"lat", "lng"}.issubset(analysis_results.columns)
                        else pd.DataFrame()
                    )
                    if not valid_map_df.empty:
                        st.markdown("#### ğŸ—ºï¸ ì„œìš¸ ì§€ë„ ì‹œê°í™”")

                        metric_options = ["í‰ê· ì¥ì†Œì„±ì ìˆ˜", "í‰ê· ê°ì„±ì ìˆ˜"] + score_columns
                        selected_metric = st.selectbox(
                            "ì§€ë„ì— í‘œì‹œí•  ì§€í‘œ",
                            options=metric_options,
                            index=0,
                            key="tab4_map_metric",
                        )
                        st.caption("HeatMapì€ ì„ íƒí•œ ì§€í‘œ ê°’ì„ ì´ìš©í•´ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.")

                        try:
                            placeness_map = tab4_analysis.build_placeness_map(
                                valid_map_df,
                                value_col=selected_metric,
                            )
                            st_folium(placeness_map, width=None, height=500, key="tab4_map")
                        except Exception as map_err:
                            st.warning(f"ì§€ë„ ì‹œê°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {map_err}")
                    else:
                        st.info("ì§€ë„ í‘œì‹œë¥¼ ìœ„í•œ ì¢Œí‘œ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

                    # ì ‘ê·¼ì„± ë¹„êµ (ë„ë³´ ì‹œê°„ vs ì ‘ê·¼ì„± ì ìˆ˜)
                    if accessibility_col and accessibility_col in analysis_results.columns:
                        analysis_results[accessibility_col] = pd.to_numeric(
                            analysis_results[accessibility_col], errors="coerce"
                        )

                    if accessibility_col and {"walk_time_minutes", accessibility_col}.issubset(analysis_results.columns):
                        st.markdown("#### ğŸš‡ ì ‘ê·¼ì„± ì‹¤í—˜")
                        valid_access_df = analysis_results.dropna(subset=["walk_time_minutes", accessibility_col])
                        if len(valid_access_df) >= 2:
                            try:
                                corr_access, p_access = stats.pearsonr(
                                    valid_access_df["walk_time_minutes"].astype(float),
                                    valid_access_df[accessibility_col].astype(float),
                                )
                                p_access_text = f"{p_access:.4f}" if p_access >= 1e-4 else f"{p_access:.2e}"
                                relation_access = "ì–‘ì˜" if corr_access > 0 else "ìŒì˜"
                                st.write(
                                    f"ì ‘ê·¼ì„± ìƒê´€ê³„ìˆ˜: **{corr_access:.3f}** (p-value={p_access_text}) "
                                    f"â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ {relation_access} ìƒê´€ê´€ê³„ê°€ í™•ì¸ë©ë‹ˆë‹¤ (Î±=0.05)."
                                    if p_access < 0.05
                                    else (
                                        f"ì ‘ê·¼ì„± ìƒê´€ê³„ìˆ˜: **{corr_access:.3f}** (p-value={p_access_text}) "
                                        "â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë¡œ ë³´ê¸° ì–´ë µìŠµë‹ˆë‹¤ (Î±=0.05)."
                                    )
                                )

                                access_fig = px.scatter(
                                    valid_access_df,
                                    x="walk_time_minutes",
                                    y=accessibility_col,
                                    trendline="ols",
                                    labels={"walk_time_minutes": "ë„ë³´ ì‹œê°„(ë¶„)", accessibility_col: "ì ‘ê·¼ì„± ìš”ì¸ ì ìˆ˜"},
                                    title="ë„ë³´ ì‹œê°„ vs ì ‘ê·¼ì„± ìš”ì¸ ì ìˆ˜",
                                )
                                st.plotly_chart(access_fig, use_container_width=True, key="tab4_access_corr")
                                slope_access, intercept_access, _, _, _ = stats.linregress(
                                    valid_access_df["walk_time_minutes"].astype(float),
                                    valid_access_df[accessibility_col].astype(float),
                                )
                                st.caption(f"íšŒê·€ì„ : y = {slope_access:.3f}x + {intercept_access:.3f}")
                            except Exception as access_err:
                                st.warning(f"ë„ë³´ ì‹œê°„ ìƒê´€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {access_err}")
                        else:
                            st.info("ë„ë³´ ì‹œê°„ ìƒê´€ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ ìœ íš¨í•œ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

                        if "straight_distance_m" in valid_access_df.columns and valid_access_df["straight_distance_m"].notna().any():
                            st.markdown("#### ğŸš‡ ì§ì„ ê±°ë¦¬ ê¸°ë°˜ ì ‘ê·¼ì„± ì‹¤í—˜")
                            valid_distance_df = valid_access_df.dropna(subset=["straight_distance_m"])
                            if len(valid_distance_df) >= 2:
                                try:
                                    corr_dist, p_dist = stats.pearsonr(
                                        valid_distance_df["straight_distance_m"].astype(float),
                                        valid_distance_df[accessibility_col].astype(float),
                                    )
                                    p_dist_text = f"{p_dist:.4f}" if p_dist >= 1e-4 else f"{p_dist:.2e}"
                                    relation_dist = "ì–‘ì˜" if corr_dist > 0 else "ìŒì˜"
                                    st.write(
                                        f"ì§ì„ ê±°ë¦¬ ìƒê´€ê³„ìˆ˜: **{corr_dist:.3f}** (p-value={p_dist_text}) "
                                        f"â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ {relation_dist} ìƒê´€ê´€ê³„ê°€ í™•ì¸ë©ë‹ˆë‹¤ (Î±=0.05)."
                                        if p_dist < 0.05
                                        else (
                                            f"ì§ì„ ê±°ë¦¬ ìƒê´€ê³„ìˆ˜: **{corr_dist:.3f}** (p-value={p_dist_text}) "
                                            "â†’ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë¡œ ë³´ê¸° ì–´ë µìŠµë‹ˆë‹¤ (Î±=0.05)."
                                        )
                                    )
                                    distance_fig = px.scatter(
                                        valid_distance_df,
                                        x="straight_distance_m",
                                        y=accessibility_col,
                                        trendline="ols",
                                        labels={"straight_distance_m": "ì§ì„  ê±°ë¦¬(m)", accessibility_col: "ì ‘ê·¼ì„± ìš”ì¸ ì ìˆ˜"},
                                        title="ì§ì„  ê±°ë¦¬ vs ì ‘ê·¼ì„± ìš”ì¸ ì ìˆ˜",
                                    )
                                    st.plotly_chart(distance_fig, use_container_width=True, key="tab4_access_dist_corr")
                                    slope_dist, intercept_dist, _, _, _ = stats.linregress(
                                        valid_distance_df["straight_distance_m"].astype(float),
                                        valid_distance_df[accessibility_col].astype(float),
                                    )
                                    st.caption(f"íšŒê·€ì„ : y = {slope_dist:.3f}x + {intercept_dist:.3f}")
                                except Exception as dist_err:
                                    st.warning(f"ì§ì„ ê±°ë¦¬ ìƒê´€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {dist_err}")
                            else:
                                st.info("ì§ì„ ê±°ë¦¬ ìƒê´€ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ ìœ íš¨í•œ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                        else:
                            st.info("ì ‘ê·¼ì„± ë¹„êµë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    else:
                        st.info("ìƒê´€ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
