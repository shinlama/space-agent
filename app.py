import streamlit as st
import pandas as pd
import numpy as np
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path
import warnings
import re

# Streamlit í˜ì´ì§€ ì„¤ì • (wide ëª¨ë“œë¡œ ì „ì²´ ë„ˆë¹„ ì‚¬ìš©)
st.set_page_config(layout="wide")

warnings.filterwarnings("ignore")

# --- 1. ì¥ì†Œì„± ìš”ì¸ ì •ì˜ (Sentence-BERT Input) ---
FACTOR_DEFINITIONS = {
    "ë¬¼ë¦¬ì  íŠ¹ì„±": {
        "ì‹¬ë¯¸ì„±": "ì¸í…Œë¦¬ì–´, ì¡°ëª…, ê°€êµ¬, ìƒ‰ì±„ ë“± ì‹œê°ì  ì•„ë¦„ë‹¤ì›€ê³¼ ë¯¸ì  ì¦ê±°ì›€ì„ í†µí•´ ì‹¬ë¦¬ì  ìš•êµ¬ë¥¼ ì¶©ì¡±ì‹œí‚¤ëŠ” ì •ë„ì…ë‹ˆë‹¤. í‚¤ì›Œë“œ: ì•„ë¦„ë‹¤ì›€, ì˜ˆì¨, ë””ìì¸, ì¸í…Œë¦¬ì–´, ì¡°ëª…, ì„¸ë ¨ë¨, ê°ê°ì , ë¶„ìœ„ê¸°, ì‹œê°ì , ì¡°í™”ë¡œì›€, ë¯¸ì  ì¦ê±°ì›€",
        "í˜•íƒœì„±": "ê³µê°„ì˜ ì¤‘ì‹¬, ì¶•, ë°©í–¥ì„±, ê²½ê³„, ì—ì›ŒìŒˆì„ í†µí•´ êµ¬ì¡°ì  ì§ˆì„œë¥¼ êµ¬ì¶•í•˜ê³ , ê³µê°„ ì§€ê° ë° ì •ìœ„(Orientation)ì— ë„ì›€ì„ ì£¼ëŠ” ë¬¼ë¦¬ì  êµ¬ì„±ì…ë‹ˆë‹¤. í‚¤ì›Œë“œ: ì¤‘ì‹¬, ì¶•, ë°©í–¥ì„±, ê²½ê³„, ì—ì›ŒìŒˆ, ê°œë°©ê°, êµ¬ì¡°, ë°°ì¹˜, ë™ì„ , ê³µê°„ êµ¬ì„±, íš¨ìœ¨ì„±, ì§ˆì„œì •ì—°, ì²´ê³„ì ",
        "ê°ê°ì  ê²½í—˜": "ë°°ê²½ ìŒì•…, í–¥ê¸°, ê°€êµ¬ ì§ˆê°, ìƒ‰ì±„ ë“± ì˜¤ê°ì„ ìê·¹í•˜ëŠ” ì‹¤ë‚´ë””ìì¸ ìš”ì†Œê°€ ì¾Œì í•˜ê³  ë‹ë³´ì´ë©°, ì¥ì†Œì— ëŒ€í•œ íŠ¹ë³„í•œ ê¸°ì–µê³¼ ì˜ë¯¸ë¥¼ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤. í‚¤ì›Œë“œ: ìŒì•…, í–¥ê¸°, ëƒ„ìƒˆ, ì§ˆê°, ì´‰ê°, ì˜¤ê°, ê°ê°ì , ì²­ê°, í›„ê°, ë¶„ìœ„ê¸°, ìƒ‰ì±„, ê¸°ì–µ",
        "ì ‘ê·¼ì„±": "ëŒ€ì¤‘êµí†µ ì ‘ê·¼, ë„ë³´ ê°€ëŠ¥ì„± ë“± ì¥ì†Œë¥¼ ì‰½ê²Œ ì°¾ì•„ì˜¤ê³  ì´ìš©í•  ìˆ˜ ìˆëŠ” ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê³µê°„ì§€ê°ì˜ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. í‚¤ì›Œë“œ: ì ‘ê·¼ì„±, ìœ„ì¹˜, ê±°ë¦¬, êµí†µ, ë²„ìŠ¤, ì§€í•˜ì² , ë„ë³´, ì´ë™, í¸ë¦¬í•¨, ì£¼ì°¨, ì§„ì…, Traffic accessibility, Walkability",
        "ì¾Œì ì„±": "ì±„ê´‘, ì˜¨ìŠµë„, ì²­ê²°, ì•ˆì „ ë“± ê³µê°„ ì´ìš©ìê°€ ëŠë¼ëŠ” ë¬¼ë¦¬ì  ì•ˆë½ê°ê³¼ ì¾Œì í•¨ì„ ì˜ë¯¸í•˜ë©°, ê³µê°„ì§€ê°ì˜ í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤. í‚¤ì›Œë“œ: ì²­ê²°, ì˜¨ë„, ì±„ê´‘, í†µí’, ìœ„ìƒ, ë°ìŒ, ëƒ‰ë‚œë°©, ê³µê¸°, ì •ëˆ, ì•ˆì „, ì•ˆë½í•¨, Safe and clean"
    },
    "í™œë™ì  íŠ¹ì„±": {
        "í™œë™ì„±": "ëŒ€í™”, ì—…ë¬´, íœ´ì‹, ì‹ì‚¬ ë“± ë‹¤ì–‘í•œ í™œë™ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ë£¨ì–´ì§€ëŠ” ì •ë„ë¥¼ ì˜ë¯¸í•˜ë©°, ê¸°ëŠ¥ì˜ ë³µí•©ì„±ê³¼ ììœ ë¡œìš´ í–‰ë™ ì„ íƒì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. í‚¤ì›Œë“œ: ëŒ€í™”, ì—…ë¬´, íšŒì˜, ì‹ì‚¬, íœ´ì‹, í™œë™, ëª¨ì„, ì¼, ì‘ì—…, ì´ìš©, ê¸°ëŠ¥ì  ë³µí•©ì„±, ë‹¤ì–‘ì„±",
        "ì‚¬íšŒì„±": "ë‹¤ë¥¸ ì‚¬ëŒë“¤ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ì–´ìš¸ë¦¬ê±°ë‚˜ êµë¥˜í•  ìˆ˜ ìˆëŠ” ê°œë°©ì ì´ê³  ì¹œê·¼í•œ ë¶„ìœ„ê¸°ë¥¼ ì˜ë¯¸í•˜ë©°, ê³µë™ ìœ ëŒ€ê° í˜•ì„± ë° ì‚¬íšŒì  ìš•êµ¬ë¥¼ ì¶©ì¡±ì‹œí‚µë‹ˆë‹¤. í‚¤ì›Œë“œ: êµë¥˜, ì†Œí†µ, ì¹œê·¼, ì¹œì ˆ, ì„œë¹„ìŠ¤, ì–´ìš¸ë¦¼, ëŒ€ì¸ ê´€ê³„, ì»¤ë®¤ë‹ˆí‹°, ê°œë°©ì , êµê°, ì‚¬íšŒì , í•¨ê»˜, íŒŒí‹°, Social participation",
        "ì°¸ì—¬ì„±": "ì´ìš©ìê°€ ì´ë²¤íŠ¸, ì²´í—˜, í´ë˜ìŠ¤ ë“± ê³µê°„ ë‚´ì—ì„œ ëŠ¥ë™ì ìœ¼ë¡œ ì°¸ì—¬í•˜ê³  ê²½í—˜í•  ìˆ˜ ìˆëŠ” ì •ë„ë¥¼ ì˜ë¯¸í•˜ë©°, í™˜ê²½ì— ì£¼ì²´ì ìœ¼ë¡œ ì˜í–¥ì„ ì£¼ë ¤ëŠ” í†µì œ ìš•êµ¬ë¥¼ ì¶©ì¡±ì‹œí‚µë‹ˆë‹¤. í‚¤ì›Œë“œ: ì°¸ì—¬, ì²´í—˜, í´ë˜ìŠ¤, ì›ë°ì´, ì›Œí¬ìˆ, í–‰ì‚¬, ì´ë²¤íŠ¸, í™œë™, ì§ì ‘, ê²½í—˜, ì£¼ì²´ì , Self participation"
    },
    "ì˜ë¯¸ì  íŠ¹ì„±": {
        "ê³ ìœ ì„±": "ë‹¤ë¥¸ ì¥ì†Œì™€ ì°¨ë³„í™”ë˜ëŠ” ë…íŠ¹í•œ ì½˜ì…‰íŠ¸ë‚˜ ìƒì§•ì  ë””ìì¸ìœ¼ë¡œ ì¥ì†Œë§Œì˜ ì •ì²´ì„±ì„ í˜•ì„±í•˜ë©°, ì´ìš©ìê°€ ìì•„ë¥¼ í‘œì¶œí•˜ëŠ” ìˆ˜ë‹¨ìœ¼ë¡œ í™œìš©ë©ë‹ˆë‹¤. í‚¤ì›Œë“œ: ë…íŠ¹, ê°œì„±, ì°¨ë³„í™”, ì»¨ì…‰, ìƒì§•, ìœ ë‹ˆí¬, ë…ì°½ì , ì•„ì´ë´í‹°í‹°, ê³ ìœ , ì •ì²´ì„±, Preference, Meaning, Personal identity",
        "ê¸°ì–µ/ê²½í—˜": "íŠ¹ë³„í•œ ì¶”ì–µì´ë‚˜ ì˜ë¯¸ ìˆëŠ” ê²½í—˜ì„ ì œê³µí•˜ì—¬ ì˜¤ë˜ ê¸°ì–µì— ë‚¨ìœ¼ë©°, ì‹¬ë¦¬ì  ìš”ì†Œê°€ ê°œì…ëœ ê±´ì¶•ì  ì²´í—˜ì„ í†µí•´ ì¥ì†Œì„±ì„ ì§€ì†ì‹œí‚µë‹ˆë‹¤. í‚¤ì›Œë“œ: ì¶”ì–µ, ê¸°ì–µ, ê²½í—˜, ê°ë™, ì¸ìƒì , íšŒìƒ, ìŠ¤í† ë¦¬, ì˜ë¯¸, íŠ¹ë³„í•¨, íšŒê³ , Functional attachment",
        "ì§€ì—­ ì •ì²´ì„±": "ì¥ì†Œê°€ ìœ„ì¹˜í•œ ì§€ì—­ì˜ ë¬¸í™”, ìƒì§•ì„ ë°˜ì˜í•˜ì—¬ ê³ ìœ í•œ ì§€ì—­ ì´ë¯¸ì§€ë¥¼ í˜•ì„±í•˜ë©°, ì§€ì—­ì„±ì´ë‚˜ ì „í†µì„± ë¶€ê°ì„ í†µí•´ ê°•í•œ ì¥ì†Œì„±ì„ ê°–ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ: ì§€ì—­ì„±, ì§€ì—­ ì´ë¯¸ì§€, ë¬¸í™”, ì „í†µ, ìƒì§•, ë¡œì»¬, ë™ë„¤, í•«í”Œë ˆì´ìŠ¤, ì§€ì—­ ê¸°ë°˜, Cultural image, Regional landmark",
        "ë¬¸í™”ì  ë§¥ë½": "ê³µê°„ì´ ìœ„ì¹˜í•œ ì§€ì—­ì˜ ì—­ì‚¬, ë¬¸í™”ì  ë°°ê²½, ìŠ¤í† ë¦¬ ë“±ì„ ë°˜ì˜í•˜ë©°, ë¬¸ëª…ê¶Œì˜ ë¬¸í™”ì  ì²´ê³„ì— ë”°ë¥¸ ì˜ë¯¸ì  ì§ˆì„œë¥¼ í†µí•´ ì¥ì†Œì˜ ë§¥ë½ì„ ê°•í™”í•©ë‹ˆë‹¤. í‚¤ì›Œë“œ: ì—­ì‚¬, ë¬¸í™”, ë°°ê²½, ìŠ¤í† ë¦¬, ì „í†µ, ì„œì‚¬, ì§€ì—­ì„±, ì˜ë¯¸, ë§¥ë½, ì˜¤ë˜ëœ"
    }
}

ALL_FACTORS = {k: v for outer_dict in FACTOR_DEFINITIONS.values() for k, v in outer_dict.items()}

# --- 2. ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì„¤ì • ---
BASE_DIR = Path(__file__).resolve().parent
GOOGLE_REVIEW_SAMPLE_CSV = BASE_DIR / "google_reviews_scraped_cleaned.csv"

# --- 2-1. ì•Œê³ ë¦¬ì¦˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---
SIMILARITY_THRESHOLD = 0.4  # ë¦¬ë·°ì™€ ìš”ì¸ ì •ì˜ ê°„ ìµœì†Œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0.0~1.0)
# ê¶Œì¥ê°’: 0.5 (ì¤‘ê°„ í•„í„°ë§) - ë„ˆë¬´ ë‚®ìœ¼ë©´(0.3~0.4) ê´€ë ¨ ì—†ëŠ” ë¦¬ë·° í¬í•¨, ë„ˆë¬´ ë†’ìœ¼ë©´(0.7~0.8) ê´€ë ¨ ë¦¬ë·° ëˆ„ë½

# --- 3. ëª¨ë¸ ë¡œë“œ ë° ìºì‹± (Streamlit ì„±ëŠ¥ ìµœì í™”) ---
# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì´ë¦„ ì €ì¥
_sentiment_model_name = None

@st.cache_resource
def load_models():
    """Sentence-BERTì™€ ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    global _sentiment_model_name
    # 1. Sentence-BERT ëª¨ë¸ ë¡œë“œ (ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°ìš©)
    with st.spinner("ëª¨ë¸ ë¡œë“œ ì¤‘: Sentence-BERT (ìœ ì‚¬ë„ìš©)..."):
        try:
            sbert_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        except Exception as e:
            st.warning(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: {e}")
            sbert_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')
    
    # 2. ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ (í•œêµ­ì–´ ë¦¬ë·° ê°ì„± ë¶„ì„ íŠ¹í™” ëª¨ë¸)
    with st.spinner("ëª¨ë¸ ë¡œë“œ ì¤‘: í•œêµ­ì–´ ê°ì„± ë¶„ì„ ëª¨ë¸..."):
        sentiment_pipeline = None
        model_loaded = False
        
        # ìš°ì„ ìˆœìœ„ 1: í•œêµ­ì–´ ê°ì„± ë¶„ì„ ì „ìš© fine-tuned ëª¨ë¸
        model_candidates = [
            {
                "name": "matthewburke/korean_sentiment",
                "description": "í•œêµ­ì–´ ê°ì„± ë¶„ì„ ì „ìš© ëª¨ë¸"
            },
            {
                "name": "nlptown/bert-base-multilingual-uncased-sentiment",
                "description": "ë‹¤êµ­ì–´ ê°ì„± ë¶„ì„ ëª¨ë¸ (í•œêµ­ì–´ í¬í•¨, 5ë‹¨ê³„ ê°ì„±)"
            },
            {
                "name": "beomi/KoELECTRA-v3-discriminator",
                "description": "KoELECTRA v3 (ìµœì‹  ë²„ì „)"
            },
            {
                "name": "beomi/KcELECTRA-base",
                "description": "KoELECTRA base (ê¸°ì¡´)"
            },
            {
                "name": "monologg/kobert-base-v1",
                "description": "KoBERT (fallback)"
            }
        ]
        
        for model_info in model_candidates:
            try:
                sentiment_model_name = model_info["name"]
                st.info(f"ì‹œë„ ì¤‘: {model_info['description']} ({sentiment_model_name})")
                
                # íŠ¹ë³„ ì²˜ë¦¬: nlptown ëª¨ë¸ì€ ì´ë¯¸ fine-tunedë˜ì–´ ìˆìŒ
                if "nlptown" in sentiment_model_name:
                    sentiment_pipeline = pipeline(
                        "sentiment-analysis",
                        model=sentiment_model_name,
                        device=0 if torch.cuda.is_available() else -1
                    )
                else:
                    tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)
                    # num_labels í™•ì¸ (nlptownì€ 5, ë‚˜ë¨¸ì§€ëŠ” 2 ë˜ëŠ” 3)
                    if "nlptown" in sentiment_model_name or "multilingual" in sentiment_model_name:
                        num_labels = 5
                    else:
                        num_labels = 2
                    
                    model = AutoModelForSequenceClassification.from_pretrained(
                        sentiment_model_name, 
                        num_labels=num_labels
                    )
                    device = 0 if torch.cuda.is_available() else -1
                    sentiment_pipeline = pipeline(
                        "sentiment-analysis",
                        model=model,
                        tokenizer=tokenizer,
                        device=device
                    )
                
                st.success(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_info['description']}")
                _sentiment_model_name = sentiment_model_name
                model_loaded = True
                break
                
            except Exception as e:
                st.warning(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({model_info['name']}): {e}")
                continue
        
        if not model_loaded or sentiment_pipeline is None:
            st.error("ëª¨ë“  ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•´ì£¼ì„¸ìš”.")
            st.stop()

    return sbert_model, sentiment_pipeline, _sentiment_model_name

# --- 3-1. ìˆ«ì-only í…ìŠ¤íŠ¸ í™•ì¸ í•¨ìˆ˜ ---
def is_numeric_only(text: str) -> bool:
    """
    í…ìŠ¤íŠ¸ê°€ ìˆ«ìë§Œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        text: í™•ì¸í•  í…ìŠ¤íŠ¸
    
    Returns:
        bool: ìˆ«ìë§Œ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ True
    """
    if text is None:
        return False
    text = str(text).strip()
    return bool(re.fullmatch(r"[0-9]+(\.[0-9]+)?", text))

# --- 3-1-1. ë©”íƒ€ë°ì´í„°-only í…ìŠ¤íŠ¸ í™•ì¸ í•¨ìˆ˜ ---
def is_metadata_only(text: str) -> bool:
    """
    í…ìŠ¤íŠ¸ê°€ ë©”íƒ€ë°ì´í„°ë§Œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    (ì˜ˆ: "ì„œë¹„ìŠ¤ë§¤ì¥ ë‚´ ì‹ì‚¬ì‹ì‚¬ ìœ í˜•ì•„ì¹¨ ì‹ì‚¬", "ì‹ì‚¬ ìœ í˜•ë¸ŒëŸ°ì¹˜" ë“±)
    
    Args:
        text: í™•ì¸í•  í…ìŠ¤íŠ¸
    
    Returns:
        bool: ë©”íƒ€ë°ì´í„°ë§Œ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ True
    """
    if text is None:
        return False
    text = str(text).strip()
    
    # ë©”íƒ€ë°ì´í„° íŒ¨í„´ë“¤
    metadata_patterns = [
        r'^ì„œë¹„ìŠ¤.*ì‹ì‚¬.*ìœ í˜•',
        r'^ì‹ì‚¬.*ìœ í˜•',
        r'^ì„œë¹„ìŠ¤.*ë§¤ì¥.*ë‚´.*ì‹ì‚¬',
        r'^ìŒì‹:\s*\d+.*ì„œë¹„ìŠ¤:\s*\d+.*ë¶„ìœ„ê¸°:\s*\d+$',  # "ìŒì‹: 5ì„œë¹„ìŠ¤: 5ë¶„ìœ„ê¸°: 5" ê°™ì€ íŒ¨í„´
        r'^ìŒì‹:\s*\d+$',  # "ìŒì‹: 5" ê°™ì€ íŒ¨í„´
        r'^ì„œë¹„ìŠ¤:\s*\d+$',
        r'^ë¶„ìœ„ê¸°:\s*\d+$',
    ]
    
    for pattern in metadata_patterns:
        if re.match(pattern, text, re.IGNORECASE):
            return True
    
    # ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸ (10ì ì´í•˜)ë„ ë©”íƒ€ë°ì´í„°ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŒ
    # í•˜ì§€ë§Œ ì´ê±´ ë„ˆë¬´ ê´‘ë²”ìœ„í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
    # if len(text) <= 10:
    #     return True
    
    return False

# --- 3-2. ê°ì„± ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ í—¬í¼ í•¨ìˆ˜ ---
def process_sentiment_result(result, model_name=""):
    """
    ë‹¤ì–‘í•œ ê°ì„± ë¶„ì„ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ í†µì¼ëœ í˜•ì‹(ê¸ì •/ë¶€ì •, ì ìˆ˜)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        result: sentiment_pipelineì˜ ê²°ê³¼ (dict ë˜ëŠ” list)
        model_name: ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„ (ì„ íƒì )
    
    Returns:
        tuple: (label: str, score: float) - 'ê¸ì •'/'ë¶€ì •'/'ì¤‘ë¦½', 0.0~1.0 ì ìˆ˜
    """
    if isinstance(result, list):
        # ë°°ì¹˜ ê²°ê³¼ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©
        result = result[0] if len(result) > 0 else {}
    
    label = str(result.get('label', '')).upper()
    score = float(result.get('score', 0.5))
    
    # nlptown ëª¨ë¸ ì²˜ë¦¬ (5ë‹¨ê³„: 1-5ì )
    if 'nlptown' in model_name.lower() or 'multilingual' in model_name.lower():
        # label í˜•ì‹: "1 star", "2 stars", "3 stars", "4 stars", "5 stars"
        if '5' in label or 'FIVE' in label:
            return ('ê¸ì •', 0.9)
        elif '4' in label or 'FOUR' in label:
            return ('ê¸ì •', 0.7)
        elif '3' in label or 'THREE' in label:
            return ('ì¤‘ë¦½', 0.5)
        elif '2' in label or 'TWO' in label:
            return ('ë¶€ì •', 0.3)
        elif '1' in label or 'ONE' in label:
            return ('ë¶€ì •', 0.1)
        else:
            # ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ íŒë‹¨
            if score >= 0.6:
                return ('ê¸ì •', score)
            elif score <= 0.4:
                return ('ë¶€ì •', 1 - score)
            else:
                return ('ì¤‘ë¦½', 0.5)
    
    # ì¼ë°˜ì ì¸ 2ë‹¨ê³„ ëª¨ë¸ ì²˜ë¦¬ (ê¸ì •/ë¶€ì •)
    if any(pos in label for pos in ['POSITIVE', 'ê¸ì •', 'LABEL_1', '1', 'POS']):
        return ('ê¸ì •', score)
    elif any(neg in label for neg in ['NEGATIVE', 'ë¶€ì •', 'LABEL_0', '0', 'NEG']):
        return ('ë¶€ì •', 1 - score)
    else:
        # ë ˆì´ë¸”ì„ ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš° ì ìˆ˜ë¡œ íŒë‹¨
        if score >= 0.6:
            return ('ê¸ì •', score)
        elif score <= 0.4:
            return ('ë¶€ì •', 1 - score)
        else:
            return ('ì¤‘ë¦½', 0.5)

# --- 4. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ---
@st.cache_data
def load_data(file_path: Path):
    """ë¦¬ë·° ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    if not file_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    try:
        df = pd.read_csv(
            file_path, 
            encoding="utf-8-sig",
            on_bad_lines='skip',  # ì˜ëª»ëœ ë¼ì¸ì€ ê±´ë„ˆë›°ê¸°
            quoting=1,  # QUOTE_ALL
            escapechar='\\'
        )
    except UnicodeDecodeError:
        df = pd.read_csv(
            file_path, 
            encoding="cp949",
            on_bad_lines='skip',
            quoting=1,
            escapechar='\\'
        )
    except Exception as e:
        st.warning(f"CSV ì½ê¸° ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰
        try:
            df = pd.read_csv(
                file_path, 
                encoding="utf-8-sig",
                on_bad_lines='skip',
                engine='python'
            )
        except:
            df = pd.read_csv(
                file_path, 
                encoding="utf-8-sig",
                on_bad_lines='skip',
                sep=',',
                quotechar='"',
                escapechar='\\',
                engine='python'
            )
    
    # ì»¬ëŸ¼ëª… ì •ê·œí™” (í•œêµ­ì–´ ì»¬ëŸ¼ëª… ì²˜ë¦¬)
    column_mapping = {
        "ìƒí˜¸ëª…": "cafe_name",
        "ë¦¬ë·°": "review_text",
        "cafe_name": "cafe_name",
        "review_text": "review_text"
    }
    
    for old_col, new_col in column_mapping.items():
        if old_col in df.columns and new_col not in df.columns:
            df[new_col] = df[old_col]
    
    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
    if "cafe_name" not in df.columns or "review_text" not in df.columns:
        st.error(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")
        st.stop()
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    initial_count = len(df)
    initial_cafe_count = df['cafe_name'].nunique() if 'cafe_name' in df.columns else 0
    
    df = df[['cafe_name', 'review_text']].dropna()
    after_dropna_count = len(df)
    after_dropna_cafe_count = df['cafe_name'].nunique() if 'cafe_name' in df.columns else 0
    
    df = df[df['review_text'].astype(str).str.strip() != '']
    final_count = len(df)
    final_cafe_count = df['cafe_name'].nunique() if 'cafe_name' in df.columns else 0
    
    st.success(f"ë¦¬ë·° ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì´ {final_count}ê±´")
    st.info(f"ğŸ“Š ê³ ìœ  ì¹´í˜ ìˆ˜: {final_cafe_count}ê°œ (ì´ˆê¸°: {initial_cafe_count}ê°œ, ê²°ì¸¡ì¹˜ ì œê±° í›„: {after_dropna_cafe_count}ê°œ)")
    
    if initial_cafe_count > final_cafe_count:
        excluded = initial_cafe_count - final_cafe_count
        st.warning(f"âš ï¸ {excluded}ê°œ ì¹´í˜ê°€ ë¹ˆ ë¦¬ë·°ë¡œ ì¸í•´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return df

# --- 5. ì•Œê³ ë¦¬ì¦˜ í•µì‹¬: ê°ì„± ë¶„ì„ ë° ìœ ì‚¬ë„ ê¸°ë°˜ ìš”ì¸ ì ìˆ˜ ê³„ì‚° ---
def calculate_place_scores(df_reviews, sbert_model, sentiment_pipeline, factor_defs, similarity_threshold=0.5):
    """
    Sentence-BERTì™€ ê°ì„± ë¶„ì„ì„ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œì„± ìš”ì¸ë³„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    ë¦¬ë·°ë³„ ì ìˆ˜ë„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    st.subheader("1. Sentence-BERT ì„ë² ë”© ìƒì„±")
    
    # 1. ì¥ì†Œì„± ì •ì˜ ë¬¸ì¥ ì„ë² ë”© (ê³ ì • ë²¡í„°)
    factor_sentences = list(factor_defs.values())
    factor_names = list(factor_defs.keys())
    
    with st.spinner("ì¥ì†Œì„± ìš”ì¸ ì •ì˜ ì„ë² ë”© ìƒì„± ì¤‘..."):
        factor_embeddings = sbert_model.encode(factor_sentences, convert_to_tensor=True, show_progress_bar=False)
    
    # ê²°ê³¼ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸
    results_list = []
    review_scores_list = []  # ë¦¬ë·°ë³„ ì ìˆ˜ ì €ì¥
    
    # ì¹´í˜ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì²˜ë¦¬
    cafe_groups = df_reviews.groupby('cafe_name')
    total_cafes = len(cafe_groups)
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, (cafe_name, group) in enumerate(cafe_groups):
        status_text.text(f"ì²˜ë¦¬ ì¤‘: {cafe_name} ({idx+1}/{total_cafes})")
        progress_bar.progress((idx + 1) / total_cafes)
        
        # 2. ê°œë³„ ë¦¬ë·° ì„ë² ë”©
        review_texts = group['review_text'].astype(str).tolist()
        review_indices = group.index.tolist()
        
        with st.spinner(f"{cafe_name} ë¦¬ë·° ì„ë² ë”© ìƒì„± ì¤‘..."):
            review_embeddings = sbert_model.encode(review_texts, convert_to_tensor=True, show_progress_bar=False)
        
        # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ë¦¬ë·° ë¬¸ì¥ vs. 12ê°œ ìš”ì¸ ì •ì˜ ë¬¸ì¥)
        similarity_matrix = cosine_similarity(
            review_embeddings.cpu().numpy(), 
            factor_embeddings.cpu().numpy()
        )
        
        # 4. ìš”ì¸ë³„ ì ìˆ˜ ì§‘ê³„
        cafe_scores = {'cafe_name': cafe_name}
        
        # ê° ë¦¬ë·°ë³„ë¡œ ìš”ì¸ ì ìˆ˜ ê³„ì‚°
        for review_idx, (review_text, review_original_idx) in enumerate(zip(review_texts, review_indices)):
            review_factor_scores = {
                'review_index': review_original_idx,
                'cafe_name': cafe_name,
                'review_text': review_text
            }
            
            # ê° ìš”ì¸ë³„ë¡œ ë°˜ë³µ
            for i, factor_name in enumerate(factor_names):
                similarity_score = similarity_matrix[review_idx, i]
                
                # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°ì—ë§Œ ì ìˆ˜ ê³„ì‚°
                if similarity_score >= similarity_threshold:
                    # í•´ë‹¹ ë¦¬ë·°ì— ëŒ€í•œ ê°ì„± ë¶„ì„
                    try:
                        sentiment_result = sentiment_pipeline([review_text])[0]
                        label, positive_prob = process_sentiment_result(sentiment_result, _sentiment_model_name)
                        
                        # ìœ ì‚¬ë„ì™€ ê°ì„± ì ìˆ˜ë¥¼ ê²°í•© (ê°€ì¤‘ í‰ê· )
                        combined_score = 0.6 * similarity_score + 0.4 * positive_prob
                        review_factor_scores[f'{factor_name}_ì ìˆ˜'] = combined_score
                        review_factor_scores[f'{factor_name}_ìœ ì‚¬ë„'] = similarity_score
                    except Exception as e:
                        review_factor_scores[f'{factor_name}_ì ìˆ˜'] = np.nan
                        review_factor_scores[f'{factor_name}_ìœ ì‚¬ë„'] = similarity_score
                else:
                    review_factor_scores[f'{factor_name}_ì ìˆ˜'] = np.nan
                    review_factor_scores[f'{factor_name}_ìœ ì‚¬ë„'] = similarity_score
            
            review_scores_list.append(review_factor_scores)
        
        # ê° ìš”ì¸ë³„ë¡œ ë°˜ë³µ (ì¹´í˜ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°)
        for i, factor_name in enumerate(factor_names):
            # 4-1. ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒì¸ ë¬¸ì¥ ì„ ë³„
            relevant_review_indices = np.where(similarity_matrix[:, i] >= similarity_threshold)[0]
            
            if len(relevant_review_indices) > 0:
                relevant_texts = [review_texts[idx] for idx in relevant_review_indices]
                
                # 4-2. ê°ì„± ë¶„ì„ ì ìš© (0~1 ê¸ì • ì ìˆ˜)
                try:
                    sentiment_results = sentiment_pipeline(relevant_texts)
                    
                    # í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì„± ì ìˆ˜ ì¶”ì¶œ
                    sentiment_scores = []
                    for res in sentiment_results:
                        label, score = process_sentiment_result(res, _sentiment_model_name)
                        sentiment_scores.append(score)
                    
                    # 4-3. ì„¸ë¶€ í•­ëª© ìµœì¢… ì ìˆ˜ ì‚°ì¶œ (ì‚°ìˆ  í‰ê· )
                    avg_score = np.mean(sentiment_scores) if sentiment_scores else 0.5
                    cafe_scores[f'ì ìˆ˜_{factor_name}'] = avg_score
                    cafe_scores[f'ë¦¬ë·°ìˆ˜_{factor_name}'] = len(relevant_texts)
                    
                except Exception as e:
                    st.warning(f"{cafe_name} - {factor_name} ê°ì„± ë¶„ì„ ì˜¤ë¥˜: {e}")
                    cafe_scores[f'ì ìˆ˜_{factor_name}'] = np.nan
                    cafe_scores[f'ë¦¬ë·°ìˆ˜_{factor_name}'] = 0
            else:
                # ê´€ë ¨ ë¦¬ë·°ê°€ ì—†ìœ¼ë©´ NaN ì²˜ë¦¬
                cafe_scores[f'ì ìˆ˜_{factor_name}'] = np.nan
                cafe_scores[f'ë¦¬ë·°ìˆ˜_{factor_name}'] = 0
        
        results_list.append(cafe_scores)
    
    progress_bar.empty()
    status_text.empty()
    
    df_cafe_scores = pd.DataFrame(results_list)
    df_review_scores = pd.DataFrame(review_scores_list)
    
    return df_cafe_scores, df_review_scores

# --- 6. ì•Œê³ ë¦¬ì¦˜ í•µì‹¬: ê°œë³„ ë¦¬ë·° ê°ì„± ë¶„ì„ (í•œêµ­ì–´ ê°ì„± ë¶„ì„ ëª¨ë¸ í™œìš©) ---
def run_sentiment_analysis(df_reviews, sentiment_pipeline, model_name="", ratings=None):
    """
    ê°œë³„ ë¦¬ë·° í…ìŠ¤íŠ¸ì— ëŒ€í•´ í•œêµ­ì–´ ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        df_reviews: ë¦¬ë·° ë°ì´í„°í”„ë ˆì„
        sentiment_pipeline: ê°ì„± ë¶„ì„ íŒŒì´í”„ë¼ì¸
        model_name: ëª¨ë¸ ì´ë¦„
        ratings: í‰ì  ë¦¬ìŠ¤íŠ¸ (ì„ íƒì , ë©”íƒ€ë°ì´í„°-only ë¦¬ë·° ì²˜ë¦¬ìš©)
    """
    st.subheader("2. ê°œë³„ ë¦¬ë·° ê°ì„± ë¶„ì„ (í•œêµ­ì–´ ê°ì„± ë¶„ì„ ëª¨ë¸)")
    
    review_texts = df_reviews['review_text'].astype(str).tolist()
    
    # í‰ì  ì •ë³´ ì¶”ì¶œ (ìˆìœ¼ë©´ ì‚¬ìš©)
    if ratings is None:
        if 'í‰ì ' in df_reviews.columns:
            ratings = df_reviews['í‰ì '].astype(float).tolist()
        elif 'rating' in df_reviews.columns:
            ratings = df_reviews['rating'].astype(float).tolist()
        else:
            ratings = [None] * len(review_texts)
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    progress_bar = st.progress(0)
    batch_size = 32
    total_batches = (len(review_texts) + batch_size - 1) // batch_size
    
    sentiment_scores = []
    sentiment_labels = []
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(review_texts))
        batch_texts = review_texts[start_idx:end_idx]
        
        progress_bar.progress((batch_idx + 1) / total_batches)
        
        try:
            # ìˆ«ì-only ë¦¬ë·°ì™€ ì¼ë°˜ í…ìŠ¤íŠ¸ ë¦¬ë·° ë¶„ë¦¬
            text_batch = []
            batch_results_map = {}  # ì¸ë±ìŠ¤ -> ê²°ê³¼ ë§¤í•‘
            
            for idx, text in enumerate(batch_texts):
                global_idx = start_idx + idx
                rating = ratings[global_idx] if global_idx < len(ratings) and ratings[global_idx] is not None else None
                
                # ìˆ«ì-only ë¦¬ë·°ëŠ” ë³„ì  ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
                if is_numeric_only(text):
                    try:
                        rating_value = float(text)
                        if rating_value >= 4.0:
                            batch_results_map[idx] = ("ê¸ì •", 0.9)
                        elif rating_value >= 3.0:
                            batch_results_map[idx] = ("ì¤‘ë¦½", 0.5)
                        else:
                            batch_results_map[idx] = ("ë¶€ì •", 0.1)
                    except ValueError:
                        # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ì²˜ë¦¬
                        batch_results_map[idx] = ("ì¤‘ë¦½", 0.5)
                # ë©”íƒ€ë°ì´í„°-only ë¦¬ë·°ë„ ë³„ì  ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
                elif is_metadata_only(text) and rating is not None:
                    try:
                        rating_value = float(rating)
                        if rating_value >= 4.0:
                            batch_results_map[idx] = ("ê¸ì •", 0.9)
                        elif rating_value >= 3.0:
                            batch_results_map[idx] = ("ì¤‘ë¦½", 0.5)
                        else:
                            batch_results_map[idx] = ("ë¶€ì •", 0.1)
                    except (ValueError, TypeError):
                        # í‰ì  ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ì²˜ë¦¬
                        batch_results_map[idx] = ("ì¤‘ë¦½", 0.5)
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¦¬ë·°ëŠ” ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•´ ìˆ˜ì§‘
                    text_batch.append((idx, text))
            
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¦¬ë·°ëŠ” ëª¨ë¸ ì‚¬ìš©
            if text_batch:
                text_only = [text for _, text in text_batch]
                model_results = sentiment_pipeline(text_only)
                
                # ëª¨ë¸ ê²°ê³¼ë¥¼ ì¸ë±ìŠ¤ì— ë§¤í•‘
                for (idx, _), res in zip(text_batch, model_results):
                    label, score = process_sentiment_result(res, model_name)
                    batch_results_map[idx] = (label, score)
            
            # ì›ë˜ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ì¶”ê°€
            for idx in range(len(batch_texts)):
                label, score = batch_results_map[idx]
                sentiment_labels.append(label)
                sentiment_scores.append(score)
                
        except Exception as e:
            st.warning(f"ë°°ì¹˜ {batch_idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë¦½ ì ìˆ˜ í• ë‹¹
            sentiment_labels.extend(['ì¤‘ë¦½'] * len(batch_texts))
            sentiment_scores.extend([0.5] * len(batch_texts))
    
    progress_bar.empty()
    
    # ë¦¬ë·° ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
    df_reviews = df_reviews.copy()
    df_reviews['sentiment_score'] = sentiment_scores
    df_reviews['sentiment_label'] = sentiment_labels
    
    # ì¹´í˜ë³„ í‰ê·  ê°ì„± ì ìˆ˜ ì‚°ì¶œ
    avg_sentiment = df_reviews.groupby('cafe_name')['sentiment_score'].mean().reset_index()
    avg_sentiment.rename(columns={'sentiment_score': 'í‰ê· _ë¦¬ë·°_ê°ì„±ì ìˆ˜'}, inplace=True)
    
    st.success("ê°œë³„ ë¦¬ë·° ê°ì„± ë¶„ì„ ë° ì¹´í˜ë³„ í‰ê·  ì‚°ì¶œ ì™„ë£Œ.")
    return df_reviews, avg_sentiment

# --- 7. Streamlit UI êµ¬ì„± ---
def main():
    st.title("ì¥ì†Œì„± ê¸°ë°˜ ê³µê°„ ì •ëŸ‰ í‰ê°€ ì‹œìŠ¤í…œ (LLM & BERT)")
    st.markdown("---")
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    file_path = GOOGLE_REVIEW_SAMPLE_CSV
    
    # 1. ëª¨ë¸ ë¡œë“œ
    sbert_model, sentiment_pipeline, sentiment_model_name = load_models()
    
    # 2. ë°ì´í„° ë¡œë“œ
    if not file_path.exists():
        st.error(f"âš ï¸ ì—ëŸ¬: ë¦¬ë·° ë°ì´í„° íŒŒì¼ '{file_path.name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.info(f"ì˜ˆìƒ ê²½ë¡œ: {file_path}")
        return
    
    try:
        df_reviews = load_data(file_path)
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return
    
    if df_reviews.empty:
        st.warning("ë¡œë“œëœ ë¦¬ë·° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° í†µê³„ í‘œì‹œ
    unique_cafes = df_reviews['cafe_name'].nunique()
    reviews_per_cafe = df_reviews.groupby('cafe_name').size()
    
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ì¹´í˜ ìˆ˜", f"{unique_cafes:,}ê°œ")
    with col2:
        st.metric("í‰ê·  ë¦¬ë·° ìˆ˜/ì¹´í˜", f"{reviews_per_cafe.mean():.1f}ê°œ")
    with col3:
        st.metric("ìµœëŒ€ ë¦¬ë·° ìˆ˜/ì¹´í˜", f"{reviews_per_cafe.max()}ê°œ")
    
    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    st.markdown("---")
    st.header("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
    
    # ì „ì²´ ë°ì´í„° ë¡œë“œ (ë¯¸ë¦¬ë³´ê¸°ìš©)
    try:
        df_preview = pd.read_csv(
            file_path, 
            encoding="utf-8-sig",
            on_bad_lines='skip',  # ì˜ëª»ëœ ë¼ì¸ì€ ê±´ë„ˆë›°ê¸°
            quoting=1,  # QUOTE_ALL (ëª¨ë“  í•„ë“œë¥¼ ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°)
            escapechar='\\'  # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì
        )
    except UnicodeDecodeError:
        df_preview = pd.read_csv(
            file_path, 
            encoding="cp949",
            on_bad_lines='skip',
            quoting=1,
            escapechar='\\'
        )
    except Exception as e:
        st.warning(f"CSV ì½ê¸° ì¤‘ ì¼ë¶€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰ (ìµœì„ ì˜ ë…¸ë ¥ìœ¼ë¡œ ì½ê¸°)
        try:
            df_preview = pd.read_csv(
                file_path, 
                encoding="utf-8-sig",
                on_bad_lines='skip',
                engine='python'  # Python ì—”ì§„ ì‚¬ìš© (ë” ê´€ëŒ€í•¨)
            )
        except:
            df_preview = pd.read_csv(
                file_path, 
                encoding="utf-8-sig",
                on_bad_lines='skip',
                sep=',',
                quotechar='"',
                escapechar='\\',
                engine='python'
            )
    
    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸ ë° ì„ íƒ
    required_cols = ['ìƒí˜¸ëª…', 'ì‹œêµ°êµ¬ëª…', 'í–‰ì •ë™ëª…', 'í‰ì ', 'ë¦¬ë·°']
    available_cols = [col for col in required_cols if col in df_preview.columns]
    
    if len(available_cols) == len(required_cols):
        # í–‰ì •êµ¬ë³„ë¡œ ì •ë ¬ (ì‹œêµ°êµ¬ëª…, ìƒí˜¸ëª…, í–‰ì •ë™ëª… ìˆœ)
        df_preview_sorted = df_preview[available_cols].copy()
        df_preview_sorted = df_preview_sorted.sort_values(by=['ì‹œêµ°êµ¬ëª…', 'ìƒí˜¸ëª…', 'í–‰ì •ë™ëª…'], ascending=[True, True, True])
        
        # í‘œë¥¼ í™”ë©´ ì „ì²´ ë„ˆë¹„ë¡œ í‘œì‹œí•˜ê¸° ìœ„í•œ CSS ìŠ¤íƒ€ì¼
        st.markdown("""
<style>
        .stDataFrame {
            width: 100% !important;
        }
        div[data-testid="stDataFrame"] {
            width: 100% !important;
    }
</style>
""", unsafe_allow_html=True)

        st.dataframe(
            df_preview_sorted,
            use_container_width=True,
            hide_index=True,
            height=600
        )
        st.caption(f"ì „ì²´ {len(df_preview_sorted):,}ê°œ ë¦¬ë·° (í–‰ì •êµ¬ë³„ ì •ë ¬)")
        
        # ê°ì„± ë¶„ì„ ì¶”ê°€ ë²„íŠ¼
        st.markdown("---")
        if st.button("ğŸ” ê°ì„± ë¶„ì„ ì¶”ê°€ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)", type="secondary"):
            with st.spinner(f"ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬ë·°ë³„ ê°ì„± ë¶„ì„ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
                # ë¦¬ë·° í…ìŠ¤íŠ¸ ë° í‰ì  ì¶”ì¶œ
                review_texts = df_preview_sorted['ë¦¬ë·°'].astype(str).tolist()
                ratings = df_preview_sorted['í‰ì '].astype(float).tolist() if 'í‰ì ' in df_preview_sorted.columns else [None] * len(review_texts)
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                progress_bar = st.progress(0)
                status_text = st.empty()
                batch_size = 32
                total_batches = (len(review_texts) + batch_size - 1) // batch_size
                
                sentiment_labels = []
                sentiment_scores = []
                
                for batch_idx in range(total_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min((batch_idx + 1) * batch_size, len(review_texts))
                    batch_texts = review_texts[start_idx:end_idx]
                    batch_ratings = ratings[start_idx:end_idx] if ratings else [None] * len(batch_texts)
                    
                    progress = (batch_idx + 1) / total_batches
                    progress_bar.progress(progress)
                    status_text.text(f"ì²˜ë¦¬ ì¤‘: {batch_idx + 1}/{total_batches} ë°°ì¹˜ ({len(batch_texts)}ê°œ ë¦¬ë·°)")
                    
                    try:
                        # ìˆ«ì-only ë¦¬ë·°ì™€ ì¼ë°˜ í…ìŠ¤íŠ¸ ë¦¬ë·° ë¶„ë¦¬
                        text_batch = []
                        batch_results_map = {}  # ì¸ë±ìŠ¤ -> ê²°ê³¼ ë§¤í•‘
                        
                        for idx, text in enumerate(batch_texts):
                            rating = batch_ratings[idx] if idx < len(batch_ratings) else None
                            
                            # ìˆ«ì-only ë¦¬ë·°ëŠ” ë³„ì  ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
                            if is_numeric_only(text):
                                try:
                                    rating_value = float(text)
                                    if rating_value >= 4.0:
                                        batch_results_map[idx] = ("ê¸ì •", 0.9)
                                    elif rating_value >= 3.0:
                                        batch_results_map[idx] = ("ì¤‘ë¦½", 0.5)
                                    else:
                                        batch_results_map[idx] = ("ë¶€ì •", 0.1)
                                except ValueError:
                                    # ìˆ«ì ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ì²˜ë¦¬
                                    batch_results_map[idx] = ("ì¤‘ë¦½", 0.5)
                            # ë©”íƒ€ë°ì´í„°-only ë¦¬ë·°ë„ ë³„ì  ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
                            elif is_metadata_only(text) and rating is not None:
                                try:
                                    rating_value = float(rating)
                                    if rating_value >= 4.0:
                                        batch_results_map[idx] = ("ê¸ì •", 0.9)
                                    elif rating_value >= 3.0:
                                        batch_results_map[idx] = ("ì¤‘ë¦½", 0.5)
                                    else:
                                        batch_results_map[idx] = ("ë¶€ì •", 0.1)
                                except (ValueError, TypeError):
                                    # í‰ì  ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ì²˜ë¦¬
                                    batch_results_map[idx] = ("ì¤‘ë¦½", 0.5)
                            else:
                                # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¦¬ë·°ëŠ” ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•´ ìˆ˜ì§‘
                                text_batch.append((idx, text))
                        
                        # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¦¬ë·°ëŠ” ëª¨ë¸ ì‚¬ìš©
                        if text_batch:
                            text_only = [text for _, text in text_batch]
                            model_results = sentiment_pipeline(text_only)
                            
                            # ëª¨ë¸ ê²°ê³¼ë¥¼ ì¸ë±ìŠ¤ì— ë§¤í•‘
                            for (idx, _), res in zip(text_batch, model_results):
                                label, score = process_sentiment_result(res, sentiment_model_name)
                                batch_results_map[idx] = (label, score)
                        
                        # ì›ë˜ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ì¶”ê°€
                        for idx in range(len(batch_texts)):
                            label, score = batch_results_map[idx]
                            sentiment_labels.append(label)
                            sentiment_scores.append(score)
                            
                    except Exception as e:
                        st.warning(f"ë°°ì¹˜ {batch_idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë¦½ ì²˜ë¦¬
                        sentiment_labels.extend(['ì¤‘ë¦½'] * len(batch_texts))
                        sentiment_scores.extend([0.5] * len(batch_texts))
                
                progress_bar.empty()
                status_text.empty()
                
                # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
                df_preview_with_sentiment = df_preview_sorted.copy()
                df_preview_with_sentiment['ê°ì„±ë¶„ì„'] = sentiment_labels
                df_preview_with_sentiment['ê°ì„±ì ìˆ˜'] = [f"{s:.3f}" for s in sentiment_scores]
                
                # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬ (ê°ì„±ë¶„ì„ ì»¬ëŸ¼ì„ ë¦¬ë·° ì˜†ì— ë°°ì¹˜)
                column_order = ['ìƒí˜¸ëª…', 'ì‹œêµ°êµ¬ëª…', 'í–‰ì •ë™ëª…', 'í‰ì ', 'ë¦¬ë·°', 'ê°ì„±ë¶„ì„', 'ê°ì„±ì ìˆ˜']
                df_preview_with_sentiment = df_preview_with_sentiment[column_order]
                
                st.success(f"âœ… ê°ì„± ë¶„ì„ ì™„ë£Œ! {len(sentiment_labels):,}ê°œ ë¦¬ë·° ë¶„ì„ë¨")
                
                # ê²°ê³¼ í‘œì‹œ
                st.dataframe(
                    df_preview_with_sentiment,
                    use_container_width=True,
                            hide_index=True,
                    height=600
                )
                
                # í†µê³„ ì •ë³´
                col1, col2, col3 = st.columns(3)
                with col1:
                    positive_count = sentiment_labels.count('ê¸ì •')
                    st.metric("ê¸ì • ë¦¬ë·°", f"{positive_count:,}ê°œ ({positive_count/len(sentiment_labels)*100:.1f}%)")
                with col2:
                    negative_count = sentiment_labels.count('ë¶€ì •')
                    st.metric("ë¶€ì • ë¦¬ë·°", f"{negative_count:,}ê°œ ({negative_count/len(sentiment_labels)*100:.1f}%)")
                with col3:
                    neutral_count = sentiment_labels.count('ì¤‘ë¦½')
                    if neutral_count > 0:
                        st.metric("ì¤‘ë¦½ ë¦¬ë·°", f"{neutral_count:,}ê°œ ({neutral_count/len(sentiment_labels)*100:.1f}%)")
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                csv = df_preview_with_sentiment.to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    "ğŸ“¥ ê°ì„± ë¶„ì„ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name="google_reviews_with_sentiment.csv",
                    mime="text/csv"
                )
    else:
        st.warning(f"í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {list(df_preview.columns)}")
        # ê¸°ë³¸ ì»¬ëŸ¼ìœ¼ë¡œ í‘œì‹œ
        if 'ìƒí˜¸ëª…' in df_preview.columns or 'cafe_name' in df_preview.columns:
            cafe_col = 'ìƒí˜¸ëª…' if 'ìƒí˜¸ëª…' in df_preview.columns else 'cafe_name'
            review_col = 'ë¦¬ë·°' if 'ë¦¬ë·°' in df_preview.columns else 'review_text'
            preview_cols = [cafe_col, review_col]
            if all(col in df_preview.columns for col in preview_cols):
                df_preview_sorted = df_preview[preview_cols].copy()
                if 'ì‹œêµ°êµ¬ëª…' in df_preview.columns:
                    df_preview_sorted = df_preview_sorted.sort_values(by='ì‹œêµ°êµ¬ëª…', ascending=True)
                st.dataframe(
                    df_preview_sorted,
                    use_container_width=True,
                    hide_index=True,
                    height=600
                )
                st.caption(f"ì „ì²´ {len(df_preview_sorted):,}ê°œ ë¦¬ë·°")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'df_review_scores' not in st.session_state:
        st.session_state.df_review_scores = None
    if 'df_reviews_with_sentiment' not in st.session_state:
        st.session_state.df_reviews_with_sentiment = None
    
    # --- 3. ì‹¤í–‰ íŒŒíŠ¸: ì¥ì†Œì„± ìš”ì¸ ì ìˆ˜ ê³„ì‚° ---
    st.header("ğŸ“Š 1. ì¥ì†Œì„± ìš”ì¸ë³„ ì •ëŸ‰ ì ìˆ˜ ê³„ì‚°")
    st.caption(f"ìœ ì‚¬ë„ ì„ê³„ê°’: {SIMILARITY_THRESHOLD} (ì½”ë“œ ë‚´ ê³ ì •ê°’)")
    
    # Sentence-BERTë¥¼ ì‚¬ìš©í•œ ìš”ì¸ ì ìˆ˜ ê³„ì‚° ì‹¤í–‰
    if st.button("ì¥ì†Œì„± ìš”ì¸ ì ìˆ˜ ê³„ì‚° ì‹œì‘", type="primary"):
        with st.spinner("12ê°œ ì¥ì†Œì„± ìš”ì¸ë³„ ì ìˆ˜ ê³„ì‚° ì¤‘ (Sentence-BERT & Sentiment Analysis)..."):
            try:
                df_place_scores, df_review_scores = calculate_place_scores(
                    df_reviews.copy(), 
                    sbert_model, 
                    sentiment_pipeline, 
                    ALL_FACTORS, 
                    similarity_threshold=SIMILARITY_THRESHOLD
                )
                
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.df_review_scores = df_review_scores
                
                st.subheader("âœ… ì¹´í˜ë³„ ì¥ì†Œì„± ìš”ì¸ ì ìˆ˜ (0~1)")
                st.dataframe(df_place_scores.set_index('cafe_name'), use_container_width=True)
                
                # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                csv = df_place_scores.to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    "ì¥ì†Œì„± ìš”ì¸ ì ìˆ˜ CSV ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name="placeness_factor_scores.csv",
                    mime="text/csv"
                )
                
            except Exception as e:
                st.error(f"ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                st.code(traceback.format_exc())
    
    # --- 4. ì‹¤í–‰ íŒŒíŠ¸: ê°œë³„ ë¦¬ë·° ê°ì„± ë¶„ì„ (KoBERT) ---
    st.header("2. ê°œë³„ ë¦¬ë·° ê°ì„± ë¶„ì„ ë° ì¹´í˜ë³„ í‰ê· ")
    
    # KoBERTë¥¼ ì‚¬ìš©í•œ ê°œë³„ ë¦¬ë·° ê°ì„± ë¶„ì„ ì‹¤í–‰
    if st.button("KoBERT ê°œë³„ ë¦¬ë·° ê°ì„± ë¶„ì„ ì‹œì‘", type="primary"):
        with st.spinner("ê°œë³„ ë¦¬ë·° ê¸ì •/ë¶€ì • ê°ì„± ì ìˆ˜ ê³„ì‚° ì¤‘ (KoBERT/KoELECTRA)..."):
            try:
                df_reviews_with_sentiment, df_avg_sentiment = run_sentiment_analysis(
                    df_reviews.copy(), 
                    sentiment_pipeline,
                    sentiment_model_name
                )
                
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.df_reviews_with_sentiment = df_reviews_with_sentiment
                
                st.subheader("âœ… ì¹´í˜ë³„ í‰ê·  ê°ì„± ì ìˆ˜")
                st.dataframe(df_avg_sentiment.set_index('cafe_name'), use_container_width=True)
                
                st.subheader("âœ… ê°œë³„ ë¦¬ë·° ê°ì„± ë¶„ì„ ê²°ê³¼ (ìƒ˜í”Œ)")
                sample_df = df_reviews_with_sentiment[['cafe_name', 'review_text', 'sentiment_label', 'sentiment_score']].head(20)
                st.dataframe(sample_df, use_container_width=True)
                
                # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                csv = df_reviews_with_sentiment.to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    "ğŸ“¥ ê°œë³„ ë¦¬ë·° ê°ì„± ë¶„ì„ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name="review_sentiment_analysis.csv",
                    mime="text/csv"
                )
            except Exception as e:
                st.error(f"ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                st.code(traceback.format_exc())
    
    # --- 5. ë¦¬ë·°ë³„ ìƒì„¸ ê²°ê³¼ í‘œì‹œ ---
    st.header("ğŸ“Š ë¦¬ë·°ë³„ ìƒì„¸ ë¶„ì„ ê²°ê³¼")
    
    # ë‘ ë¶„ì„ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
    has_sentiment = st.session_state.df_reviews_with_sentiment is not None
    has_placeness = st.session_state.df_review_scores is not None
    
    if not has_sentiment and not has_placeness:
        st.info("ğŸ‘† ìœ„ì˜ ë‘ ë¶„ì„ì„ ëª¨ë‘ ì‹¤í–‰í•˜ë©´ ë¦¬ë·°ë³„ ìƒì„¸ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        # ê²°ê³¼ ë³‘í•©
        if has_sentiment and has_placeness:
            # ë‘ ê²°ê³¼ë¥¼ ë³‘í•©
            df_sentiment = st.session_state.df_reviews_with_sentiment.copy()
            df_placeness = st.session_state.df_review_scores.copy()
            
            # ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
            df_sentiment['review_index'] = df_sentiment.index
            df_placeness['review_index'] = df_placeness['review_index']
            
            # ë³‘í•©
            df_merged = pd.merge(
                df_sentiment[['review_index', 'cafe_name', 'review_text', 'sentiment_label', 'sentiment_score']],
                df_placeness,
                on=['review_index', 'cafe_name', 'review_text'],
                how='outer'
            )
            
            # 12ê°œ ìš”ì¸ ì ìˆ˜ ì»¬ëŸ¼ ì¶”ì¶œ
            factor_names = list(ALL_FACTORS.keys())
            factor_score_cols = [f'{factor}_ì ìˆ˜' for factor in factor_names]
            
            # í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ
            display_cols = ['cafe_name', 'review_text', 'sentiment_label', 'sentiment_score'] + factor_score_cols
            
            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
            available_cols = [col for col in display_cols if col in df_merged.columns]
            
            st.subheader("âœ… ë¦¬ë·°ë³„ ê°ì„± ë¶„ì„ + ì¥ì†Œì„± ìš”ì¸ ì ìˆ˜")
            
            # í•„í„° ì˜µì…˜
            col1, col2 = st.columns(2)
            with col1:
                selected_cafe = st.selectbox(
                    "ì¹´í˜ ì„ íƒ (ì „ì²´ ë³´ê¸°)",
                    options=['ì „ì²´'] + sorted(df_merged['cafe_name'].unique().tolist()),
                    key="review_detail_cafe_filter"
                )
            with col2:
                selected_sentiment = st.selectbox(
                    "ê°ì„± í•„í„°",
                    options=['ì „ì²´', 'ê¸ì •', 'ë¶€ì •'],
                    key="review_detail_sentiment_filter"
                )
            
            # í•„í„°ë§
            filtered_df = df_merged.copy()
            if selected_cafe != 'ì „ì²´':
                filtered_df = filtered_df[filtered_df['cafe_name'] == selected_cafe]
            if selected_sentiment != 'ì „ì²´':
                filtered_df = filtered_df[filtered_df['sentiment_label'] == selected_sentiment]
            
            # ê²°ê³¼ í‘œì‹œ
            if len(filtered_df) > 0:
                display_df = filtered_df[available_cols].copy()
                
                # ì ìˆ˜ í¬ë§·íŒ… (ì†Œìˆ˜ì  3ìë¦¬)
                for col in factor_score_cols:
                    if col in display_df.columns:
                        display_df[col] = display_df[col].apply(lambda x: f"{x:.3f}" if pd.notna(x) else "N/A")
                
                # ê°ì„± ì ìˆ˜ í¬ë§·íŒ…
                if 'sentiment_score' in display_df.columns:
                    display_df['sentiment_score'] = display_df['sentiment_score'].apply(lambda x: f"{x:.3f}" if pd.notna(x) else "N/A")
                
                st.dataframe(
                    display_df,
                    use_container_width=True,
                    hide_index=True,
                    height=600
                )

                st.caption(f"ì´ {len(filtered_df):,}ê°œ ë¦¬ë·° í‘œì‹œ")
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                csv = filtered_df[available_cols].to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    "ğŸ“¥ ë¦¬ë·°ë³„ ìƒì„¸ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
                    data=csv,
                    file_name="review_detailed_analysis.csv",
                    mime="text/csv"
                )
            else:
                st.warning("ì„ íƒí•œ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        elif has_sentiment:
            st.info("ì¥ì†Œì„± ìš”ì¸ ì ìˆ˜ ê³„ì‚°ì„ ì‹¤í–‰í•˜ë©´ ë¦¬ë·°ë³„ ìƒì„¸ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            sample_df = st.session_state.df_reviews_with_sentiment[['cafe_name', 'review_text', 'sentiment_label', 'sentiment_score']].head(50)
            st.dataframe(sample_df, use_container_width=True, hide_index=True, height=400)
        
        elif has_placeness:
            st.info("ê°ì„± ë¶„ì„ì„ ì‹¤í–‰í•˜ë©´ ë¦¬ë·°ë³„ ìƒì„¸ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            sample_df = st.session_state.df_review_scores[['cafe_name', 'review_text'] + [f'{factor}_ì ìˆ˜' for factor in list(ALL_FACTORS.keys())[:5]]].head(50)
            st.dataframe(sample_df, use_container_width=True, hide_index=True, height=400)

if __name__ == "__main__":
    main()
