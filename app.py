import streamlit as st
import googlemaps
from openai import OpenAI
from langgraph.graph import StateGraph, END
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import os
from dotenv import load_dotenv
import json
import re
from wordcloud import WordCloud
import numpy as np
import warnings # ê²½ê³  ë©”ì‹œì§€ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€

# NLP ëª¨ë¸ ê´€ë ¨ ì„í¬íŠ¸ (Hugging Face Transformers)
try:
    from sentence_transformers import SentenceTransformer
    from transformers import pipeline
    from transformers import logging as transformers_logging # íŠ¸ëœìŠ¤í¬ë¨¸ ë¡œê¹… ì„í¬íŠ¸
    # ëª…ì‹œì  ë¡œë“œë¥¼ ìœ„í•œ ì¶”ê°€ import
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
except ImportError:
    st.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(sentence-transformers, transformers)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. `pip install sentence-transformers transformers` ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    st.stop()


# ----------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •, ìºì‹± ë° ìœ í‹¸ë¦¬í‹°
# ----------------------------------------------------

# í°íŠ¸ ê²½ë¡œ ìºì‹± (WordCloudìš©)
@st.cache_resource(show_spinner=False)
def get_font_path():
    # ì‹¤ì œ í™˜ê²½ì— ë§ì¶° í°íŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆì‹œ: NotoSansKR-Regular.ttf íŒŒì¼ì´ 'fonts' í´ë”ì— ìˆë‹¤ê³  ê°€ì •
    # ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜, ì‹œìŠ¤í…œ í°íŠ¸ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # í˜„ì¬ëŠ” ë”ë¯¸ ê²½ë¡œë¡œ ì„¤ì •í•˜ë©°, ì‹¤í–‰ í™˜ê²½ì— ë”°ë¼ ìˆ˜ì • í•„ìš”
    try:
        # Streamlit í°íŠ¸ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì‹¤ì œ í°íŠ¸ ê²½ë¡œë¥¼ ì§€ì •
        return os.path.join(os.path.dirname(__file__), 'fonts', 'NotoSansKR-Regular.ttf')
    except:
        return None # í°íŠ¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°

font_path = get_font_path()

# WordCloud ì´ë¯¸ì§€ ìºì‹± (PIL ì´ë¯¸ì§€ ë°˜í™˜)
@st.cache_data(show_spinner=False)
def generate_wordcloud(text: str, font_path: str, colormap: str = "Greens", **kwargs):
    if not text or not text.strip():
        return None
    
    # í°íŠ¸ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ê²½ê³ 
    if font_path and os.path.exists(font_path):
        font_kwarg = {'font_path': font_path}
    else:
        # st.warning("WordCloud í°íŠ¸ íŒŒì¼ ê²½ë¡œê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font_kwarg = {}
        
    wc = WordCloud(
        **font_kwarg,
        background_color="white",
        width=600,
        height=300,
        scale=2,
        max_words=180,
        prefer_horizontal=0.9,
        colormap=colormap,
        collocations=True,
        normalize_plurals=False,
        relative_scaling=0.35,
        min_font_size=8,
        max_font_size=90,
        random_state=42,
        regexp=r"[ê°€-í£a-zA-Z]+" # í•œê¸€, ì˜ë¬¸ë§Œ í¬í•¨
    ).generate(text)
    return wc.to_image()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

st.set_page_config(page_title="Seoul Place Recommendation", page_icon="ğŸ—ºï¸", layout="centered")

# ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜ (OpenAI ê¸°ë°˜)
def semantic_split(text: str) -> list[str]:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•´ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì¥ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    """
    try:
        prompt = f"""
        ì•„ë˜ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„(í•˜ë‚˜ì˜ ê°ì •ì´ë‚˜ í‰ê°€ë¥¼ ë‹´ì€ ë‹¨ë½)ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”.
        ê° ë¬¸ì¥ì€ ë…ë¦½ì ì¸ íŒë‹¨ì´ ê°€ëŠ¥í•œ ë‹¨ìœ„ì—¬ì•¼ í•˜ë©°, ë¶ˆí•„ìš”í•œ ì ‘ì†ì‚¬ëŠ” ì œê±°í•˜ì„¸ìš”.
        ì¶œë ¥ì€ JSON ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.

        ì˜ˆì‹œ:
        ì…ë ¥: "ì¹´í˜ ë¶„ìœ„ê¸°ê°€ ì¢‹ê³  ì»¤í”¼ëŠ” ë§›ìˆì§€ë§Œ ì¢Œì„ì´ ì¢ì•„ìš”."
        ì¶œë ¥: ["ì¹´í˜ ë¶„ìœ„ê¸°ê°€ ì¢‹ë‹¤.", "ì»¤í”¼ê°€ ë§›ìˆë‹¤.", "ì¢Œì„ì´ ì¢ë‹¤."]

        ë¦¬ë·° í…ìŠ¤íŠ¸:
        {text}
        """

        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "semantic_split_schema",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "sentences": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["sentences"],
                        "additionalProperties": False
                    }
                }
            },
            temperature=0
        )
        content = resp.choices[0].message.content
        parsed = json.loads(content) if content else {"sentences": []}
        if isinstance(parsed, list):
            candidates = parsed
        else:
            candidates = parsed.get("sentences", [])
        cleaned = [c.strip() for c in candidates if isinstance(c, str) and len(c.strip()) > 2]
        if cleaned:
            return cleaned
    except Exception as e:
        print(f"Semantic split error: {e}")
        # í´ë°±ì€ ì•„ë˜ ì¼ë°˜ ë¶„ê¸°ì—ì„œ ìˆ˜í–‰
        pass
    # í´ë°±: ë¬¸ì¥ë¶€í˜¸ â†’ ì ‘ì† í‘œí˜„ 2ë‹¨ê³„ ë¶„í• 
    fallback_units = []
    for s in re.split(r'[.!?]\s*', text):
        if not s or not s.strip():
            continue
        parts = re.split(r'(?:í•˜ì§€ë§Œ|ê·¸ëŸ¬ë‚˜|ê·¸ëŸ°ë°|ì¸ë°|ì§€ë§Œ|ëŠ”ë°)', s)
        for p in parts:
            p = p.strip()
            if len(p) > 2:
                fallback_units.append(p)
    return fallback_units


# ìºì‹œëœ ì˜ë¯¸ ë¶„í•  ë˜í¼
@st.cache_data(show_spinner=False)
def cached_semantic_split(text: str) -> List[str]:
    return semantic_split(text)


# ìºì‹œëœ LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ë˜í¼
@st.cache_data(show_spinner=False)
def cached_unified_summary(review_text: str):
    try:
        sample = review_text[:1200]
        unified_prompt = f"""
        ë‹¤ìŒ ë¦¬ë·°ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¥ì†Œë¥¼ ë¶„ì„í•˜ì—¬ í•œ ë²ˆì— ê²°ê³¼ë§Œ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
        1) positive_keywords: ì¥ì†Œ/ì¥ì†Œì„± ê´€ë ¨ ê¸ì • í•µì‹¬ ë‹¨ì–´ ìµœëŒ€ 10ê°œ (ë¬¸ìì—´ ë°°ì—´)
        2) negative_keywords: ì¥ì†Œ/ì¥ì†Œì„± ê´€ë ¨ ë¶€ì • í•µì‹¬ ë‹¨ì–´ ìµœëŒ€ 10ê°œ (ë¬¸ìì—´ ë°°ì—´)
        3) summary: ì „ë°˜ì  ë¶„ìœ„ê¸°/ê³µê°„ íŠ¹ì„±/ì£¼ìš” ê²½í—˜ ì¤‘ì‹¬ì˜ 5~8ë¬¸ì¥ ìš”ì•½ (ë¬¸ìì—´)

        ë¦¬ë·° í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì •ì˜ êµ¬ê°„ ì•ˆì˜ í…ìŠ¤íŠ¸ë§Œ ì°¸ê³ í•˜ì„¸ìš”:
        ```
        {sample}
        ```
        """
        unified_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": unified_prompt}],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "unified_summary_schema",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "positive_keywords": {"type": "array", "items": {"type": "string"}},
                            "negative_keywords": {"type": "array", "items": {"type": "string"}},
                            "summary": {"type": "string"}
                        },
                        "required": ["positive_keywords", "negative_keywords", "summary"],
                        "additionalProperties": False
                    }
                }
            },
            temperature=0
        )
        content = unified_response.choices[0].message.content
        parsed = json.loads(content) if content else {"positive_keywords": [], "negative_keywords": [], "summary": ""}
        return {
            "positive_keywords": parsed.get("positive_keywords", []) or [],
            "negative_keywords": parsed.get("negative_keywords", []) or [],
            "summary": (parsed.get("summary") or "").strip() or "ë¦¬ë·° ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ LLM ìš”ì•½ì´ ì–´ë µìŠµë‹ˆë‹¤."
        }
    except Exception as e:
        print(f"LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "positive_keywords": [],
            "negative_keywords": [],
            "summary": "LLM ìš”ì•½ ì‹¤íŒ¨. NLP ë¶„ì„ë§Œ ì§„í–‰ë¨."
        }

# ----------------------------------------------------
# 2. API í‚¤ ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ----------------------------------------------------

if "history" not in st.session_state:
    st.session_state.history = []

# API í‚¤ ë¡œë“œ
gmaps_key = os.getenv("Maps_API_KEY") or st.secrets.get("Maps_API_KEY", "")
openai_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")

if "gmaps_key" not in st.session_state:
    st.session_state.gmaps_key = gmaps_key or ""
if "openai_key" not in st.session_state:
    st.session_state.openai_key = openai_key or ""

# API í‚¤ ì…ë ¥ UI (í‚¤ê°€ ì—†ëŠ” ê²½ìš°)
if not st.session_state.gmaps_key or not st.session_state.openai_key:
    st.title("ğŸ—ºï¸ Seoul Place Recommendation and Spatial Evaluation System")

    st.info("API í‚¤ë¥¼ `.env` íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.")
    st.markdown("---")
    gmaps_input = st.text_input("Google Maps API Key", type="password")
    openai_input = st.text_input("OpenAI API Key", type="password")

    if st.button("Start"):
        if gmaps_input and openai_input:
            st.session_state.gmaps_key = gmaps_input
            st.session_state.openai_key = openai_input
            st.rerun()
        else:
            st.warning("Please enter both API keys.")
    st.stop()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (OpenAI clientëŠ” í´ë°± ë¡œì§ì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ ì „ì—­ì ìœ¼ë¡œ ìœ ì§€)
try:
    gmaps = googlemaps.Client(key=st.session_state.gmaps_key)
    # LLM í´ë¼ì´ì–¸íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    client = OpenAI(api_key=st.session_state.openai_key)
except Exception as e:
    st.error(f"API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()


# ----------------------------------------------------
# 3. LangGraph State ì •ì˜ ë° ëª¨ë¸ ë¡œë”© (ìºì‹œ)
# ----------------------------------------------------

class AgentState(BaseModel):
    query: str
    places: Optional[List[Dict[str, Any]]] = Field(default_factory=list)
    answer: Optional[str] = ""

# ì¥ì†Œì„± ìš”ì¸ ì„ë² ë”© ë° ëª¨ë¸ ë¡œë“œ (Sentence-BERT)
def _compute_factors_hash(path: str = "factors.json") -> str:
    try:
        with open(path, "rb") as f:
            import hashlib
            return hashlib.md5(f.read()).hexdigest()
    except Exception:
        return ""

@st.cache_resource(show_spinner="ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_category_embeddings(factors_hash: str):
    try:
        # í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©ì— ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©
        model = SentenceTransformer("jhgan/ko-sroberta-multitask")
        # factors.json íŒŒì¼ ë¡œë“œ
        with open("factors.json", "r", encoding="utf-8") as f:
            factors = json.load(f)
    except FileNotFoundError:
        st.error("ì˜¤ë¥˜: 'factors.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— íŒŒì¼ì„ ìƒì„±í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()
    except Exception as e:
        st.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    embeddings = {}
    score_structure = {}
    
    # 11ê°œ ì„¸ë¶€ ìš”ì¸ì˜ ì •ì˜ ë¬¸ì¥ì„ ì„ë² ë”©
    for main_cat, subcats in factors.items():
        score_structure[main_cat] = {}
        for subcat, definition in subcats.items():
            emb = model.encode(definition, normalize_embeddings=True)
            embeddings[subcat] = emb
            score_structure[main_cat][subcat] = None
            
    return embeddings, model, score_structure

factors_hash = _compute_factors_hash()
category_embeddings, embed_model, new_score_structure_template = load_category_embeddings(factors_hash)

@st.cache_resource(show_spinner="ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
def load_sentiment_model_tabularis():
    """
    ê³µê°œ ê°ì • ë¶„í¬í˜• ëª¨ë¸ (tabularisai/multilingual-sentiment-analysis) ê¸°ë°˜
    - ì¶œë ¥: 0.0 ~ 1.0 ì—°ì† ì ìˆ˜ (ë¶€ì •â†’ê¸ì •)
    """
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    import numpy as np

    model_name = "tabularisai/multilingual-sentiment-analysis"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    pipe = pipeline("text-classification", model=model, tokenizer=tokenizer, return_all_scores=True)
    weights = np.linspace(0, 1, 5)  # [0.0, 0.25, 0.5, 0.75, 1.0]

    def predict_score(sentences: List[str]):
        if not sentences:
            return []
        results = pipe(sentences)
        scores = []
        for res in results:
            probs = np.array([r['score'] for r in res])
            score = float(np.dot(probs, weights))
            scores.append(score)
        return scores

    return predict_score


sentiment_model = load_sentiment_model_tabularis()


# ----------------------------------------------------
# 4. LangGraph Node ì •ì˜
# ----------------------------------------------------

def search_places(state: AgentState):
    """Google Maps APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜"""
    if state.places is None:
        state.places = []
    try:
        # ì„œìš¸ì‹œì²­ ê¸°ì¤€(37.5665,126.9780) ë°˜ê²½ 10km ê²€ìƒ‰
        res = gmaps.places(query=state.query, language="ko", location="37.5665,126.9780", radius=10000)
        state.places = res.get('results', [])[:5]
    except Exception as e:
        st.error(f"Google Maps ì¥ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("Google Maps API í‚¤ê°€ ìœ íš¨í•œì§€, ë˜ëŠ” Places APIê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        state.places = []
    return state.dict()

def analyze_reviews(state: AgentState):
    """Sentence-BERT + Tabularis ê¸°ë°˜ ê°ì„± ë¶„ì„ìœ¼ë¡œ ì¥ì†Œì„± ì •ëŸ‰ í‰ê°€"""
    if state.places is None:
        state.places = []

    place_infos = []
    
    # ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì • (ì™„í™”)
    SIMILARITY_THRESHOLD = 0.35
    
    for place in state.places:
        place_id = place.get("place_id")
        if not place_id:
            continue

        details = gmaps.place(place_id=place_id, language="ko").get('result', {})
        reviews = details.get('reviews', [])[:10] # ìµœëŒ€ 10ê°œ ë¦¬ë·°
        review_text = "\n".join([review['text'] for review in reviews if review.get('text')])

        scores = json.loads(json.dumps(new_score_structure_template))
        
        # LLM í˜¸ì¶œì„ ìœ„í•œ í‚¤ì›Œë“œ/ìš”ì•½ ë³€ìˆ˜
        summary = "ë¶„ì„ ì¤‘..."
        positive_keywords = [] # LLMì´ ì¶”ì¶œí•œ ë‹¨ì–´
        negative_keywords = [] # LLMì´ ì¶”ì¶œí•œ ë‹¨ì–´
        
        # ì¥ì†Œì„± ì •ëŸ‰ í‰ê°€ (NLP ê¸°ë°˜)
        if review_text.strip():
            # OpenAI ê¸°ë°˜ ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬ (ë¬¸ì„œ ì „ì²´ë¥¼ ì…ë ¥) - ìºì‹œ ì‚¬ìš©
            review_units = cached_semantic_split(review_text)
            if not review_units:
                # ì¶”ê°€ ë°©ì–´: í´ë°± ë¶„í•  ì¬ì‹¤í–‰ (ìºì‹œ ì‚¬ìš©)
                review_units = cached_semantic_split(" ".join([r.get('text','') for r in reviews]))
            
            # 1. LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ë° ì›Œë“œí´ë¼ìš°ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ (OpenAI API ì‚¬ìš©)
            try:
                cached = cached_unified_summary(review_text)
                positive_keywords = cached["positive_keywords"]
                negative_keywords = cached["negative_keywords"]
                summary = cached["summary"]
            except Exception as e:
                print(f"LLM ìš”ì•½/í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                summary = "LLM ìš”ì•½ ì‹¤íŒ¨. NLP ë¶„ì„ë§Œ ì§„í–‰ë¨."
                positive_keywords, negative_keywords = [], []


            # 2. ì¥ì†Œì„± ì„¸ë¶€ í•­ëª©ë³„ ì ìˆ˜ ì‚°ì • (SBERT + SA ê¸°ë°˜)
            
            # ì¥ì†Œì„± ì„¸ë¶€ í•­ëª© ì´ë¦„: [ê´€ë ¨ ì˜ë¯¸ë‹¨ìœ„ì˜ ê°ì„± ì ìˆ˜ ë¦¬ìŠ¤íŠ¸]
            factor_sentiment_map = {f_name: [] for f_name in category_embeddings.keys()}
            
            # ì˜ë¯¸ ë‹¨ìœ„ë³„ ê°ì„± ì ìˆ˜ ê³„ì‚° (0~1 ì—°ì†ê°’)
            sentiment_scores = sentiment_model(review_units)

            # ê°ì„± ë¶„ì„ ê²°ê³¼ì™€ ì˜ë¯¸ ë‹¨ìœ„ë¥¼ ë§¤í•‘ (ë°°ì¹˜ ì„ë² ë”© + ë²¡í„°í™” ìœ ì‚¬ë„)
            # ê³¼ë„í•œ ê¸¸ì´ ë°©ì§€: ì˜ë¯¸ ë‹¨ìœ„ ìƒí•œ
            MAX_UNITS = 60
            if len(review_units) > MAX_UNITS:
                review_units = review_units[:MAX_UNITS]
                sentiment_scores = sentiment_scores[:MAX_UNITS]

            try:
                unit_embs = embed_model.encode(review_units, normalize_embeddings=True)
                # ìš”ì¸ ì„ë² ë”© í–‰ë ¬ êµ¬ì„± (ìˆœì„œ ê³ ì •)
                subcat_list = list(category_embeddings.keys())
                factor_mat = np.stack([category_embeddings[s] for s in subcat_list], axis=0)
                # ìœ ì‚¬ë„ í–‰ë ¬: [num_units, num_subcats]
                sim_mat = np.matmul(unit_embs, factor_mat.T)

                for i, unit in enumerate(review_units):
                    s_score = float(sentiment_scores[i])
                    sims = sim_mat[i]
                    for j, sim in enumerate(sims):
                        if sim > SIMILARITY_THRESHOLD:
                            f_name = subcat_list[j]
                            factor_sentiment_map[f_name].append(s_score)
            except Exception as e:
                print(f"ë°°ì¹˜ ì„ë² ë”©/ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")

            # 3. í•­ëª©ë³„ ìµœì¢… ì ìˆ˜ ê³„ì‚° (0~1 ìŠ¤ì¼€ì¼ ê·¸ëŒ€ë¡œ í‰ê· )
            for main_cat, subcats in scores.items():
                for subcat in subcats.keys():
                    vals = factor_sentiment_map.get(subcat, [])
                    if vals:
                        scores[main_cat][subcat] = float(np.mean(vals))
                    else:
                        # ê´€ë ¨ ë¬¸ì¥ì´ ì—†ëŠ” ê²½ìš° ì¤‘ë¦½ ì ìˆ˜ (0.5) ë¶€ì—¬
                        scores[main_cat][subcat] = 0.5
        
        # ìµœì¢… ì •ë³´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        place_infos.append({
            'name': place.get('name', 'ì´ë¦„ ì—†ìŒ'), 
            'summary': summary,
            'address': place.get('formatted_address', place.get('vicinity', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')),
            'scores': scores, 
            'geometry': place.get('geometry', {}), 
            'place_id': place.get('place_id', ''),
            'positive_keywords': positive_keywords, 
            'negative_keywords': negative_keywords, 
        })

    state.places = place_infos
    return state.dict()

# ----------------------------------------------------
# 5. LangGraph êµ¬ì„±
# ----------------------------------------------------

graph = StateGraph(AgentState)
graph.add_node("search_places", search_places)
graph.add_node("analyze_reviews", analyze_reviews)
graph.set_entry_point("search_places")
graph.add_edge("search_places", "analyze_reviews")
graph.add_edge("analyze_reviews", END)
agent = graph.compile()


# ----------------------------------------------------
# 6. Streamlit UI
# ----------------------------------------------------

st.title("ì¥ì†Œì„± ìš”ì¸ ê¸°ë°˜ ê³µê°„ ì •ëŸ‰ í‰ê°€ ë„êµ¬")

st.markdown("ë¶„ì„í•  ê³µê°„ì˜ ìœ„ì¹˜ì™€ ê°ì„±/ê¸°ëŠ¥ì  íŠ¹ì„±ì„ ì…ë ¥í•˜ì‹­ì‹œì˜¤. Â \n"
             "<span style='color:gray'>(ì˜ˆ: ì‹ ì´Œ ì¡°ìš©í•œ ì¹´í˜, ì¢…ë¡œêµ¬ ì „í†µì ì¸ ìŒì‹ì , ë§ˆí¬êµ¬ ì‚°ì±…ë¡œ ê³µì›)</span>", 
             unsafe_allow_html=True)
query = st.text_input("", placeholder="ì˜ˆ: ì‹ ì´Œ ì¡°ìš©í•œ ì¹´í˜")

if st.button("ì¥ì†Œì„± ì •ëŸ‰ ë¶„ì„ ì‹œì‘"):
    if not query.strip():
        st.warning("ì¥ì†Œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("NLP ë° LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ë¦¬ë·°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¥ì†Œì„± ìš”ì¸ì„ ì •ëŸ‰ í‰ê°€í•˜ëŠ” ì¤‘..."):
            result = agent.invoke({"query": query, "places": [], "answer": ""})
            places = result.get('places', [])
            st.session_state.history.append((query, places))
            st.rerun()

# ê²°ê³¼ ì¶œë ¥
if st.session_state.history:
    latest_query, latest_places = st.session_state.history[-1]
    st.markdown(f"---")
    st.markdown(f"### '{latest_query}'ì— ëŒ€í•œ ì¥ì†Œì„± í‰ê°€ ê²°ê³¼")

    for i, place in enumerate(latest_places):
        with st.container(border=True):
            st.subheader(place.get('name', 'ì´ë¦„ ì •ë³´ ì—†ìŒ'))
            st.markdown(f"**ğŸ“ ì£¼ì†Œ:** {place.get('address', 'ì£¼ì†Œ ì •ë³´ ì—†ìŒ')}")
            st.markdown(f"**ğŸ“ ë¦¬ë·° ìš”ì•½ (LLM ìƒì„±):** {place.get('summary', 'ìš”ì•½ ì •ë³´ ì—†ìŒ')}")

            scores = place.get('scores')
            if scores:
                st.markdown(f"**ğŸ“Š ì¥ì†Œì„± ì¢…í•© í‰ê°€ (NLP ê¸°ë°˜)**")

                # Sunburst ì°¨íŠ¸ ë°ì´í„° ìƒì„±
                labels = []
                parents = []
                values = []
                colors = []

                # ë¶€ë“œëŸ¬ìš´ íŒŒìŠ¤í…”í†¤ ìƒ‰ìƒ ë§µ (factors.json êµ¬ì¡°ì™€ ë™ì¼í•œ ëŒ€ë¶„ë¥˜ ë¼ë²¨)
                color_map = {
                    "ë¬¼ë¦¬ì  íŠ¹ì„±": "rgb(173, 216, 230)",     # ì—°í•œ íŒŒë€ìƒ‰ (Light Blue)
                    "í™œë™ì  íŠ¹ì„±": "rgb(152, 251, 152)",   # ì—°í•œ ì—°ë‘ìƒ‰ (Light Lime Green)
                    "ì˜ë¯¸ì  íŠ¹ì„±": "rgb(255, 182, 193)" # ì—°í•œ ë¶„í™ìƒ‰ (Light Pink)
                }

                # ë£¨íŠ¸ ë…¸ë“œ ì¶”ê°€ (ì „ì²´ ì ìˆ˜ì˜ í‰ê· ìœ¼ë¡œ ì„¤ì •)
                all_scores = [s for main_cat, sub_scores in scores.items() for s in sub_scores.values() if s is not None]
                total_score = sum(all_scores)
                score_count = len(all_scores)
                root_value = total_score / score_count if score_count > 0 else 0.5

                labels.append(place['name'])
                parents.append("")
                values.append(root_value)
                colors.append("#FFFFFF")

                # ëŒ€ë¶„ë¥˜ì™€ ì„¸ë¶€ ë¶„ë¥˜ ì¶”ê°€
                for main_cat, sub_scores in scores.items():
                    main_scores = [s for s in sub_scores.values() if s is not None]
                    main_avg = sum(main_scores) / len(main_scores) if main_scores else 0
                    
                    labels.append(main_cat)
                    parents.append(place['name'])
                    values.append(main_avg)
                    colors.append(color_map.get(main_cat, "#CCCCCC"))
                    
                    for sub_cat, score in sub_scores.items():
                        if score is not None:
                            labels.append(f"{sub_cat}: {score:.2f}") # ì ìˆ˜ë¥¼ ë¼ë²¨ì— í¬í•¨
                            parents.append(main_cat)
                            values.append(float(score))
                            colors.append(color_map.get(main_cat, "#CCCCCC"))
                
                # Sunburst ì°¨íŠ¸ ìƒì„±
                try:
                    fig_sunburst = go.Figure(go.Sunburst(
                        labels=labels,
                        parents=parents,
                        values=values,
                        branchvalues="remainder",
                        marker=dict(colors=colors),
                        hovertemplate='<b>%{customdata[0]}</b><br>ì ìˆ˜: %{value:.2f}', # customdataëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ valueë§Œ í‘œì‹œ
                        maxdepth=2,
                        insidetextorientation='radial'
                    ))
                    
                    fig_sunburst.update_layout(
                        margin=dict(t=20, l=10, r=10, b=10),
                        height=400,
                        title_text=f"{place['name']} ì¥ì†Œì„± ì¢…í•© í‰ê°€",
                        font=dict(size=12, family="NotoSansKR, sans-serif")
                    )
                    
                    st.plotly_chart(fig_sunburst, use_container_width=True, key=f"sunburst_{i}_{place.get('place_id','')}")
                    
                except Exception as e:
                    # Sunburst ì‹¤íŒ¨ ì‹œ Treemap ì‹œë„
                    st.error(f"Sunburst ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    pass

                st.markdown(f"**ğŸ“Š ì¥ì†Œì„± ëŒ€ë¶„ë¥˜ í‰ê°€**")
                main_scores = {main: round(sum(filter(None, sub.values())) / len(sub), 2) for main, sub in scores.items() if any(s is not None for s in sub.values())}
                if main_scores:
                    df = pd.DataFrame(list(main_scores.items()), columns=['ë¶„ë¥˜', 'ì ìˆ˜'])
                    fig_bar = px.bar(df, x='ë¶„ë¥˜', y='ì ìˆ˜', color='ë¶„ë¥˜', color_discrete_map=color_map, range_y=[0, 1], text_auto='.2f')
                    fig_bar.update_layout(showlegend=False, title_text="")
                    st.plotly_chart(fig_bar, use_container_width=True, key=f"bar_{i}_{place.get('place_id','')}")
                else:
                    st.warning("ì •ëŸ‰ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

                # ì›Œë“œ í´ë¼ìš°ë“œ ì‹œê°í™” (LLM ì¶”ì¶œ í‚¤ì›Œë“œ ì‚¬ìš©)
                if place.get('positive_keywords') or place.get('negative_keywords'):
                    st.markdown("---")
                    st.markdown("**ğŸ“ ë¦¬ë·° í‚¤ì›Œë“œ ë¶„ì„ (LLM ì¶”ì¶œ)**")
                    
                    col_pos, col_neg = st.columns(2)
                    
                    # ê¸ì • ì›Œë“œ í´ë¼ìš°ë“œ
                    if place.get('positive_keywords'):
                        with col_pos:
                            st.markdown("#### âœ… ê¸ì • í‚¤ì›Œë“œ")
                            text = " ".join(place['positive_keywords'])
                            if text:
                                img = generate_wordcloud(text, font_path, colormap="Greens")
                                if img is not None:
                                    st.image(img, use_container_width=True)
                            else:
                                st.info("ê¸ì • í‚¤ì›Œë“œ ì—†ìŒ")
                    
                    # ë¶€ì • ì›Œë“œ í´ë¼ìš°ë“œ
                    if place.get('negative_keywords'):
                        with col_neg:
                            st.markdown("#### âŒ ë¶€ì • í‚¤ì›Œë“œ")
                            text = " ".join(place['negative_keywords'])
                            if text:
                                img = generate_wordcloud(text, font_path, colormap="Reds")
                                if img is not None:
                                    st.image(img, use_container_width=True)
                            else:
                                st.info("ë¶€ì • í‚¤ì›Œë“œ ì—†ìŒ")
                
                # ì§€ë„ ë° ë¡œë“œë·° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                if place.get('geometry') and place['geometry'].get('location'):
                    lat, lng = place['geometry']['location']['lat'], place['geometry']['location']['lng']
                    
                    map_key = f"map_{i}_{place['place_id']}"
                    streetview_key = f"street_{i}_{place['place_id']}"
                    
                    if map_key not in st.session_state:
                        st.session_state[map_key] = False
                    if streetview_key not in st.session_state:
                        st.session_state[streetview_key] = False
                    
                    col1, col2 = st.columns(2)
                    
                    # ë²„íŠ¼ í´ë¦­ ì‹œ ìƒíƒœ í† ê¸€ í›„ ì¬ì‹¤í–‰í•˜ì—¬ ì§€ë„ í‘œì‹œ
                    if col1.button("ğŸ—ºï¸ ì§€ë„ ë³´ê¸°", key=f"btn_{map_key}"):
                        st.session_state[map_key] = not st.session_state[map_key]
                        st.rerun()
                    
                    if col2.button("ğŸš— ë¡œë“œë·° ë³´ê¸°", key=f"btn_{streetview_key}"):
                        st.session_state[streetview_key] = not st.session_state[streetview_key]
                        st.rerun()
                    
                    if st.session_state[map_key] or st.session_state[streetview_key]:
                        st.markdown("**ğŸ“ ìœ„ì¹˜ ì •ë³´**")
                        
                        if st.session_state[map_key]:
                            st.markdown("**ğŸ—ºï¸ ì§€ë„**")
                            # Google Maps Embed API
                            map_url = f"https://www.google.com/maps/embed/v1/place?key={st.session_state.gmaps_key}&q={lat},{lng}"
                            st.components.v1.iframe(map_url, height=400, width=700)
                        
                        if st.session_state[streetview_key]:
                            st.markdown("**ğŸš— ë¡œë“œë·°**")
                            # Google Maps Street View Embed API
                            streetview_url = f"https://www.google.com/maps/embed/v1/streetview?key={st.session_state.gmaps_key}&location={lat},{lng}"
                            st.components.v1.iframe(streetview_url, height=400, width=700)
                else:
                    st.info("ğŸ“ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ì–´ ì§€ë„ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
